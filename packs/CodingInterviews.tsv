Pack	Topic	Problem Name	Problem Link	Question Statement	Time Complexity	Memory Complexity	Expectation	A	B	C	D	Correct	Message Response For A	Message Response For B	Message Response For C	Message Response For D	Hint	C++ Code Link	Python Code Link	Java Code Link	English Video Solution
Coding Interviews	Arrays: Prefix Sum	Maximum Difference Between Increasing Elements	https://leetcode.com/problems/maximum-difference-between-increasing-elements/	**Given** a integer array *nums* of size n, find the maximum difference between nums[i] and nums[j] (i.e., nums[j] - nums[i]), such that 0 <= i < j < n and nums[i] < nums[j].   /n/nReturn the maximum difference. If no such i and j exists, return -1.	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	Cumulative Sum	Sliding Window	Sliding Window + Hash Table	Linear Scan + Single Pointer	D	Incorrect!  This doesn't keep track of the minimum element encountered as we pass through the array.	Not quite the optimal approach!  A sliding window is typically used when we're looking for a contiguous array of a fixed length.	Extremely inefficient.  This would be O(n) in memory with no improvement in time complexity, whereas the optimal solution is O(1) in memory.	Correct!  We can iterate through the array from back to front in a single pass, keeping track of the MAXIMUM element found so far, while checking each new element to see if it's less than the maximum element. If so, we calculate the difference between this element and the maximum, and then update the maximum difference if necessary.  Time Complexity: O(n)  Memory Complexity: O(1).	Brute-force approach: using nested loops, consider all possible pairs (i, j) where i < j and nums[i] < nums[j]. For each pair, compute their difference and keep track of the maximum difference found so far.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Maximum%20Difference%20Between%20Increasing%20Elements.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Maximum%20Difference%20Between%20Increasing%20Elements.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Maximum%20Difference%20Between%20Increasing%20Elements.java	https://youtu.be/CwE92dPwMBs
Coding Interviews	Arrays:Sliding Window	SubArray Sum Equals K	https://leetcode.com/problems/subarray-sum-equals-k/	Given an array of integers nums and an integer k, return the total number of subarrays whose sum equals to k.    A subarray is a contiguous non-empty sequence of elements within an array.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Cumulative Sum	Binary Indexed Tree (BIT)	Sliding Window + Hash Table	Sorting + Sliding Window	C	Feasible, if inefficient!  We'd precompute the prefix sums of the given array, with the ith prefix sum representing the sum of elements from index 0 to index i in the array. The subarray sum between two indices i and j can then be computed as the difference between the prefix sums at these indices.  Time Complexity: O(n^2) due to the nexted loop, which makes it inferior to the sliding window approach with a hash table.	This is an obscure and slightly inefficient approach.  The BIT can conceivably compute the sum of a range of elements. The subarray whose sum equals the target value k can be found by iteratively adding elements to the BIT and checking if the difference between the current sum and k has been seen before. However, it's O(n log(n)) in time, and also takes up a lot of space compared to other methods.	Correct!  This only needs a single pass over the array, and uses constant space to maintain the hash table! The hash table also allows for constant time lookups of the sum of elements up to each index, which helps to reduce the time complexity of the algorithm.  Time Complexity: O(n).  Memory Complexity: O(n).	Suboptimal.  This approach involves sorting the given array and then iterating over it to compute the sum of all possible subarrays. The idea is that the sorted array will have all the subarrays with the same sum grouped together, which can then be counted using a sliding window approach. However, the sorting step takes O(n log(n)) time.	In the brute-force approach, we consider all possible subarrays of the given array and calculate their sum. We can then count the number of subarrays whose sum equals to k. This can be done by using two nested loops to iterate through all possible starting and ending indices of the subarrays and calculating their sum. If the sum equals k, increment a counter.  Time Complexity: O(n^3), as we are iterating through all possible subarrays.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Subarray%20Sum%20Equals%20K.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Subarray%20Sum%20Equals%20K.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Subarray%20Sum%20Equals%20K.java	https://youtu.be/cByVCkZJMr0
Coding Interviews	Arrays: Prefix Sum	Contiguous Array	https://leetcode.com/problems/contiguous-array/	Given a binary array nums, return the maximum length of a contiguous subarray with an equal number of 0 and 1.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Sliding Window + Hash Table	Stack	Kadane's Algorithm	Two Linear Passes	A	Correct!  The given solutions make use of a relatively famous application of the sliding window technique to solve this problem! I strongly encourage you to try it yourself!  Time Complexity: O(n).  Memory Complexity: O(n).	Not quite the best approach!  It IS O(n) in both time and memory complexity, but requires more operations to maintain the stack and find the farthest elements, which makes it slightly inferior to the Sliding Window approach.	This would not be a good approach for solving this kind of problem!	In this suboptimal attempt, the first pass replaces all 0s with -1 and computes the prefix sums of the resulting array. In the second pass, the maximum length of the contiguous subarray with zero sum is computed.  Time Complexity: O(n)  Space complexity: O(n) due to the use of prefix sums, but it requires two passes over the input array, making it less efficient than the optimal sliding window approach. We prefer both the sliding window and the stack to this method.	A brute force approach to solve this problem would be to consider all possible subarrays and for each subarray count the number of 0's and 1's. If the number of 0's and 1's is the same, update the maximum length of subarrays found so far.   Time Complexity: O(n^3) as we need to consider all possible subarrays, and for each subarray, we would need to count the number of 0's and 1's, which would take O(n) time.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Contiguous%20Array.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Contiguous%20Array.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Contiguous%20Array.java	https://youtu.be/ijJMT1slQdQ
Coding Interviews	Arrays: Sliding Window	Contains Duplicate II	https://leetcode.com/problems/contains-duplicate-ii/	Given an integer array nums and an integer k, return true if there are two distinct indices i and j in the array such that nums[i] == nums[j] and abs(i - j) <= k.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Cumulative Sum + Hash Table	Sorting	Sliding Window + Hash Table	Sliding Window + Cleverly Nested Loops	C	This idea just doesn't work at all. We're looking for a matching pair in the array, rather than anything that involves computing a sum.	Complicated!  At best, this would yield a solution of O(n log(n)) time due to the sorting step.	Correct!  In this approach, we maintain a window of size k and slide it over the input array. We use a hash table to keep track of the count of each element within the current window. Whenever we slide the window to the right, we decrement the count of the element that is no longer in the window and increment the count of the new element that enters the window. We also keep track of the maximum count seen so far within any window of size k. If this maximum count is greater than or equal to t, we return true.  The key data structure used in this approach is a hash table, which allows us to efficiently keep track of the count of each element within the current window. We also use two pointers to keep track of the current window, and a variable to keep track of the maximum count seen so far.  Time Complexity: O(n).  Memory Complexity: O(n).	However ingenious, nested loops will remain O(n^2) with this approach! This would involve using a sliding window of size k to check every possible pair of elements within the window, moving the window one element at a time until we have checked all possible pairs.	A brute force approach would involve checking every pair of indices (i, j) such that i < j and nums[i] == nums[j] and abs(i - j) <= k. This can be done by using two nested loops to iterate over all possible pairs of indices, and then checking the conditions for each pair. If a pair is found that satisfies the conditions, return true. Otherwise, if no such pair is found, return false. The time complexity of this approach would be O(n^2), where n is the length of the array nums, since we need to iterate over all pairs of indices. The space complexity would be O(1), since we are only using a constant amount of extra space to store the loop variables and the boolean flag for the result.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Contains%20Duplicate%20II.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Contains%20Duplicate%20II.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Contains%20Duplicate%20II.java	https://youtu.be/gFLbo_FFRDs
Coding Interviews	Arrays: Sliding Window	Max Consecutive Ones III	https://leetcode.com/problems/max-consecutive-ones-iii/	Given a binary array nums and an integer k, return the maximum number of consecutive 1's in the array if you can flip at most k 0's.	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	Two Pointers	Sliding Window	Floyd's Tortoise And Hare	Hash Table	B	Incorrect!  This approach is similar to the correct one, but it does not consider the fact that we can flip at most k zeroes. It may update the result even when it has already flipped k zeros, leading to an incorrect answer.	Correct!  As the name suggests, we maintain a window of the required elements and slide it over the array. In this particular problem, we keep a window of consecutive 1's while tracking the number of 0's encountered inside the window. If the number of 0's exceeds k, we move the window's start index to the right until the number of 0's inside the window is less than or equal to k.	No!	This is not the best idea.  This uses a hash table to store the number of zeroes and ones in each subarray of the input array, then iterates through it to find the subarray with the maximum number of ones that can be obtained by flipping at most k zeroes. However, the time complexity of this approach would be O(n^2), which is inefficient.	One brute force approach is to try all possible subarrays of the given array and keep track of the maximum number of consecutive 1s that can be obtained by flipping at most k 0s. Here's an outline of the algorithm: 1)Initialize a variable maxLen to 0.  2) Iterate over all possible subarrays of the given array using two nested loops, one for the start index and one for the end index.  3) Count the number of zeros in the current subarray. If the count is less than or equal to k, update maxLen to the maximum of its current value and the length of the subarray.  4) Return maxLen.  However, this approach has a time complexity of O(n^3), where n is the length of the input array, since it requires iterating over all possible subarrays. There are more efficient algorithms that can solve this problem in O(n) time using sliding window or prefix sum techniques.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Max%20Consecutive%20Ones%20III.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Max%20Consecutive%20Ones%20III.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Max%20Consecutive%20Ones%20III.java	https://youtu.be/OT1vDll3F0Y
Coding Interviews	Arrays: Sliding Window	Longest Substring Without Repeating Characters	https://leetcode.com/problems/longest-substring-without-repeating-characters/	Given a string s, find the length of the longest   substring  without repeating characters.	O(n)	O(min(n,m))	We're expecting an optimized solution of time complexity O(n) and memory complexity O(min(n,m)).	Stack + Hash Table	Sliding Window + Hash Set	Two Pointers	Sliding Window + Hash Table	D	Overly engineered!  This solution has a time complexity of O(n^2) in the worst case, while the space complexity is O(n).  The idea here is to iterate through each character in s, and for each character, we first check if it has been visited before. If it has, we pop characters from the stack until we reach the first occurrence of the current character, and mark all popped characters as unvisited in the hash table.  We then push the current character onto the stack, mark it as visited, and update the result with the maximum length seen so far.	Inefficient!  We'd combine a sliding window approach with a hash set by maintaining a hash set of characters in the current window and sliding the window along the string. We'd add characters to the hash set while moving the window forward, and remove characters from the hash set while moving it backwards. The time complexity of this approach would be O(n^2) in the worst case, where n is the length of the input string.	Not Quite!  This is a solid approach, using two pointers to maintain a sliding window of distinct characters. The left pointer can mark the start of the window, with a right pointer expanding the window each time. When encountering a repeating character, we move the left pointer to the index after the repeated character. The time complexity of this approach would be O(n) in the best case, where all characters are distinct, and O(2n) in the worst case, where all characters are the same.	Correct!  Our algorithm initializes two pointers, start and end, to the beginning of the string. It then moves the end pointer to the right until it reaches a repeated character. At this point, it moves the start pointer to the next index AFTER the previous occurrence of the repeated character, effectively removing that character and any previous characters from the substring.  After each iteration, it calculates the length of the current substring and updates the maximum length. Finally, it returns the maximum length.  Time Complexity: O(n), since it performs a constant number of operations for each character in the string.  Space Complexity: O(min(n, m)), where m is the size of the character set. In the worst case where all characters in the string are unique, the space complexity is O(n).	With brute-force, this can be done using two nested loops to iterate over the starting and ending indices of the substring, with a hash set keeping track of the unique characters in the current substring. This would be O(n^3) since we are iterating over all possible substrings, making it extremely inefficient for larger inputs.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Longest%20Substring%20Without%20Repeating%20Characters.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Longest%20Substring%20Without%20Repeating%20Characters.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Longest%20Substring%20Without%20Repeating%20Characters.java	https://youtu.be/No-PWvLlPS0
Coding Interviews	Arrays: Sliding Window	Maximum Subarray	https://leetcode.com/problems/maximum-subarray/	Given an integer array nums, find the subarray with the largest sum, and return its sum.	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	Greedy Algorithm	Kadane's Algorithm	Divide And Conquer	Sort + Sum	B	Rather unwieldy!  The idea would be to add the current element at each index to the sum if it increases the sum - and start a new subarray otherwise. This approach may fail for certain input cases, such as when starting a new subarray prematurely would cause the maximum subarray sum to be missed. Kadane's algorithm is better for handling all input cases correctly.	Correct!  Kadane's algorithm works by iterating through the array, calculating the maximum sum ending at the current index. If the maximum sum ending at the current index is greater than the current element, then it updates the maximum sum ending at the current index. Finally, it updates the result with the maximum of the current result and the maximum sum ending at the current index.  The time complexity of Kadane's algorithm is O(n), as it only needs to iterate through the array once.  Memory Complexity: O(1), as it only uses constant extra space to store the maximum sum ending at the current index and the maximum subarray sum found so far.	Not the most efficient approach here!  This would divide the array in half, while recursively finding both the maximum subarray sum for each half AND the maximum subarray sum crossing the middle. The results can then be combined successfully, albeit with a time complexity of O(n log(n)), which is not as efficient as Kadane's algorithm. It also requires more memory to store the recursive calls and results.	This approach does not work because the subarray must be contiguous. Sorting the array can change the order of the elements and therefore break the contiguity of the subarray.	The brute-force is simplistic here: try all possible subarrays and keep track of the maximum sum.  This approach has a time complexity of O(n^2), which is not efficient for large input sizes. It is also unnecessary because Kadane's algorithm has a linear time complexity.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Maximum%20Subarray.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Maximum%20Subarray.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Maximum%20Subarray.java	https://youtu.be/ntYRH32QidE
Coding Interviews	Arrays: Sliding Window	Maximum Absolute Sum Of Any Subarray	https://leetcode.com/problems/maximum-absolute-sum-of-any-subarray/	You are given an integer array nums. The absolute sum of a subarray [numsl, numsl+1, ..., numsr-1, numsr] is abs(numsl + numsl+1 + ... + numsr-1 + numsr).    Return the maximum absolute sum of any (possibly empty) subarray of nums.    Note that abs(x) is defined as follows:    If x is a negative integer, then abs(x) = -x.  If x is a non-negative integer, then abs(x) = x.	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	Divide And Conquer	Sort and Sum	Kadane's Algorithm	Dynamic Programming	C	Incorrect.  The idea is to divide the array into two subarrays: one with all negative numbers and one with all non-negative numbers. We'd then return the absolute sum of the subarray with the largest sum.  However, it may not handle cases where the array contains both positive and negative numbers correctly.	Suboptimal at best!  The idea is to sort the array in non-decreasing order and return the absolute sum of the subarray containing the largest and smallest elements.  Sorting the array takes O(n log(n)) time, which is not optimal for this problem. In addition, this approach doesn't consider all subarrays, and it may miss the maximum absolute sum.	Correct!  To solve this problem, our solution first applies Kadane's algorithm to find the maximum sum subarray and the minimum sum subarray of the given array. It then takes the maximum of the absolute value of the maximum sum subarray and the absolute value of the minimum sum subarray to get the maximum absolute sum of any subarray.  Time Complexity: O(n), as it uses Kadane's algorithm twice, with both iterations taking linear time.  Space Complexity: O(1), as it only uses constant extra space to store the maximum sum and minimum sum subarrays found so far.	Madness! The method uses DP to keep track of the maximum and minimum absolute sum of any subarray that ends at each index.  This approach has a time complexity of O(n^2), due to the use of nested loops that iterate over all previous indices. It can also produce incorrect results, as the maximum absolute sum subarray may not end at the last index. For example, if the array has a single negative number, the maximum absolute sum subarray will be of length 1 and will not end at the last index.	This has an intuitive brute-force approach, where we iterate through all subarrays of nums and calculate their absolute sums. After that, we return the maximum of all the absolute sums found.  However, this approach is O(n^3) in time complexity.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Maximum%20Absolute%20Sum%20of%20Any%20Subarray.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Maximum%20Absolute%20Sum%20of%20Any%20Subarray.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Maximum%20Absolute%20Sum%20of%20Any%20Subarray.java	https://youtu.be/AcxHNqNGoP4
Coding Interviews	Arrays: Sliding Window	Maximum Sum Circular Subarray	https://leetcode.com/problems/maximum-sum-circular-subarray/	Given a circular integer array nums of length n, return the maximum possible sum of a non-empty subarray of nums.    A circular array means the end of the array connects to the beginning of the array. Formally, the next element of nums[i] is nums[(i + 1) % n] and the previous element of nums[i] is nums[(i - 1 + n) % n].    A subarray may only include each element of the fixed buffer nums at most once. Formally, for a subarray nums[i], nums[i + 1], ..., nums[j], there does not exist i <= k1, k2 <= j with k1 % n == k2 % n.	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	Divide and Conquer	Dynamic Programming	Kadane's Algorithm	Sliding Window	C	Over-complicated!  We divide the array into two, recursively finding the maximum sum subarray in each part. Then, we need to consider the subarrays that cross the midpoint and calculate their sum. The maximum of these three sums would be the maximum sum circular subarray. However, a D&C approach would take O(n^2) time complexity and O(n) space complexity!	Overly complex compared to other approaches!  Time Complexity: O(n^2)   Space Complexity: O(n).	Correct!  We can first find the maximum subarray sum with Kadane's algorithm, and then the minimum subarray sum using the same algorithm. Then, we can calculate the sum of the entire array. The solution then uses these values to calculate the maximum sum subarray in a circular array. If the sum of the entire array equals the minimum subarray sum, then the array has no circular subarray, and the maximum subarray sum is simply the maximum subarray sum found in the linear array. Otherwise, the maximum sum subarray could be a circular subarray, and its sum can be calculated as either the maximum subarray sum, or the difference between the sum of the entire array and the minimum subarray sum.  Time Complexity: O(n)  Memory Complexity: O(1).	This is a very expensive window!   We can calculate the maximum sum subarray that starts from each index, and then calculate the maximum sum subarray that wraps around the array, which can be done by splitting it into two subarrays and calculating their sums using Kadane's algorithm. The maximum of these three sums would be the maximum sum circular subarray.  This approach would take O(n^2) time complexity and O(1) space complexity.	A brute force approach to solve this problem is to iterate through every possible subarray of nums and calculate its sum. Since the array is circular, we can create a new array with size 2*n, where the first n elements are the same as nums, and the last n elements are also the same as nums. Then, for each starting index i (0 <= i < n), we can iterate j (i <= j < i+n), and calculate the sum of nums[i] + nums[i+1] + ... + nums[j]. If the j index goes over n, we can simply subtract n from it to get the corresponding index in the original nums array. We can keep track of the maximum sum we find and return it at the end.  Although this approach is simple to understand and implement, its time complexity is O(n^2), making it inefficient for large arrays.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Maximum%20Sum%20Circular%20Subarray.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Maximum%20Sum%20Circular%20Subarray.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Maximum%20Sum%20Circular%20Subarray.java	https://youtu.be/GYq4vjnJFxU
Coding Interviews	Arrays: Two Pointers	Two Sum II: Input Array Is Sorted	https://leetcode.com/problems/two-sum-ii-input-array-is-sorted/	Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.    You may assume that each input would have exactly one solution, and you may not use the same element twice.    You can return the answer in any order.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Sorting	Floyd's Tortoise And Hare	Kadane's Algorithm + Hash Set	Two Pointers	D	Not the best approach listed.  Hypothetically, we'd sort the array first. We could use two pointers: one starting from the beginning of the array, and one starting from the end. Move the pointers towards each other, checking the sum of the values at each pointer. If the sum is greater than the target, move the end pointer to the left. If the sum is less than the target, move the start pointer to the right.  Time complexity: O(n log(n)) due to sorting.  While this approach is more efficient than the brute force approach, it changes the order of the elements in the array and does not return the original indices of the two numbers that add up to the target.	Simply, no!	Why?  There's no reason to utilize Kadane's algorithm here: it is reserved for finding the maximum or minimum sums of arrays or subarrays.	Correct!  To optimize the solution, we can use a two-pointer approach since the array is sorted. We initialize two pointers, one at the beginning of the array (left) and one at the end (right). We compare the sum of the numbers at the left and right indices with the target value. If the sum is less than the target, we move the left pointer to the right to consider a larger number. If the sum is greater than the target, we move the right pointer to the left to consider a smaller number. If the sum is equal to the target, we return the indices of the two numbers.  Time Complexity: O(n), where n is the size of the input array.  Space Complexity: O(n) because the hash table may contain all elements of the array.	Brute-force: For each pair of indices (i,j) with i < j, check if nums[i] + nums[j] == target. Time complexity is O(n^2).  This approach is very inefficient since it requires a nested loop to check every possible pair of indices.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Two%20Sum%20II%20-%20Input%20Array%20Is%20Sorted.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Two%20Sum%20II%20-%20Input%20Array%20Is%20Sorted.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Two%20Sum%20II%20-%20Input%20Array%20Is%20Sorted.java	https://youtu.be/rwFbAkpdS-8
Coding Interviews	Arrays: Two Pointers	Move Zeroes	https://leetcode.com/problems/move-zeroes/	Given an integer array nums, move all 0's to the end of it while maintaining the relative order of the non-zero elements.	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	Copy Array Approach	Count-Based Approach	Linear Pass + Swapping	Two Pointers	D	Not optimal.  This approach requires an additional array to be created - violating the requirement of doing the operation in-place.	You'll need to re-read the question.  This approach violates the requirement of doing the operation in-place!	Incorrect: this is rather untuitive!  This approach results in elements being moved out of order and may result in non-zero elements being moved past other zeros, requiring additional iterations to move them back.	Correct!  In this approach, we use two pointers, start and end, to traverse the array. The start pointer is used to keep track of the current position to place the next non-zero element, while the end pointer is used to traverse the array and find non-zero elements. When a non-zero element is found, it is swapped with the element at the start position, and the start pointer is incremented by 1. By the end of the traversal, all non-zero elements have been moved to the front of the array, while the remaining elements are all zeros at the end.  Time complexity: O(n) since we only traverse the array once.  Space complexity: O(1) since we are modifying the input array in-place.	One simple brute force approach is to iterate through the array and for each non-zero element, swap it with the first available zero element after it. This approach ensures that all non-zero elements are moved to the front of the array while maintaining their relative order, and all remaining elements at the end of the array are zeroes.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Move%20Zeroes.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Move%20Zeroes.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Move%20Zeroes.java	https://youtu.be/M6q8k435x6Q
Coding Interviews	Arrays: Two Pointers	Remove Duplicates From Sorted Array	https://leetcode.com/problems/remove-duplicates-from-sorted-array/	Given an integer array nums sorted in non-decreasing order, remove the duplicates in-place such that each unique element appears only once. The relative order of the elements should be kept the same. Then return the number of unique elements in nums.    Consider the number of unique elements of nums be k, to get accepted, you need to do the following things:    Change the array nums such that the first k elements of nums contain the unique elements in the order they were present in nums initially. The remaining elements of nums are not important as well as the size of nums.  Return k.	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	Hash Set	Two Pointers	Linear Pass With Deletion	Copy Array Approach	B	The extra memory makes this sub-optimal.  The idea? Use a hash set to store the unique elements and then copy the elements back to the original array.  Although this approach can be implemented in O(n) time, it requires extra memory to store the hash set, which violates the requirement of doing the operation in-place.	Correct! The idea behind this approach is to use two pointers to traverse the array: a slow pointer that points to the last unique element found so far, and a fast pointer that scans the array to find the next unique element. If the fast pointer finds a new unique element, it is swapped with the element pointed by the slow pointer, and the slow pointer is moved to the next position.  Time Complexity: O(n), where n is the length of the input array  Space complexity: O(1), since you do not use any additional memory to store the unique elements.	Expensive!  We'd be iterating through the array and removing any element that has already been seen.  This approach would require removing elements from the middle of the array, which is a very costly operation with an array, as all subsequent elements would have to be shifted to fill the gap.	No.  This approach violates the requirement of doing the operation in-place.	A simple brute force approach to solve this problem would be to iterate over the array and for each element, check if it is already in the set. If it is not, then add it to the set and increment the count of unique elements. At the end, return the count of unique elements.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Remove%20Duplicates%20from%20Sorted%20Array.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Remove%20Duplicates%20from%20Sorted%20Array.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Remove%20Duplicates%20from%20Sorted%20Array.java	https://youtu.be/rzDiYyV_Em4
Coding Interviews	Arrays: Two Pointers	Longest Mountain in Array	https://leetcode.com/problems/longest-mountain-in-array/	You may recall that an array arr is a mountain array if and only if:    arr.length >= 3  There exists some index i (0-indexed) with 0 < i < arr.length - 1 such that:  arr[0] < arr[1] < ... < arr[i - 1] < arr[i]  arr[i] > arr[i + 1] > ... > arr[arr.length - 1]  Given an integer array arr, return the length of the longest subarray, which is a mountain. Return 0 if there is no mountain subarray.	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	Two Pointers	Recursion	Sorting + Comparing	Two-Pass Iteration	A	Correct!  This method involves iterating over the array using two pointers, one starting from the beginning of the array and another from the end of the array, and moving them towards each other until they meet at a certain condition. In this solution, however, two pointers are not used in the traditional sense. Instead, we have two variables 'up' and 'down' that represent the length of the increasing and decreasing subarrays respectively.  At each index of the array, we update 'up' and 'down' based on whether the array is still increasing or decreasing. If we encounter a point where the array stops increasing or decreasing, we reset both 'up' and 'down'. Finally, if we find a subarray that is both increasing and decreasing, we calculate the length of this subarray and check if it's the longest so far.  Time Complexity: O(n).  Memory Complexity: O(1).	Yikes!  This approach would have a time complexity of O(2^n), where n is the size of the array, and would be extremely inefficient - even for moderately sized arrays.	Incorrect!  This approach does not guarantee that the array has a peak element, and therefore may not return the correct answer for arrays that are not mountain arrays.	This approach involves iterating over the array twice and checking for increasing and decreasing subarrays separately. The idea here isn't bad, although the complexity is rather horrendous!  Time Complexity: O(n^2).	A brute force approach would check all possible subarrays for the mountain property.  This approach would also have a time complexity of O(n^2), and would not scale well for larger arrays.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Longest%20Mountain%20in%20Array.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Longest%20Mountain%20in%20Array.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Longest%20Mountain%20in%20Array.java	https://youtu.be/h_LxZDIc6Z8
Coding Interviews	Arrays: Two Pointers	Container With Most Water	https://leetcode.com/problems/container-with-most-water/	You are given an integer array height of length n. There are n vertical lines drawn such that the two endpoints of the ith line are (i, 0) and (i, height[i]).    Find two lines that together with the x-axis form a container, such that the container contains the most water.    Return the maximum amount of water a container can store.    Notice that you may not slant the container.	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	Sorting	Dynamic Programming	Two Pointers	Stack	C	Not quite!  The method would involve sorting the array in non-increasing order, and iterating over it to calculate the area between each pair of lines.  However, this is O(n log(n)) time for the sorting alone, with the subsequent iteration taking O(n) time.	Whoops! Quite expensive!  The idea would be to use DP to calculate the area between all pairs of lines, and return the maximum area seen.  This approach involves calculating the area between each pair of lines and storing it in a 2D array. Time Complexity: O(n^2).	Correct!  The two-pointers method involves using two pointers, one starting from the beginning of the array and another from the end of the array, and moving them towards each other until they meet at a certain condition. We calculate the area between these two pointers using the formula (min(height[left],height[right])*(right-left)). We then update the maximum area seen so far if this current area is greater than the previous maximum.  Whatever the result, we move the pointer that is pointing to the SHORTER line towards the center of the array, as moving the pointer pointing to the taller line towards the center would only result in a decrease in area. We repeat this process until the two pointers meet.	Costly!  The theory is that we'd have a stack keep track of the lines, and iterate over the array to calculate the area between each pair of lines.  This takes O(n) time. However, the subsequent iteration over the stack will take O(n^2) time in the worst case, resulting in a suboptimal solution.	The brute-force approach uses nested loops to iterate over all possible pairs of lines and calculate their area, keeping track of the maximum area seen so far.  This approach has a time complexity of O(n^2), where n is the size of the array. It will result in a time limit exceed error for large arrays.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Container%20With%20Most%20Water.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Container%20With%20Most%20Water.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Container%20With%20Most%20Water.java	https://youtu.be/cmacEvBestE
Coding Interviews	Arrays: Two Pointers	3 Sum	https://leetcode.com/problems/3sum/	Given an integer array nums, return all the triplets [nums[i], nums[j], nums[k]] such that i != j, i != k, and j != k, and nums[i] + nums[j] + nums[k] == 0.    Notice that the solution set must not contain duplicate triplets.	O(n^2)	O(n)	We're expecting an optimized solution of time complexity O(n^2) and memory complexity O(n) *****This solution is being suggested with a near identical approach to the Four Sum problem in min. There are multiple approaches much more efficient than O(n^2)! This is an exception to the rule in terms of looking for the most ideal optimum solution*****	Sorting + Binary Search	Two Pointers + Sorting	Simple Mathematics	Dynamic Programming	B	The method looks nice, but the complexity is poor.  The idea is to sort the array and use binary search to find a third element that would give it a sum of 0.  Time complexity: O(n^2 log(n)).	Correct! This solution is O(n^2) since it uses nested loops to iterate over all possible combinations of three numbers. However, by sorting the array first, we can optimize the algorithm by using two pointers to traverse the remaining elements of the array. This approach reduces the time complexity from O(n^3) to O(n^2), making the algorithm more efficient.	The maths here? Generate all triplets with a cleverly designed set of loops, and look for when they sum to zero!    However, without other data structures, it's going to give us a time complexity of O(n^3).	DP can generate subsets of size 3 very efficiently, allowing one to check if valid triplets that sum to zero are actually possible. This approach is very convoluted and inefficient, with a time complexity of O(2^n).	A simple brute force approach to solve this problem is to consider all possible combinations of three numbers from the given array and check if the sum of the three numbers is zero. To avoid duplicate triplets, we can sort the array and skip the duplicates while iterating through the array. However, this approach has a time complexity of O(n^3) which is not efficient for larger inputs.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/3Sum.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/3Sum.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/3Sum.java	https://youtu.be/o8bCc1lXtFc
Coding Interviews	Arrays: Two Pointers	4 Sum	https://leetcode.com/problems/4sum/	Given an array nums of n integers, return an array of all the unique quadruplets [nums[a], nums[b], nums[c], nums[d]] such that:    0 <= a, b, c, d < n  a, b, c, and d are distinct.  nums[a] + nums[b] + nums[c] + nums[d] == target  You may return the answer in any order.	O(n^3)	O(n)	We're expecting an optimized solution of time complexity O(n^3) and memory complexity O(n).	Binary Search	Sorting + Nested Loops	Hash Table	Two Pointers + Set + Sorting	D	For each pair of indices (i,j), use binary search to find two more indices that satisfy the sum condition.  This approach takes O(n^3 * log(n)) time complexity, which is very slow for larger inputs.	This approach has (n^3 * log(n)) time complexity, which is not efficient at all.	Sub-optimal compared to other approaches.  The idea? Use a hash table to store all possible pairs of indices (i,j) and their sums, and then iterate over all possible pairs of pairs to find quadruplets that satisfy the sum condition.  This approach takes O(n^2) extra space, and its time complexity is not optimal.	Correct!  The idea is to sort the given input array and then use two nested loops to iterate over all possible pairs of elements. For each pair, two pointers are used to scan the remaining array to find two elements such that their sum equals the remainder required to reach the target sum.  A set is used to store unique quadruplets and avoid duplicates. The elements of any quadruplet found are sorted and then inserted into that set. If the insertion is successful, the quadruplet is added to the final result.  Time Complexity: O(n^3), where n is the size of the input array.  Space Complexity: O(n) due to the use of a set to store unique quadruplets.	The brute-force here is very basic: use three nested loops to iterate over all possible quadruplets, and check the sum.  This approach takes O(n^4) time complexity.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/4Sum.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/4Sum.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/4Sum.java	https://youtu.be/r2jAEF-nIKY
Coding Interviews	Arrays: Two Pointers	Maximum Product of 3 Numbers	https://leetcode.com/problems/maximum-product-of-three-numbers/	Given an integer array nums, find three numbers whose product is maximum and return the maximum product.	O(n log(n))	O(log(n))	We're expecting an optimized solution of time complexity O(n log(n)) and memory complexity O(log(n)).	Sort-Based Approach	Heap-Based Approach	Skinner's Steamed Ham Approach	Hash Table	B	This approach is suboptimal because it takes O(n log(n)) time for the sorting and doesn't handle cases where there are negative numbers in the array properly.	Correct!  The solution uses two priority queues (min-heap and max-heap) to keep track of the largest and smallest elements in the array respectively. The code then pops the largest elements from the max-heap and the smallest elements from the min-heap to obtain the three largest and two smallest elements in the array. Finally, it calculates the maximum of the two possible products of these five elements.  Time Complexity: O(n log(n)) as it involves sorting the array and performing heap operations.  Space Complexity: O(log(n)) due to the use of priority queues.	At this time of day, in this part of the country, localized entirely within your computer? May I see it? I don't think it's right, but I guess it could be an Albany expression. Try again!	The (suboptimal) proposed solution uses a hash table to store the frequencies of each number and then iterates through all possible combinations of three numbers.  This approach has a time complexity of O(n^2), but may fail when there are more than two negative numbers that produce a larger product when multiplied with a positive number.	Brute force approach: generate all possible combinations of three elements and calculate their product, then return the maximum.  This approach is suboptimal because it takes O(n^3) time for the combination generation and multiplication, which is too slow for large arrays.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Maximum%20Performance%20of%20a%20Team.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Maximum%20Performance%20of%20a%20Team.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Maximum%20Performance%20of%20a%20Team.java	https://youtu.be/Fk_wqUvp3yM
Coding Interviews	Arrays: Two Pointers	Longest Consecutive Subsequence	https://leetcode.com/problems/longest-consecutive-sequence/description/	Given an unsorted array of integers nums, return the length of the longest consecutive elements sequence.    You must write an algorithm that runs in O(n) time.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Hash Set and Linear Scan	Dynamic Programming	Sort + Linear Scan	Hash Set + Multiple Scans	A	Correct! I've even seen this done using DFS!  We use a hashset to store the numbers in the input array, and then iterate over the input array to find the starting element of each consecutive subsequence. For each starting element found, the algorithm scans forward in the input array, counting the length of the current consecutive sequence until it ends. The length of each consecutive sequence is compared to the current maximum and the maximum is updated if necessary.  Time Complexity: O(n).  Memory Complexity: O(n).	Hmmm…this approach would be O(n^2) in time complexity. Do better.	The sorting operation automatically rules this out: it's O(n log(n)) in complexity.	Technically, yes...but not the most optimal!  A linear scan is enough! This approach would be O(n), but has slightly more memory usage. You could first count the frequency of each number in a hash table, and then use a hash set to find the longest consecutive sequence. However, this would take two passes over the array, which is O(2n)...well O(n) in memory. However, why do this when we have a more efficient approach?	A brute force approach for this problem would involve iterating through each element in the array and then checking how far it can extend in both directions to form a consecutive sequence. For each element, the algorithm would check if the previous or next element is present in the array and keep track of the length of the consecutive sequence. The algorithm would return the maximum length of the consecutive sequence found. However, this approach would have a time complexity of O(n^3) as it involves nested loops. Therefore, it is not suitable for solving the problem within the required O(n) time complexity.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Longest%20Consecutive%20Sequence.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Longest%20Consecutive%20Sequence.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Longest%20Consecutive%20Sequence.java	https://youtu.be/KbSrxxLeY9o
Coding Interviews	Arrays: Ad-Hoc	Missing Number	https://leetcode.com/problems/missing-number/	Given an array nums containing n distinct numbers in the range [0, n], return the only number in the range that is missing from the array.	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	Hash Set	Boolean Array Approach	Gauss Formula	Sort-based Approach	C	Expensive in terms of memory.  The hash set stores the elements of the vector. We can then iterate over the sequence of numbers from 0 to n and check if each number is present in it: finally, we can return the first number that is not found.  Time Complexity: O(n), where n is the size of the vector.  Space Complexity: O(n), as it stores the elements in a hash set.	A little high in memory.  We create a boolean array of size n+1 and initialize each entry to false. Then, we iterate over the vector, and for each number, set the corresponding boolean element to true. Finally, we iterate over the boolean array and return the index of the first false element.  Time Complexity: O(n), where n is the size of the vector.  Space Complexity: O(n), as it creates a boolean array of size n+1.	Correct!  The Gauss formula is used to find the sum of a sequence of numbers - in this case, the sum of numbers from 0 to n. The sum formula is expressed as n*(n+1)/2.  The solution calculates both the expected sum using the Gauss Sum Formula, AND the actual sum of elements present in the vector. If the expected sum is equal to the actual sum, then no number is missing, and the function returns the size of the vector. Otherwise, it returns the missing number by subtracting the actual sum from the expected sum.  Time Complexity: O(n), where n is the size of the input vector.  Memory Complexity: O(1), as it uses constant space to store the expected sum, actual sum, and size of the vector.	Not optimal here.  This approach has a time complexity of O(n log(n)), where n is the size of the vector, due to the sorting process. The algorithm sorts the vector first and then iterates over it to find the missing number. This solution is suboptimal, as it takes longer to execute than the Gauss Formula solution.	The brute-force method? Iterate over the vector and check if each number from 0 to n is present in the vector, then return the first number that is not found.  This approach has a time complexity of O(n^2), where n is the size of the vector.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Missing%20Number.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Missing%20Number.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Missing%20Number.java	https://youtu.be/SnfjCLYPh9g
Coding Interviews	Arrays: Ad-Hoc	Trapping Rain Water	https://leetcode.com/problems/trapping-rain-water/	Given n non-negative integers representing an elevation map where the width of each bar is 1, compute how much water it can trap after raining.	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	Two Pointers	Greedy Approach	Stack	Dynamic Programming	A	Correct!  In this approach, the code first finds the highest bar's index by iterating over the array once. It then uses two pointers, one from the start and one from the end of the array, to compute the amount of water trapped.  The algorithm iterates from left to right for bars to the left of the maximum index and from right to left for bars to the right of the maximum index. At each iteration, the algorithm computes the maximum height of bars seen so far and uses it to calculate the water trapped. The amount of water trapped is the difference between the maximum height seen so far and the current height.  Time Complexity: O(n), where n is the size of the input array, as it makes two passes over the array. Memory Complexity: O(1), as it uses only a constant amount of extra space to store the variables used.	Too greedy!  In the greedy approach, we find the highest bar in the array and calculate the amount of water trapped to its left and right separately. Time complexity: O(n). Space complexity: O(1).  This approach is suboptimal because it assumes that the highest bar is the only one that matters when computing the amount of trapped water. This is not necessarily true, as there can be lower bars that form a barrier to water flow and trap more water.	Suboptimal for this problem. We would iterate over every bar in the array, and maintain a stack of bars that can potentially form a basin. When a bar is encountered that can form a basin with the previous bars, calculate the amount of water trapped and add it to the answer. Time complexity: O(n). Space complexity: O(n).  This approach is suboptimal because it requires additional space to maintain the stack, making it less memory efficient. Also, it assumes that the bars form a single basin, which is not necessarily true if there are multiple peaks in the elevation map.	This approach is suboptimal because it requires additional space to store the precomputed maximum heights, making it less memory efficient. Also, it takes longer to set up the precomputed values, which increases the constant factor in the time complexity.  The idea? Precompute the maximum height of bars to the left and right of each bar, and then calculate the amount of trapped water for each bar by taking the minimum of the maximum heights and subtracting the height of the bar. Time complexity: O(n). Space complexity: O(n).	The brute force solution involves iterating over every bar in the array, calculating the amount of water trapped between each bar and its adjacent bars, and adding them up. The time complexity is O(n^2), and the space complexity is O(1).	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Trapping%20Rain%20Water.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Trapping%20Rain%20Water.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Trapping%20Rain%20Water.java	https://youtu.be/EaPkkp_Qyn8
Coding Interviews	Arrays: Ad-Hoc	Build Array From Permutation	https://leetcode.com/problems/build-array-from-permutation/	Given a zero-based permutation nums (0-indexed), build an array ans of the same length where ans[i] = nums[nums[i]] for each 0 <= i < nums.length and return it.    A zero-based permutation nums is an array of distinct integers from 0 to nums.length - 1 (inclusive).	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	Linear Iteration + Swapping	Index Mapping	Hash Table	Sorting	B	This is not quite the right approach.  It would mean we iterate through the nums array and swap nums[i] and nums[nums[i]] until the array is fully sorted.  The weakness? This involves repeatedly swapping elements, which can be time-consuming and unnecessary. Additionally, it doesn't guarantee that the resulting array will be the correct answer.	Correct!  In this approach, for each index i, calculate the corresponding index nums[i], and store nums[i] in the i-th position of the output array ans. This can be done by adding nums[nums[i]]*n to the i-th position of nums, then dividing each element of nums by n to get the corresponding element of ans.  Time Complexity: O(n), where n is the size of the input array.  Memory Complexity: O(1) as the output array ans is not stored separately.	This less optimal approach uses a hash table to map each index i in nums to the value of nums[nums[i]]. Then, we'd iterate through the nums array and fill in ans[i] with the value mapped to by i in the hash table.  However the hash table takes up extra memory space and adds overhead to the algorithm. Additionally, iterating through the nums array and looking up values in the hash table can be slower than the optimal solution.	This approach is suboptimal because it involves sorting the nums array, which can be time-consuming and unnecessary. Additionally, it involves creating a new array, which takes up extra memory space.  That's the 'why?', here's the 'how?': sort the nums array and create a new empty array ans of length n. For each index i in nums, set ans[i] to nums[nums[i]]. Return ans.	The brute-force approach is to create a new empty array ans of length n. For each index i in nums, set ans[i] to nums[nums[i]], then return ans.  This approach is suboptimal because it involves creating a new array, which takes up extra memory space. Additionally, it involves iterating through the entire nums array twice, which can be slower than the optimal solution.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Build%20Array%20from%20Permutation.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Build%20Array%20from%20Permutation.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Build%20Array%20from%20Permutation.java	https://youtu.be/SYIAhTtcVAY
Coding Interviews	Arrays: Ad-Hoc	Next Permutation	https://leetcode.com/problems/next-permutation/	A permutation of an array of integers is an arrangement of its members into a sequence or linear order.    For example, for arr = [1,2,3], the following are all the permutations of arr: [1,2,3], [1,3,2], [2, 1, 3], [2, 3, 1], [3,1,2], [3,2,1].  The next permutation of an array of integers is the next lexicographically greater permutation of its integer. More formally, if all the permutations of the array are sorted in one container according to their lexicographical order, then the next permutation of that array is the permutation that follows it in the sorted container. If such arrangement is not possible, the array must be rearranged as the lowest possible order (i.e., sorted in ascending order).    For example, the next permutation of arr = [1,2,3] is [1,3,2].  Similarly, the next permutation of arr = [2,3,1] is [3,1,2].  While the next permutation of arr = [3,2,1] is [1,2,3] because [3,2,1] does not have a lexicographical larger rearrangement.  Given an array of integers nums, find the next permutation of nums.    The replacement must be in place and use only constant extra memory.	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	Sort + Swap	Next Permutation Algorithm	Recursive Approach	Sort Without Swapping	B	Incorrect!  This approach sorts the array in descending order, which isn't necessarily the next permutation. Moreover, the approach fails to find the correct next permutation in cases where there are repeated elements in the array.	Correct!  The method works by first finding the rightmost index i such that nums[i] < nums[i+1], indicating that a larger permutation is possible. Then, the algorithm finds the rightmost index j such that nums[j] > nums[i] and swaps the two values. Finally, it reverses the elements after i to produce the next lexicographically greater permutation.    Time Complexity: O(n), where n is the size of the input array  Space Complexity: O(1), as the algorithm performs the swaps and reversals in place without using any extra memory.	This approach finds all possible permutations of the array, which is an expensive operation and not required by the problem. It also doesn't fulfill the requirement of the problem, which is to implement the algorithm in place and using only constant extra memory.	This approach is incorrect because it doesn't actually find the next lexicographically greater permutation of the array, but rather returns the lowest possible permutation.	A brute force approach to finding the next permutation of an array is to generate all the permutations of the given array, sort them in lexicographical order, and find the next permutation of the given array in the sorted order. However, generating all permutations takes factorial time, which is not efficient for larger arrays.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Next%20Permutation.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Next%20Permutation.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Next%20Permutation.java	https://youtu.be/IyU4nHBoDu0
Coding Interviews	String	Valid Anagram	https://leetcode.com/problems/valid-anagram/	Given two strings s and t, return true if t is an anagram of s, and false otherwise.    An Anagram is a word or phrase formed by rearranging the letters of a different word or phrase, typically using all the original letters exactly once.    For this instance, let's say we want to include unicode characters - NOT restricting ourselves to lowercase English letters, as in the common version of this problem.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Sort Strings + Check	Count Arrays	Frequency Counting + Hash Tables	Hash Set	C	There are better approaches.  This particular approach requires sorting both strings, which takes O(n log(n)) time complexity in the worst case. Moreover, it does not use the concept of frequency, which means it may not work for certain cases.	A little high in memory complexity! The approach is simple: create two arrays of size 26 (one for s and one for t), representing the count of each letter in the strings. Check if the two arrays are equal.  Although this approach has the same time complexity as the Frequency Counter approach, it requires a fixed-size array of length 26, which means it can't handle strings with characters outside the range of 'a' to 'z'. Additionally, it uses more space than the Frequency Counter approach since the arrays are fixed-size, even if the input strings are small.	Correct!  In the Frequency Counter approach, a frequency table is constructed for each of the input strings, which counts the frequency of each character in the string. Then, the frequency tables of the two strings are compared to check whether they are anagrams. If the frequency tables match, then the strings are anagrams.  The solution first constructs two frequency tables (unordered_map) for both strings. Then, it checks whether the sizes of the two frequency tables are equal. If not, the strings cannot be anagrams. If the sizes are equal, then it compares the frequency of each character in the first frequency table with the corresponding frequency in the second frequency table. If any frequency does not match, the strings are not anagrams. If all frequencies match, the strings are anagrams.	Create a HashSet for s and remove each character in t from the set. If the set is empty, s and t are anagrams.  Although this approach can determine if two strings are anagrams, it requires extra space to store the HashSet. It also has a higher time complexity of O(n log(n)) compared to the Frequency Counter approach. Additionally, the HashSet approach cannot handle cases where a character appears multiple times in either string, while the Frequency Counter approach can handle these cases.	The brute-force approach here? For each character in s, check if it exists in t. If found, remove it from t. At the end, if t is empty, s and t are anagrams.  This approach has a time complexity of O(n^2), since it involves nested loops, and can be very slow for larger inputs.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Valid%20Anagram.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Valid%20Anagram.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Valid%20Anagram.java	https://youtu.be/djOYflPBd7c
Coding Interviews	String	Is Subsequence	https://leetcode.com/problems/is-subsequence/	Given two strings s and t, return true if s is a subsequence of t, or false otherwise.    A subsequence of a string is a new string that is formed from the original string by deleting some (can be none) of the characters without disturbing the relative positions of the remaining characters. (i.e., ace is a subsequence of abcde while aec is not).	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	Dynamic Programming	Sorting + Binary Search	Simple Linear Search	Two Pointers	D	Very inefficient.  The method? Construct a matrix dp of size (m+1)x(n+1), where m and n are the lengths of s and t, respectively. Initialize the first row and column with zeros. Then, for each i and j, if s[i-1]==t[j-1], set dp[i][j] = dp[i-1][j-1] + 1, otherwise set dp[i][j] = max(dp[i-1][j], dp[i][j-1]). If dp[m][n] == m, return true, otherwise return false.  This approach has a time complexity of O(mn), and it requires constructing a matrix of size O(mn), which may be memory-intensive. Moreover, this approach may not be optimal for very large inputs.	Sub-optimal.  While this approach has a time complexity of O(n log(n)), where n is the length of the longer string t, it requires sorting both strings, which has a time complexity of O(n log(n)) as well. Moreover, this approach doesn't preserve the order of characters in the original string, which violates the requirement that the relative order of characters in s must be preserved in t for s to be considered a subsequence of t.	Other approaches are much better.  The idea is to iterate over each character in s and search for it in t. If found, remove it from t and continue. If all characters in s are found, return true, otherwise return false.  This approach has a time complexity of O(n^2), where n is the length of the longer string t, because a linear search is performed for each character in s. Additionally, this approach modifies the input string t, which is not allowed in the problem statement.	Correct!  In this approach, two pointers are used to traverse both strings. The first pointer (ptr1) is used to traverse the shorter string s, while the second pointer (ptr2) is used to traverse the longer string t. At each step, the characters pointed to by ptr1 and ptr2 are compared. If they are equal, ptr1 and ptr2 are incremented, and the count of matching characters is incremented. If they are not equal, only ptr2 is incremented. The algorithm continues until ptr1 reaches the end of s, or ptr2 reaches the end of t.  Time Complexity: O(n), where n is the length of the longer string t, since each character in t is only visited once.  Space Complexity: O(1), as the only extra space used is for the pointers and the count variable.  Note that this approach assumes that the characters in both strings are in the same order as they appear in the original string, i.e., the relative order of characters in the shorter string s should be preserved in the longer string t for s to be considered a subsequence of t.	Our brute-force involves generating all possible subsequences of t and check if any of them match s.  This approach has an exponential time complexity of O(2^n), where n is the length of t, since there are 2^n possible subsequences of t. Therefore, this approach is impractical for all but the smallest inputs.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Is%20Subsequence.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Is%20Subsequence.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Is%20Subsequence.java	https://youtu.be/-13Vs3BOBak
Coding Interviews	String	Isomorphic Strings	https://leetcode.com/problems/isomorphic-strings/	Given two strings s and t, determine if they are isomorphic.    Two strings s and t are isomorphic if the characters in s can be replaced to get t.    All occurrences of a character must be replaced with another character while preserving the order of characters. No two characters may map to the same character, but a character may map to itself.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Canonical Mapping	Two Pointers	Sorting	Two Hash Tables	A	Correct!  The method first creates a canonical mapping of each character in the string, where each unique character is assigned a unique integer. Then, it compares the canonical mapping of both strings to check if they are equal. If the two strings have the same set of characters, they will have the same canonical mapping, and thus be isomorphic.  Time Complexity: O(n), where n is the length of the strings, as it iterates over the strings twice, and the unordered_map operations are constant time on average.  Space Complexity: O(n).	This is called the naïve approach for a reason.  Naive Two Pointer is suboptimal because it requires O(n^2) time complexity in the worst case, where n is the length of the input strings.  This approach involves using two pointers to traverse s and t simultaneously. At each step, the algorithm checks if the current character in s is replaced by the current character in t.	The sorting operation adds too much complexity to things.  This approach involves sorting the characters in s and t and then checking if the sorted strings are equal. If they are, the strings are isomorphic.	Quite high in space. We create two maps, one to store mappings from s to t, and another to store mappings from t to s. Iterate through each character in s and t, and update both maps accordingly. If either map already has an existing mapping for a character, and the mapping is not the same as the current one, return false. Otherwise, return true.  This approach does not handle the case where multiple characters in s or t map to the same character in the other string.	A brute force approach to solve this problem would be to check each pair of strings in both s and t to see if they are isomorphic or not. To do this, one can compare each character of s to the corresponding character of t and create a mapping of each character in s to the corresponding character in t. Then, for each character in s, check if it maps to the same character in t as it did before. If all characters in s and t are checked and the mappings are the same, then the strings are isomorphic. Highly inefficient as a solution!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Isomorphic%20Strings.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Isomorphic%20Strings.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Isomorphic%20Strings.java	https://youtu.be/eu5qLLoA7Rc
Coding Interviews	String	Verifying an Alien Dictionary	https://leetcode.com/problems/verifying-an-alien-dictionary/	In an alien language, surprisingly, they also use English lowercase letters, but possibly in a different order. The order of the alphabet is some permutation of lowercase letters.    Given a sequence of words written in the alien language, and the order of the alphabet, return true if and only if the given words are sorted lexicographically in this alien language.	O(nm)	O(k)	We're expecting an optimized solution of time complexity O(nm) and memory complexity O(k).  ***NOTE: This is really suboptimal for this problem. By far the best is actually to compare adjacent words. Let's say the interviewer demands an alternative approach (even if suboptimal). Which is best?****	Floyd-Warshall	Create New Strings For Each Word	Hash Table + Order-Based Comparison	Simple Comparison	C	If you selected this, it might be time to learn about the Floyd-Warshall algorithm, which is used as a shortest-paths algorithm.	This is not a great approach, even compared to the suboptimal 'best of the rest' answer.  Creating new strings for each word with the corresponding order values and then comparing those strings may not work because it ignores the order of the letters in the words.	Correct!  Our rather imperfect method first creates an hash table to store the order of the alphabet. It then defines a helper function to compare two words based on the order of the characters, and uses this function to compare adjacent words in the input vector to check if they are in the correct order.  The time complexity of the solution is O(nm), where n is the length of the input vector and m is the maximum length of a word in the vector. The space complexity is O(k), where k is the size of the hash table used to store the order of the alphabet.	Nested loops are rarely the answer. This approach is suboptimal because it takes O(n^2) time to compare all words with each other, which is much slower than the optimal solution.	A brute force approach to solving the Alien Dictionary problem would involve comparing each pair of adjacent words in the input list to determine if they are sorted correctly according to the given order of the alphabet.  First, we would construct a mapping between each letter in the alphabet and its corresponding position in the given order. Then, we would iterate through each pair of adjacent words in the input list and compare them character by character. If we find a pair of characters that differ, we would compare their positions in the alphabet using the mapping we constructed earlier. If the characters in the first word have a higher position in the alphabet than the characters in the second word, then the words are not sorted correctly, and we can return false immediately. If we have iterated through all pairs of words without finding any that are not sorted correctly, then we can return true.  While this approach is simple to understand, it has a time complexity of O(n^2), where n is the number of words in the input list, since we compare each pair of adjacent words. This makes it infeasible for large inputs.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Verifying%20an%20Alien%20Dictionary.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Verifying%20an%20Alien%20Dictionary.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Verifying%20an%20Alien%20Dictionary.java	https://youtu.be/1oNeNXqNNOA
Coding Interviews	String	Group Shifted Strings	https://leetcode.com/problems/group-shifted-strings/	We can shift a string by shifting each of its letters to its successive letter.    For example, abc can be shifted to be bcd.  We can keep shifting the string to form a sequence.    For example, we can keep shifting abc to form the sequence: abc -> bcd -> ... -> xyz.  Given an array of strings strings, group all strings[i] that belong to the same shifting sequence. You may return the answer in any order.	O(nm)	O(nm)	We're expecting an optimized solution of time complexity O(nm) and memory complexity O(nm).	Trie-based Grouping	Simple Brute-Force Comparison	Canoncialization + Hash Table	Simpson's Sort	C	Trie? Try again!  In this approach, the student might use a trie data structure to group together words that have the same set of characters. They could traverse the trie for each word and add it to the appropriate group. This approach would have a time complexity of O(nm) in the best case and O(nm^2) in the worst case, and a space complexity of O(nm) in the worst case.	Ouch!  This would have a time complexity of O(n^2 m), where n is the number of words and m is the maximum length of a word, and a space complexity of O(1).	Correct! The idea is to first convert all the strings to their canonical form using a simple shifting technique based on the first letter of the string. Then, we use a hash table to group together all the strings that have the same canonical form. Finally, we extract the second part of the hash table, which contains the grouped strings, and return it as a 2D vector.    The time complexity of the approach is actually O(nm), where n is the number of strings and m is the length of the longest string, as we need to iterate over all the strings and perform a single pass over each string to convert it to its canonical form. The space complexity is O(nm), as we need to store the hash table and the canonical form of each string.	D'oh! Simpson, eh? His sort wouldn't be very useful for this problem...	A brute force approach to solving this problem would be to iterate through each string in the array and then iterate through all the other strings to check if they belong to the same shifting sequence. This can be done by shifting each character in the string by the same amount and checking if the resulting string exists in the array. If a matching string is found, the two strings belong to the same shifting sequence and can be grouped together. This process is repeated for all strings in the array to find all the shifting sequences. The time complexity of this approach would be O(n^2 * k), where n is the length of the array and k is the length of the longest string, since we need to shift each character in each string and check if the resulting string exists in the array. The space complexity would also be O(n) to store the resulting groups of strings.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Group%20Shifted%20Strings.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Group%20Shifted%20Strings.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Group%20Shifted%20Strings.java	https://youtu.be/XRRQTtsw12g
Coding Interviews	String	Number of Matching Subsequences	https://leetcode.com/problems/number-of-matching-subsequences/	Given a string s and an array of strings words, return the number of words[i] that is a subsequence of s.    A subsequence of a string is a new string generated from the original string with some characters (can be none) deleted without changing the relative order of the remaining characters.    For example, ace is a subsequence of abcde.	O(n*m)	O(n+m)	We're expecting an optimized solution of time complexity O(nm) and memory complexity of O(n+m) (n being the length of the string, m being the number of words in the given vector) ****NOTE: there are a few *better* answers in terms of efficiency, such as using a Trie here****	Character Indexing + Map + Vector	Regex	Greedy Algorithm	Sorting + Binary Search	A	Correct!  Our suggested solution uses an hash table to store the indices of the words starting with each character in the string s.  Time Complexity: O(nm), where n is the length of the string s and m is the total length of all the words in the vector words.  However, in practice, the time complexity is expected to be lower, especially when there are many distinct characters in the string s compared to the number of words in the vector words.  Space Complexity: O(n+m).	Irregular.  One could use regular expressions to find the matching subsequences of the given string s in the vector words. While regular expressions can be useful for pattern matching, they can be slow and memory-intensive for large inputs.	Too greedy!  This would involve using some kind of greedy algorithm to find the matching subsequences of the given string s in the vector words. This approach involves selecting the longest matching subsequence at each step, which may not always lead to the optimal solution. This approach may work well for some inputs but can fail for others.	Not really viable here.  This approach involves sorting the vector words and using binary search to find the subsequences in the sorted vector. The time complexity of this approach would be O(m log(m) + n log(n)), where n is the length of the string s and m is the total length of all the words in the vector words. However, binary search can only find exact matches, so this approach may not work if the words in the vector words are not exact subsequences of the given string.	A simple brute force approach to solve this problem would be to iterate through each word in the array of words and check if it is a subsequence of the given string s. To check if a word is a subsequence of s, we can iterate through each character of the word and check if it exists in s, maintaining the order of characters. We can keep a count of the number of words that are subsequences of s and return the count at the end.  The time complexity of this approach is O(n*m), where n is the length of the array of words and m is the length of the given string s. The space complexity is O(1), as we are not using any extra space.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Number%20of%20Matching%20Subsequences.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Number%20of%20Matching%20Subsequences.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Number%20of%20Matching%20Subsequences.java	https://youtu.be/--k-P_PfAmA
Coding Interviews	Matrix	Search a 2D Matrix II	https://leetcode.com/problems/search-a-2d-matrix-ii/	Write an efficient algorithm that searches for a value target in an m x n integer matrix matrix. This matrix has the following properties:    Integers in each row are sorted in ascending from left to right.  Integers in each column are sorted in ascending from top to bottom.	O(m+n)	O(1)	We're expecting an optimized solution of time complexity O(m+n) and memory complexity O(1).	Convert to Array	Search Space Reduction/Binary Search	Row-Wise Binary Search	Column-Wise Binary Search	B	Conversions to arrays rarely work well.  Here, we'd convert the 2D matrix to a 1D array, perform a binary search to find the target value.  While this approach may work, it has a space complexity of O(mn) because it requires creating a new array to store all the elements of the matrix. This is inefficient and unnecessary for the given problem.	Correct! AKA Binary Search on 2D Matrix!  The method is based on starting at the top-right corner of the matrix (or bottom-left corner) and comparing the value of the element with the target value. If the target is greater than the current element, then it is guaranteed that the target value will not be present in that entire row because all elements in that row are less than the current element. Therefore, we move to the next row by incrementing the row index. If the target is less than the current element, then we can move to the left column by decrementing the column index, because all elements in that column are greater than the current element.  We repeat this process until either we find the target element or we reach the edge of the matrix.  Time Complexity: O(m+n), where m and n are the number of rows and columns in the matrix, respectively.  Space Complexity: O(1), as the algorithm only requires a constant amount of extra space to store the current row and column indices.	Quite expensive!  In this, we perform a binary search on each row of the matrix to find the target value.  While this approach does reduce the search space, it still has a time complexity of O(m log(n)) due to performing binary search on each row, which is less efficient than the optimal solution.	A little slow.  Perform a binary search on each column of the matrix to find the target value.  Similar to approach B, this approach still has a time complexity of O(n log(m)) due to performing binary search on each column, which is also less efficient than the optimal solution.	The brute-force here? Iterate through every element of the matrix and check if it is equal to the target value.  This approach has a time complexity of O(mn), which is inefficient for large matrices.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Search%20a%202D%20Matrix%20II.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Search%20a%202D%20Matrix%20II.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Search%20a%202D%20Matrix%20II.java	https://youtu.be/tT0QehcATj8
Coding Interviews	Matrix	Rotate Image	https://leetcode.com/problems/rotate-image/	You are given an n x n 2D matrix representing an image, rotate the image by 90 degrees (clockwise).    You have to rotate the image in-place, which means you have to modify the input 2D matrix directly. DO NOT allocate another 2D matrix and do the rotation.	O(n^2)	O(1)	We're expecting an optimized solution of time complexity O(n^2) and memory complexity O(1).	Transpose + Reflect Matrix	Layer by Layer Rotation	Rotate With Simple Swap Operations	Flatten Into Array + Rotate	A	Correct!  First, the transpose operation swaps the elements along the main diagonal, i.e., matrix[i][j] is swapped with matrix[j][i]. Then, the reflection operation swaps the columns of the matrix, so the first column becomes the last column, the second column becomes the second last column, and so on.  By applying these two operations in sequence, you can achieve a 90-degree clockwise rotation of the matrix.  Time Complexity: O(n^2), where n is the size of the matrix. The transpose operation takes O(n^2) time to perform, and the reflection operation takes O(n^2) time as well.  Memory Complexity: O(1) (it's an in-place algorithm!)	This approach is incorrect because it requires additional memory to store the layers being rotated and is more complicated and time-consuming compared to the given approach.	Incorrect!  This only rotates the matrix by 180 degrees, not 90 degrees.	Incorrect!  It requires additional memory to store the one-dimensional array, which is not allowed according to the problem requirements. Additionally, it is less efficient than the given approach since it involves extra copying and conversion steps.	In the brute-force approach, you create a new matrix and copy elements from the original matrix to the new matrix in the rotated positions.  This approach is incorrect because it requires additional memory to store the new matrix, which is not allowed according to the problem requirements.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Rotate%20Image.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Rotate%20Image.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Rotate%20Image.java	https://youtu.be/5_mbB30CWmA
Coding Interviews	Stack	Daily Temperatures	https://leetcode.com/problems/daily-temperatures/	Given an array of integers temperatures represents the daily temperatures, return an array answer such that answer[i] is the number of days you have to wait after the ith day to get a warmer temperature. If there is no future day for which this is possible, keep answer[i] == 0 instead.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Binary Search	Dynamic Programming	Two Pointers	Monotonic Stack	D	Not quite correct.  For each element in the array, we perform a binary search on the rest of the array to find the next warmer day.   Time complexity: O(n log(n)).  Although binary search has a better time complexity than brute force, it is still not as optimal as the Monotonic Stack approach. Also, the array is not sorted, which makes it difficult to perform binary search on the rest of the array.	Close, but not quite as good as a monotonic stack.  Here, we'd traverse the array in reverse order and keep track of the minimum temperature seen so far. Calculate the difference between the current temperature and the minimum temperature, and update the result array accordingly.  Time complexity: O(n).  Unlike the Monotonic Stack approach, it requires traversing the array in reverse order, which may not be intuitive or straightforward.	Suboptimal here.  Use two pointers to traverse the array. For each element, move the left pointer to the next warmer day and update the result array. Although this approach also has a time complexity of O(n), it does not take advantage of the monotonicity of the temperatures and may not handle certain edge cases properly.	Correct!  This famous method involves using a stack to keep track of the indices of the temperatures that are still waiting for a warmer day. We iterate through the array from left to right and for each temperature, we check if it is warmer than the temperature at the top of the stack. If it is, then we can update the result array with the difference between the current index and the index at the top of the stack, indicating the number of days to wait for a warmer temperature. We repeat this process until we find a temperature that is not warmer than the temperature at the top of the stack, or until the stack is empty.  The stack stores indices in non-increasing order of temperature. This maintains the monotonicity of the stack, ensuring that the temperatures are always decreasing or constant. Whenever we find a temperature that is warmer than the temperature at the top of the stack, we pop elements from the stack until we find a temperature that is greater than the current temperature.  Time Complexity: O(n).  Memroy Complexity: O(n).	Brute Force: for each element in the array, iterate through the rest of the array to find the next warmer day. Time complexity: O(n^2).  This approach is inefficient because it involves nested loops, resulting in a time complexity of O(n^2), which is not optimal for large input sizes.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Daily%20Temperatures.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Daily%20Temperatures.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Daily%20Temperatures.java	https://youtu.be/9D-ulWnn1ws
Coding Interviews	Stack	Next Greater Element II	https://leetcode.com/problems/next-greater-element-ii/	Given a circular integer array nums (i.e., the next element of nums[nums.length - 1] is nums[0]), return the next greater number for every element in nums.    The next greater number of a number x is the first greater number to its traversing-order next in the array, which means you could search circularly to find its next greater number. If it doesn't exist, return -1 for this number.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Hash Table	Sorting	Circular Monotonic Stack	Ingenious Nested Loops	C	A little less optimal than other approaches.  This approach uses a hash table to store the next greater element for each element. However, this approach requires iterating over the array multiple times and updating the hash table, which makes it inefficient. Additionally, this approach requires extra memory to store the hash table.	Inefficient.  This approach first sorts the array and then iterates over it to find the next greater element for each element.  Time Complexity: O(n log(n)) due to sorting.	Correct!  The approach used in the given code is called Circular Stack or Circular Monotonic Stack. The algorithm uses a stack to store the indices of elements in the circular array, such that the elements are in non-increasing order from the bottom to the top of the stack. The algorithm traverses the circular array twice and for each element, it pops the elements from the stack until it finds an element greater than the current element. If such an element is found, it is assigned as the next greater element for the top element of the stack. If not found, the top element of the stack is assigned -1. Finally, the algorithm returns the resultant array.  Time Complexity: O(n) as it traverses the circular array twice.  Space Complexity: O(n) because the size of the stack can go up to n.	There's nothing clever about this!  This approach uses two nested loops to iterate over the array and find the next greater element for each element.  Time Complexity: O(n^2) - inefficient for large input sizes.	The brute-force approach iterates over the array for each element and finds the next greater element by iterating from the next element in a circular manner. This approach has a time complexity of O(n^2) and is therefore inefficient for large input sizes.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Next%20Greater%20Element%20II.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Next%20Greater%20Element%20II.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Next%20Greater%20Element%20II.java	https://youtu.be/MzsHULrVrHo
Coding Interviews	Matrix	Range Sum Query 2D	https://leetcode.com/problems/range-sum-query-2d-immutable/	Given a 2D matrix matrix, handle multiple queries of the following type:    Calculate the sum of the elements of matrix inside the rectangle defined by its upper left corner (row1, col1) and lower right corner (row2, col2).  Implement the NumMatrix class:    NumMatrix(int[][] matrix) Initializes the object with the integer matrix matrix.  int sumRegion(int row1, int col1, int row2, int col2) Returns the sum of the elements of matrix inside the rectangle defined by its upper left corner (row1, col1) and lower right corner (row2, col2).  You must design an algorithm where sumRegion works on O(1) time complexity.	O(1) for the sumRegion calculation and O(mn) for the constructor	O(mn)	We're expecting an optimized solution of time complexity O(1) for the sumRegion calculation, O(mn) for the constructor, along with an overall memory complexity of O(mn).	Prefix Sum By Row	Prefix Sum By Column	2D Prefix Sum	Pre-compute All Submatrix Sums	C	This is not best here.  Store prefix sum for each row: For each row, create an array of prefix sum and use it to calculate the sum of any subrectangle that has that row as the upper side. To calculate the sum for the whole rectangle, iterate over all rows and calculate their contributions.   Time complexity: O(mn), and O(m) for each query.  This approach is inefficient since it requires iterating over all rows for each query.	Not the ideal solution.  Store prefix sum for each column: For each column, create an array of prefix sum and use it to calculate the sum of any subrectangle that has that column as the left side. To calculate the sum for the whole rectangle, iterate over all columns and calculate their contributions. Time complexity for initialization is O(mn) and for each query is O(n).  This approach is inefficient since it requires iterating over all columns for each query.	Correct!   Known as 2D Prefix Sum or 2D Cumulative Sum, this involves calculating the cumulative sum of all elements in the 2D matrix both horizontally and vertically.  To achieve O(1) time complexity for calculating the sum of the elements of the matrix inside the given rectangle, the cumulative sum of each cell can be used. By calculating the sum of the cells at the four corners of the rectangle and subtracting the cumulative sums of the cells outside the rectangle, the sum of the elements inside the rectangle can be obtained in constant time.  The time complexity of the constructor is O(mn) where m and n are the dimensions of the matrix, since it involves calculating the cumulative sum of each cell. The time complexity of sumRegion is O(1), since it only involves simple arithmetic operations. The space complexity is also O(mn), as the cumulative sums are stored in a 2D vector.	This is a little suboptimal.  We'd pre-compute all possible submatrices sums: Create a 2D array of size (m+1)x(n+1) where each element stores the sum of the submatrix from (0,0) to (i,j). To calculate the sum of a rectangle, use the formula sum = dp[row2+1][col2+1] - dp[row1][col2+1] - dp[row2+1][col1] + dp[row1][col1]. Time complexity for initialization is O(mn^2) and for each query is O(1).  This approach has high initialization time complexity and also requires a lot of extra memory.	Brute force: Iterate over all elements inside the given rectangle and calculate their sum. Time complexity is O(mn) where m is the number of rows and n is the number of columns.  This approach is inefficient since it calculates the sum for the same elements multiple times.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Range%20Sum%20Query%202D%20-%20Immutable.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Range%20Sum%20Query%202D%20-%20Immutable.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Range%20Sum%20Query%202D%20-%20Immutable.java	https://youtu.be/B5NVAeuGl5E
Coding Interviews	Matrix	Sparse Matrix Multiplication	https://leetcode.com/problems/sparse-matrix-multiplication/	Given two sparse matrices mat1 of size m x k and mat2 of size k x n, return the result of mat1 x mat2. You may assume that multiplication is always possible.	O(mnk)	O(mn)	We're expecting an optimized solution of time complexity O(mnk) and memory complexity O(mn).	Dense Approach: Convert into Dense Matrices	Strassen's Algorithm for Matrix Multiplication	Parallelization	Sparse Matrix Multiplication using Compressed Row Storage (CRS)	D	Not a good idea - a little 'dense' perhaps?  This approach is suboptimal because it does not exploit the sparsity of the input matrices, and it can be very inefficient in terms of time and space complexity for large sparse matrices.	You've struck out with Strassen, unfortunately.  While Strassen's algorithm is a fast algorithm for dense matrix multiplication, it is not optimized for sparse matrices. In fact, Strassen's algorithm can perform worse than the standard dense matrix multiplication for sparse matrices due to the overhead of recursion.	Suboptimal for this problem.  While parallelization can speed up matrix multiplication for dense matrices, it may not provide significant benefits for sparse matrices. In addition, parallelization can introduce additional overhead and synchronization costs that may outweigh the benefits for small to medium-sized matrices.	Correct!  The CRS algorithm is based on the observation that if the two matrices mat1 and mat2 are sparse, then most of the entries of the product mat1 x mat2 will be zero. Therefore, instead of performing a full matrix multiplication, we can focus only on the non-zero entries of mat1 and mat2, and compute only the non-zero entries of the product.  CRS stores the non-zero entries of mat1 in compressed row format, and the non-zero entries of mat2 in uncompressed column format. The algorithm then iterates over the rows of mat1 and multiplies each non-zero entry with the corresponding non-zero entries of the columns of mat2. Since mat2 is stored in column format, it can be accessed efficiently using pointer chasing.  Time Complexity: O(mnk).  Space Complexity: O(mn).	A brute-force approach multiplies each non-zero element of mat1 with each non-zero element of mat2.  This approach is highly inefficient since it requires O(mnk) operations to multiply two sparse matrices. This is much worse than the CRS algorithm which only requires O(nnz(mat1)+nnz(mat2))) operations, where nnz denotes the number of non-zero entries in the matrix.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Sparse%20Matrix%20Multiplication.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Sparse%20Matrix%20Multiplication.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Sparse%20Matrix%20Multiplication.java	https://youtu.be/OfNAOohcRUc
Coding Interviews	Stack	Min Stack	https://leetcode.com/problems/min-stack/	Design a stack that supports push, pop, top, and retrieving the minimum element in constant time.    Implement the MinStack class:    MinStack() initializes the stack object.  void push(int val) pushes the element val onto the stack.  void pop() removes the element on the top of the stack.  int top() gets the top element of the stack.  int getMin() retrieves the minimum element in the stack.  You must implement a solution with O(1) time complexity for each function.	O(1)	O(n)	We're expecting an optimized solution of time complexity O(1) and memory complexity O(n).	Two Stacks	Single Stack + Linear Search	Linked List	Stack + Hash Table	A	Correct!  As the name suggests, in this approach, two stacks are used: one stack to store the actual elements of the stack, and another stack to keep track of the minimum element at each stage of the stack. Whenever a new element is pushed onto the stack, if the minimum stack is empty or if the new element is less than or equal to the current minimum element, the new element is pushed onto the minimum stack as well. Similarly, when an element is popped from the stack, if it is the current minimum element, it is also popped from the minimum stack.  Time Complexity: O(1) for all four operations, push, pop, top, and getMin.  Memory Complexity: O(n), where n is the number of elements in the stack, as we are using two stacks to store the elements and minimums separately.	Incorrect for this question.  Uses a single stack and keep track of the minimum element by iterating over the entire stack every time getMin is called.  This approach is suboptimal because it requires O(n) time complexity for the getMin operation, which does not meet the requirement of O(1) time complexity.	Not ideal.  This uses a linked list to implement the stack, with each node storing the element and the minimum element up to that node.  This approach is suboptimal because it requires additional memory for each node, leading to higher memory usage compared to the Two-Stack Approach. Additionally, the push, pop, and top operations would still require O(1) time complexity, but the getMin operation would require iterating over the linked list, leading to slower performance compared to the Two-Stack Approach.	Rather expensive.  In this, we maintain an auxiliary data structure (like a hash table) to store the minimum element for every push operation.  This approach works, but it requires an additional data structure, which takes up extra space. Also, updating the auxiliary data structure on every push and pop operation can be time-consuming, making it less efficient than the Two-Stack Approach.	A brute force approach to solving this problem would be to maintain a single stack to store all the elements and an additional variable to store the minimum element. On every push operation, compare the new element with the minimum element, and if the new element is smaller, update the minimum variable. Similarly, on every pop operation, check if the popped element is the minimum element and update the minimum variable accordingly.  For getMin() operation, simply return the minimum variable.  However, this approach is not optimal as it requires additional comparisons on every push and pop operation, which can increase the time complexity beyond O(1) and also the minimum variable can become invalid if the minimum element is popped.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Min%20Stack.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Min%20Stack.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Min%20Stack.java	https://youtu.be/LhIET3nkkBQ
Coding Interviews	Stack	Simplify Path	https://leetcode.com/problems/simplify-path/	Given a string path, which is an absolute path (starting with a slash '/') to a file or directory in a Unix-style file system, convert it to the simplified canonical path.    In a Unix-style file system, a period '.' refers to the current directory, a double period '..' refers to the directory up a level, and any multiple consecutive slashes (i.e. '//') are treated as a single slash '/'. For this problem, any other format of periods such as '...' are treated as file/directory names.    The canonical path should have the following format:    The path starts with a single slash '/'.  Any two directories are separated by a single slash '/'.  The path does not end with a trailing '/'.  The path only contains the directories on the path from the root directory to the target file or directory (i.e., no period '.' or double period '..')  Return the simplified canonical path.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Regex	Stack	Two Linear Scans	Recursive Approach	B	Not the 'regular' approach - nor the best of the options here.  Regular expressions can match and remove segments in the input path. While true, and this approach would have a time complexity of O(n), it could be difficult to implement and may not be as efficient as the 'stack'-based approach. (Our approach utilised a vector which behaves like a stack).	Correct!  We used a vector that 'behaves' like a stack in our approach! We used a function (name 'split') to take the input along with a separator character (defaulting to '/'), which returns a vector of strings representing the substrings between the separators. The function simplifyPath uses split to ummm...split the input path into tokens, and then uses a stack to keep track of the current path as it processes each token. If a token is empty or ., it is ignored. If a token is .., the function pops the last element from the stack (if there is one). Otherwise, the token is pushed onto the stack. Finally, if the stack is empty, the function returns /, otherwise it concatenates the elements of the stack with / and returns the resulting string.  Time Complexity: O(n).  Memory Complexity: O(n).	Rather expensive.  We'd store segments in a collection after the first pass, and then simplify each segment in a second pass.  This approach would have a time complexity of O(n) in the best case, but could potentially have a worst case time complexity of O(n^2) if the input path contains many unnecessary segments.	So, you want to call the simplifyPath function recursively on each segment of the input path, and then combine the results? It's really the worst idea here, for many reasons! This approach would have a time complexity of O(2^n) in the worst case, where n is the number of segments in the input path.	A brute force approach to solving this problem would be to first split the input string path by slashes to get all the directories and file names. Then, loop through each directory and file name and build a new path by appending each directory and file name to the previous directory, while checking for any special cases such as periods or double periods.  For example, if the input path is /a//b/c/.././d, we can split it into [,a,,b,c,..,.,d]. We can then loop through this array and build the simplified canonical path by appending each directory and file name to the previous directory, while checking for special cases. So, the simplified canonical path would be /a/b/d.  However, this approach is not optimal, as it requires additional comparisons and loops to check for special cases and append each directory and file name, which can increase the time complexity beyond the required O(n) time complexity.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Simplify%20Path.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Simplify%20Path.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Simplify%20Path.java	https://youtu.be/5bD5dZBP6Qk
Coding Interviews	Stack	Largest Rectangle in Histogram	https://leetcode.com/problems/largest-rectangle-in-histogram/	Given an array of integers heights representing the histogram's bar height where the width of each bar is 1, return the area of the largest rectangle in the histogram.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Divide and Conquer	Dynamic Programming	Sorting + Comparing	Monotonic Stack Algorithm	D	Too complicated.  In D&C, we'd divide the histogram into two halves and find the maximum area for each half recursively.  This approach has a time complexity of O(n log(n)) in the worst case, but it requires extra implementation complexity and may not be as intuitive for some programmers. Additionally, it may not perform well in cases where the maximum area involves bars that span both halves of the histogram.	Quite slow.  Method: Use dynamic programming where dp[i] represents the maximum area that can be formed using bars from 0 to i. For each i, we'd iterate backwards from i to 0 to find the leftmost bar j such that heights[j] < heights[i] and find the rightmost bar k such that heights[k] < heights[i]. The maximum area for bar i is (k - j - 1) * heights[i] and the maximum area that can be formed using bars from 0 to i is the maximum of dp[i-1] and the area for bar i. Finally, we update dp[i] with the maximum area found so far.  Time Complexity: O(n^2) in the worst case, which will time out on large inputs.	Quite expensive in terms of time and memory.  Method: Sort the histogram in non-decreasing order of heights. Iterate through the sorted histogram, for each bar i, find the leftmost bar j such that heights[j] < heights[i] and find the rightmost bar k such that heights[k] < heights[i]. The maximum area for bar i is (k - j - 1) * heights[i]. Update the maximum area found so far.  Time Complexity: O(n^2) in the worst case.	Correct!  This algorithm first calculates the next smaller element and previous smaller element for each element in the given array using a stack data structure. The next smaller element for an element is the closest element on the right that is smaller than it, and the previous smaller element is the closest element on the left that is smaller than it. After getting these two arrays, we iterate through the array and calculate the area of the rectangle for each element using the formula height * (right - left + 1), where height is the height of the current element, left is the index of the previous smaller element + 1, and right is the index of the next smaller element - 1. Finally, the algorithm returns the maximum area calculated.  Time Complexity: O(n), where n is the size of the input array.  Space Complexity: O(n),	A brute force approach for this problem would be to consider all possible rectangles in the histogram and calculate the area of each one.  To do this, we can use two nested loops where the outer loop will iterate through each bar in the histogram, and the inner loop will iterate from the current bar to the end of the histogram.  For each pair of bars (i, j) where i <= j, we can calculate the area of the rectangle formed by these bars by multiplying the minimum height between them by the width (j-i+1). We can keep track of the maximum area we find as we iterate through the histogram.  The time complexity of this approach is O(n^2), where n is the number of bars in the histogram. The space complexity is O(1) as we are not using any extra space besides the input histogram array.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Largest%20Rectangle%20in%20Histogram.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Largest%20Rectangle%20in%20Histogram.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Largest%20Rectangle%20in%20Histogram.java	https://youtu.be/0T-bkV_5OhA
Coding Interviews	Stack	Maximal Rectangle	https://leetcode.com/problems/maximal-rectangle/	Given a rows x cols binary matrix filled with 0's and 1's, find the largest rectangle containing only 1's and return its area.	O(nm)	O(nm)	We're expecting an optimized solution of time complexity O(nm) and memory complexity O(nm).	DFS	Prefix Sum + Stacks	Dynamic Programming	Recursive Approach	B	Yikes!  This approach would also have a horrible time complexity, because there are O(n^2) cells in the matrix and each DFS takes O(n^2) time in the worst case. Additionally, this approach would use O(n^2) space for the visited array.	Correct!  The solution first converts the input binary matrix to a 2D vector of integers, where each cell stores the length of the maximum consecutive 1's in the same column up to that row. This is achieved by iterating over each column, and for each row, incrementing the value in the cell if it is a 1 and setting it to 0 otherwise. This operation is similar to computing a prefix sum, but for each column instead of each row.  Next, for each row of the modified 2D vector, the largest rectangle containing only 1's is computed using a stack-based approach with next/previous smaller element (NSE/PSE) arrays. NSE and PSE are precomputed for each row, where NSE stores the index of the next smaller element on the right, and PSE stores the index of the next smaller element on the left. These arrays are computed by iterating over the heights vector and maintaining a stack of increasing elements. When a smaller element is encountered, the index of the current element is stored as the next smaller element for the top element of the stack. This allows for efficient computation of the maximum area of a rectangle containing a particular height.  For each row of the modified 2D vector, the largest rectangle containing only 1's is computed by iterating over the heights vector and computing the area of the rectangle with the current height using the NSE/PSE arrays. The maximum area across all rows is then returned as the solution to the problem.	Incorrect approach.  DP computes the area of the largest rectangle containing only 1's that ends at each cell in the matrix, returning the maximum area found.  This approach is incorrect because it does not take into account the fact that the largest rectangle may not end at each cell. It is possible for a larger rectangle to include cells that were not included in any of the previous rectangles computed.	Inefficient, as this would require a lot of repeated computation.  The basic idea is to use a recursive approach where for each cell containing a 1, calculate the largest rectangle that can be formed with that cell as the top-left corner. We then keep track of the maximum area seen so far and return it.  Time Complexity: O(rows^2 * cols^2), which is very slow for larger inputs.	Brute-force here would mean we iterate over all submatrices and check if each one contains only 1's. We then return the area of the largest rectangle found.  This approach would have a time complexity of O(n^6) where n is the length of the matrix. This is because there are O(n^4) possible submatrices and each one takes O(n^2) time to check. This approach would be too slow for large matrices.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Maximal%20Rectangle.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Maximal%20Rectangle.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Maximal%20Rectangle.java	https://youtu.be/hJZkxUqzGfQ
Coding Interviews	Stack	Largest Submatrix With Rearrangements	https://leetcode.com/problems/largest-submatrix-with-rearrangements/	You are given a binary matrix matrix of size m x n, and you are allowed to rearrange the columns of the matrix in any order.    Return the area of the largest submatrix within matrix where every element of the submatrix is 1 after reordering the columns optimally.	O(m*n*log(n))	O(n)	We're expecting an optimized solution of time complexity O(m*n*log(n)) and memory complexity O(n).	Greedy Rearrangement	Row Sorting + Histogram	Column Sorting + Histogram	Random Rearrangement	C	Not quite right.  We could greedily rearrange the columns by first moving columns with the most ones to the left, followed by columns with fewer ones. This approach is incorrect because it may not always result in the optimal rearrangement of the columns.	This approach is incorrect because sorting the rows will not cluster the ones together in a way that helps us compute the maximum area of a submatrix.	Correct!  The idea is to first compute the column-wise prefix sums of the binary matrix. Then, for each row in the matrix, sort the columns in ascending order of their prefix sums. This is essentially rearranging the columns optimally such that the ones are clustered together. Finally, we can compute the maximum area of a histogram for each row of the sorted matrix using the Next Smaller Index and Previous Smaller Index methods.  Time Complexity: O(m * n * log(n)), where m is the number of rows and n is the number of columns - and we need to sort each row of the matrix, which takes O(n * log(n)) time.  Space Complexity: O(n), which is the size of the vector used to store the column-wise prefix sums.	Hmmm…this approach is incorrect because it may not always find the optimal arrangement of the columns, and may require a large number of random rearrangements to converge to the optimal solution.	In this, the brute-force could enumerate all possible submatrices of the binary matrix and check if each submatrix is made up entirely of ones. This approach is incorrect because the time complexity of this algorithm is O(m^2 * n^2), which is too high for matrices with large dimensions.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Largest%20Submatrix%20With%20Rearrangements.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Largest%20Submatrix%20With%20Rearrangements.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Largest%20Submatrix%20With%20Rearrangements.java	https://youtu.be/NW9Bma1TuWQ
Coding Interviews	Stack	Maximal Square	https://leetcode.com/problems/maximal-square/	Given an m x n binary matrix filled with 0's and 1's, find the largest square containing only 1's and return its area.	O(nm)	O(nm)	We're expecting an optimized solution of time complexity O(nm) and memory complexity O(nm).	DFS or BFS	Dynamic Programming or Stack	Bogan's Brute Force or Failure	Greedy	B	Seriously poor time complexity.   The idea? Find all squares that contain only 1's. Start at each cell containing a 1, and explore all adjacent cells to see if they form a square.   Time Complexity: O(mn * 3^(k/2)), where k is the size of the largest square in the matrix.	Correct! Although, we show a stack-based approach here.   This technique is a common approach to solving matrix-related problems, and it can be used to solve various problems such as maximum sum submatrix, longest common submatrix, etc...   We just calculate the maximum area of a square that ends at each position in the matrix.	Too naïve - and failure is not an option!   The time complexity of this approach would be O(m^2 * n^2), since you would need to check each cell and then potentially expand a square of size O(min(m, n)) in all four directions.	Don't be greedy (with your approach) here!   Start at the top left corner and greedily expand a square as far as possible to the right and downwards. Once you hit a zero, move to the next row and start again from the leftmost column. Keep track of the largest square at each point.   Time Complexity: O(nm)…in the best case! In the worst case (e.g. a matrix that is comprised entirely of 1's) it could be as bad as O(m^2n)!	A brute force approach to solving this problem would be to iterate through all possible squares in the matrix and check if they are all filled with 1's. This would involve iterating through each cell in the matrix and checking if the cell and its neighbors form a square filled with 1's. If a square is found, its area is compared to the current maximum area and updated accordingly. This approach has a time complexity of O(m^3*n^3) and space complexity of O(1), making it inefficient for large matrices.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Maximal%20Square.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Maximal%20Square.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Maximal%20Square.java	https://youtu.be/qRJO-O2pho0
Coding Interviews	Stack	Exclusive Time of Functions	https://leetcode.com/problems/exclusive-time-of-functions/	On a single-threaded CPU, we execute a program containing n functions. Each function has a unique ID between 0 and n-1.    Function calls are stored in a call stack: when a function call starts, its ID is pushed onto the stack, and when a function call ends, its ID is popped off the stack. The function whose ID is at the top of the stack is the current function being executed. Each time a function starts or ends, we write a log with the ID, whether it started or ended, and the timestamp.    You are given a list logs, where logs[i] represents the ith log(m)essage formatted as a string {function_id}:{start | end}:{timestamp}. For example, 0:start:3 means a function call with function ID 0 started at the beginning of timestamp 3, and 1:end:2 means a function call with function ID 1 ended at the end of timestamp 2. Note that a function can be called multiple times, possibly recursively.    A function's exclusive time is the sum of execution times for all function calls in the program. For example, if a function is called twice, one call executing for 2 time units and another call executing for 1 time unit, the exclusive time is 2 + 1 = 3.    Return the exclusive time of each function in an array, where the value at the ith index represents the exclusive time for the function with ID i.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Recursive	Stack	Hash Table	Brute Force Approach	B	A little inefficient!  Method: use recursion to traverse the call stack, computing the exclusive time for each function.  Weakness: This is inefficient because it requires multiple traversals of the call stack, which increases the time complexity. Recursion is neither optimal nor necessary when solving this problem.	Correct!  Our solution uses a stack to keep track of the function calls in a log.  Method: for each log(m)essage, we can parse out and extract three things: the function ID, whether it is a 'start' or 'end' call, and the timestamp. If it's a start call, then a new entry is pushed onto the stack. Otherwise, the top entry is popped from the stack, and the total execution time is calculated. The exclusive execution time for the current function is calculated by subtracting the sub-call time of the current function from the total execution time. The sub-call time is the total execution time of all child functions that were called within the current function. The exclusive execution time is added to the result array for the ID that we extracted. Finally, the sub-call time for the parent function is updated by adding the total execution time of the current function.  Time Complexity: O(n), where n is the number of logs in the input vector, with each log(m)essage being processed precisely once.   Space Complexity: O(n), where n is the number of functions. The size of the stack can be at most n.	A little inefficient!  Method: we use a dictionary to store the start and end times for each function, and then compute the exclusive time for each one.   Weakness: compared to a stack-based approach, this requires extra space to store the dictionary.  Time Complexity: O(n log(n)), where n is the number of functions.  Space Complexity: while useful for storing the start and end times for each function, a dictionary is not necessary, requiring additional space complexity as well.	Not ideal at all!  Method: the program iterates over all function calls, but it optimizes the computation of exclusive time by skipping unnecessary calls.  Weakness: this approach is inefficient because it still has a time complexity of O(n^2), where n is the number of functions.  The minor optimization is not sufficient to overcome the inefficiency of iterating over all possible function calls.	The brute force approach: the program iterates over all possible function calls and computes the exclusive time for each one. This approach is inefficient because it has a time complexity of O(n^2), where n is the number of functions.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Exclusive%20Time%20of%20Functions.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Exclusive%20Time%20of%20Functions.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Exclusive%20Time%20of%20Functions.java	https://youtu.be/X203MmDNmW0
Coding Interviews	Stack	Longest Valid Parentheses	https://leetcode.com/problems/longest-valid-parentheses/	Given a string containing just the characters '(' and ')', return the length of the longest valid (well-formed) parentheses   substring.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Stack	Two Stacks	Count-Based Approach	Dynamic Programming	A	Correct!  This solution uses a stack to keep track of indices of opening brackets '(' encountered. The stack initially has -1 as the base. The code then iterates through the given string and checks for opening brackets, pushing their indices onto the stack. When it encounters a closing bracket ')', it pops the top of the stack. If the stack is empty, it means no opening bracket was found to match the current closing bracket, and the closing bracket index is pushed onto the stack as the new base. If the stack is not empty, the length of the valid parentheses substring is computed by taking the difference between the current index and the index at the top of the stack. The maximum length seen so far is kept as the final result.  Time Complexity: O(n).  Memory Complexity: O(n).	Unnecessarily complex!  We could create two stacks, one for opening parentheses and one for closing parentheses. Then, we'd iterate through the string, pushing opening parentheses onto the opening stack and closing parentheses onto the closing stack. If the top elements of both stacks are of equal distance from the start of the string, pop them both and update the maximum length seen so far. Finally, we return the maximum length seen.	This approach is suboptimal because it does not account for the case where the valid parentheses substring is nested within another one. For example, in the string (()()), the valid parentheses substring is ()(), but the counting method would only detect the first pair of parentheses.	This is simply over-engineering the solution to the problem here.	A brute force approach to solve this problem is to generate all possible substrings of the given string and check if each substring is valid or not. To check if a substring is valid, we can use a stack to keep track of opening brackets and pop from the stack whenever we encounter a closing bracket. If the stack becomes empty, then the substring is valid. We can keep track of the length of the longest valid substring found so far and return it at the end. The time complexity of this approach is O(n^3) since we need to generate all possible substrings and check each of them. This approach is inefficient and not suitable for large input strings.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Longest%20Valid%20Parentheses.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Longest%20Valid%20Parentheses.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Longest%20Valid%20Parentheses.java	https://youtu.be/ovU2LjqR714
Coding Interviews	Priority Queue	Kth Largest Element in an Array	https://leetcode.com/problems/kth-largest-element-in-an-array	Given an integer array nums and an integer k, return the kth largest element in the array.    Note that it is the kth largest element in the sorted order, not the kth distinct element.	O(n log(k))	O(k)	We're expecting an optimized solution of time complexity O(n log(k)) and memory complexity O(k).	Sort-Based Approach	Sorting + Binary Search	Max-Heap	Min-Heap	D	Sorting takes O(n log(n)) time complexity and is not optimal as per the problem requirements.	Suboptimal!  Sorting takes O(n log(n)) time complexity, which is slower than the required O(n) time complexity.	Sub-optimal.  Creating a max-heap and removing k-1 elements takes O(n log(n)) time complexity, which is not optimal as per the problem requirements.	Correct!  The approach used in the provided solution is known as the Kth Largest Element in a Stream approach, which uses a min-heap to store the top k elements of the array. The heap is initially empty, and as elements are processed from the array, they are either ignored if they are smaller than the top of the heap (i.e., not one of the k largest elements seen so far), or they are inserted into the heap and the smallest element is removed if the heap size exceeds k. Once all elements have been processed, the top of the heap is the kth largest element in the array.  Time Complexity: O(n log(k)), as each insertion into the heap takes log(k) time, and there are n elements in the array. However, since k is given as a parameter, the time complexity is O(n) in this case. Space Complexity: O(k), as the heap contains at most k elements.	A simple brute force approach to solving the kth largest element problem is to sort the array in descending order and return the kth element. The time complexity of this approach is O(n log(n)) due to the sorting algorithm used, where n is the size of the array. The space complexity is O(1) as the sorting is performed in place.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Kth%20Largest%20Element%20in%20an%20Array.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Kth%20Largest%20Element%20in%20an%20Array.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Kth%20Largest%20Element%20in%20an%20Array.java	https://youtu.be/wX9dDigp6zY
Coding Interviews	Priority Queue	K Closest Points To Origin	https://leetcode.com/problems/k-closest-points-to-origin/	Given an array of points where points[i] = [xi, yi] represents a point on the X-Y plane and an integer k, return the k closest points to the origin (0, 0).    The distance between two points on the X-Y plane is the Euclidean distance (i.e., √(x1 - x2)2 + (y1 - y2)2).    You may return the answer in any order. The answer is guaranteed to be unique (except for the order that it is in).	O(n log(k))	O(k)	We're expecting an optimized solution of time complexity O(n log(k)) and memory complexity O(k).	Sort-Based Approach	Min-Heap	Max-Heap	Binary Search	C	Suboptimal due to the sorting.  Sorting the points by distance from the origin would have a time complexity of O(n log(n)), which is not optimal.  This would also require extra memory to store the sorted array.	The max-heap is better for this famous problem!	Correct! The approach used in the provided solution is called K-closest points using Max Heap.  In this method, a max heap is used to maintain the K-closest points. Initially, the first K points are pushed onto the max heap. For each subsequent point, its distance from the origin is calculated and compared with the maximum distance in the max heap. If the distance of the current point is less than the maximum distance in the max heap, then the point is added to the max heap and the point with the maximum distance is removed.  Finally, the K-closest points are extracted from the max heap in descending order of distance and returned as the result.  Time Complexity: O(n log(k)), where n is the number of points in the input and k is the number of closest points to be found. This is due to the use of a max heap with size k, which has an insertion time of log(k) and the loop to iterate through all points.  Space Complexity: O(k), as we are only storing the k-closest points in the max heap.	The binary search approach is not applicable since there is no sorted order to the distances.	A simple brute force approach to solving the k closest points to the origin problem is to calculate the distance from each point to the origin, store the distances and their corresponding points in a vector of pairs, sort the vector in ascending order by the distances, and return the first k elements of the sorted vector. This approach has a time complexity of O(n log(n)) due to the sorting operation, and a space complexity of O(n) to store the distances and points.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/K%20Closest%20Points%20to%20Origin.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/K%20Closest%20Points%20to%20Origin.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/K%20Closest%20Points%20to%20Origin.java	https://youtu.be/heWa-6bIheA
Coding Interviews	Priority Queue	Maximum Performance of a Team	https://leetcode.com/problems/maximum-performance-of-a-team/	You are given two integers n and k and two integer arrays speed and efficiency both of length n. There are n engineers numbered from 1 to n. speed[i] and efficiency[i] represent the speed and efficiency of the ith engineer respectively.    Choose at most k different engineers out of the n engineers to form a team with the maximum performance.    The performance of a team is the sum of their engineers' speeds multiplied by the minimum efficiency among their engineers.    Return the maximum performance of this team. Since the answer can be a huge number, return it modulo 1e9 + 7.	O(n log(n))	O(log(k))	We're expecting an optimized solution of time complexity O(n log(n)) and memory complexity O(log(k)).	Greedy Algorithm + Priority Queue	Selection Sort	Dynamic Programming	Bogan's Brute Force Approach	A	Correct!  The method sorts the engineers in decreasing order of efficiency, then uses a priority queue to keep track of k best speed values encountered so far. It then adds the speeds of engineers one by one to the queue, and calculates the maximum performance of the team at each step by taking the sum of speeds from the queue and multiplying it with the efficiency of the current engineer. The maximum performance seen so far is updated with the new value if necessary.  Time Complexity: O(n log(n)).  Memory Complexity: O(log(k)).	Incorrect selection.  This approach would struggle to consider the efficiency of the engineers and may not result in the maximum performance of the team.	Over-engineeered approach: ironic, given the problem statement!  This would use DP to calculate the maximum performance for every possible combination of engineers and choose the best k.  However, it has a time complexity of O(2^n), which is exponential, making it infeasible for large values of n.	Impractical.  This approach has an exponential time complexity and is not feasible for large values of n and k.	A brute force approach to solving this problem would involve generating all possible combinations of teams by selecting at most k engineers from the n available, computing the performance of each team and returning the maximum performance obtained.  One way to implement this would be to use nested loops to generate all possible combinations of k engineers out of n, and then iterate through each team, computing their performance and keeping track of the maximum performance seen so far.  The time complexity of this approach would be O(n^k), which is exponential, since there are n choose k possible combinations of engineers. As such, it would not be practical for large values of n and k.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Maximum%20Performance%20of%20a%20Team.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Maximum%20Performance%20of%20a%20Team.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Maximum%20Performance%20of%20a%20Team.java	https://youtu.be/ZjkL-kMsBFg
Coding Interviews	Priority Queue	Maximum Number of Events	https://leetcode.com/problems/maximum-number-of-events-that-can-be-attended/	You are given an array of events where events[i] = [startDayi, endDayi]. Every event i starts at startDayi and ends at endDayi.    You can attend an event i at any day d where startTimei <= d <= endTimei. You can only attend one event at any time d.    Return the maximum number of events you can attend.	O(n log(n))	O(n)	We're expecting an optimized solution of time complexity O(n log(n)) and memory complexity O(n).	Sort By Earliest End Time	Burari Random Approach	Min-Heap	Sort by Earliest Start Time	C	Incorrect approach.  Sorting by end time only takes into account the duration of events and not their start times, which can lead to missing out on events that have a later end time but a shorter duration, and a later start time.	No such algorithm exists (at the time of writing)!  Randomly attending is incorrect because it does not take into account the duration or timing of events, which can lead to missing out on events that have a shorter duration or start later, but have an earlier end time.	Correct! A good approach uses a min-heap to keep track of the events' end days.  Sort the events by their start day.  Loop through each day from day 1 until all events are processed.  Remove the events that have ended before the current day from the heap.  If there are no available events and there are more events left, jump to the next event start day.  Add all events that start on the current day to the heap.  Attend the event with the earliest end day from the heap.  Time Complexity: O(n log(n)).  Memory Complexity: O(n).	Incorrect approach.  Attending events in order of start time only takes into account the order in which events begin, and not their duration or end times, which can lead to missing out on events that have a later start time but an earlier end time.	A simple brute force approach for this problem would be to generate all possible combinations of events and count the number of valid combinations.  To generate all possible combinations, one could start with the first event and then recursively add the next event that does not overlap with any of the events already chosen. For each valid combination, the count of events attended can be computed and compared to the maximum count seen so far.  The time complexity of this approach would be O(2^n) where n is the number of events, since there are 2^n possible combinations. The space complexity would also be O(n) to store the current combination. However, this approach is not practical for large values of n since the number of combinations grows exponentially.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Maximum%20Number%20of%20Events%20That%20Can%20Be%20Attended.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Maximum%20Number%20of%20Events%20That%20Can%20Be%20Attended.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Maximum%20Number%20of%20Events%20That%20Can%20Be%20Attended.java	https://youtu.be/-a7AUX-684I
Coding Interviews	Priority Queue	Sliding Window Maximum	https://leetcode.com/problems/sliding-window-maximum/	You are given an array of integers nums, there is a sliding window of size k which is moving from the very left of the array to the very right. You can only see the k numbers in the window. Each time the sliding window moves right by one position.    Return the max sliding window.	O(n)	O(k)	We're expecting an optimized solution of time complexity O(n) and memory complexity of O(k). (n is the the number of elements in the input array…I'll let you work out k :)).	Two Loops	Max-Heap	Bogan's Brute Force	Deque	D	Highly impractical!  A simple for loop to iterate through the array and use another loop to find the maximum value in the current sliding window. This approach would have a time complexity of O(n*k) and would be highly inefficient for large inputs.	Space complexity is a problem here.  We could use a max-heap to store the elements in the current sliding window and update it every time the window moves. However, this approach would require a lot of memory to store the heap and would not be optimal for very large inputs.  The memory required would become impractical for very large inputs. In addition, the time complexity of updating the heap every time the window moves would also be high.	People named Bogan are rarely the best people to ask!  This approach creates all possible sliding windows and finds the maximum value for each one. It would have a time complexity of O(n^2) and would be highly inefficient for even moderately sized inputs. As a result, it would take an extremely long time to process large inputs and would not be practical in most situations.	Correct!  In this method, we use a deque to store the indices of the elements in the current window, such that the deque is always in decreasing order of the values of elements in the window. This ensures that the first element in the deque will always be the maximum value in the current window.  At each step, you check if the first element of the deque is outside the current window. If it is, you remove it from the deque. Then, you remove all elements from the back of the deque that are smaller than the current element. Finally, you add the current element to the back of the deque. If the current index is greater than or equal to k-1, then the maximum value for the current window is the first element of the deque, which you add to the result vector.  Time Complexity: O(n).  Memory Complexity: O(k).	A brute force approach to solve this problem would be to iterate through the array and for each position, consider the sliding window of size k starting from that position, and find the maximum element in the window. Repeat this process for all possible starting positions of the sliding window and return the maximum elements found. This approach would have a time complexity of O(n*k) since for each position we are iterating over a window of size k to find the maximum element.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Sliding%20Window%20Maximum.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Sliding%20Window%20Maximum.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Sliding%20Window%20Maximum.java	https://youtu.be/7gLOpQius8E
Coding Interviews	Priority Queue	Find Median from Data Stream	https://leetcode.com/problems/find-median-from-data-stream/	The median is the middle value in an ordered integer list. If the size of the list is even, there is no middle value, and the median is the mean of the two middle values.    For example, for arr = [2,3,4], the median is 3.  For example, for arr = [2,3], the median is (2 + 3) / 2 = 2.5.  Implement the MedianFinder class:    MedianFinder() initializes the MedianFinder object.  void addNum(int num) adds the integer num from the data stream to the data structure.  double findMedian() returns the median of all elements so far. Answers within 10-5 of the actual answer will be accepted.	O(log(n))	O(1)	We're expecting an optimized solution of time complexity O(log(n)) and memory complexity O(1).	Min-Heap + Max-Heap	Single Vector	Linked List	Two Arrays	A	Correct!  This approach maintains two heaps, a max-heap and a min-heap, to keep track of the elements in the data stream. The max-heap stores the smaller half of the elements and the min-heap stores the larger half of the elements. The size of the max-heap should be either equal to or one more than the size of the min-heap, depending on the total number of elements.  When a new element is added to the stream, it is first inserted into either the max-heap or the min-heap depending on its value. The heaps are then balanced by moving elements from one heap to the other, as needed.  To find the median, we check the size of the heaps. If the size of the max-heap is greater than the size of the min-heap, we return the top element of the max-heap as the median. Otherwise, we return the average of the top elements of both heaps as the median.  Time Complexity: O(log(n)).  Memroy Complexity: O(1).	Suboptimal.  Sorting the entire array every time findMedian() is called will take O(n log(n)) time, making it inefficient for large input sizes.	Not the best approach.  Here, the idea is to use a linked list to store the elements and traverse through it every time findMedian() is called.  This approach is suboptimal because it will take O(n) time to traverse through the linked list every time findMedian() is called, which can become inefficient for large input sizes.	A little weaker than using the min-heap and max-heap.  Here, the idea is to maintain two arrays, one for elements smaller than the current median and the other for elements larger than the current median.  This approach is suboptimal because it will take O(n) time to update both the arrays every time addNum() is called, and it will also take O(n) time to calculate the median every time findMedian() is called.	There is no straightforward brute force approach to solve this problem. However, one naive approach would be to add the incoming integer to an array and sort it every time the median is needed. Once the array is sorted, the median can be calculated accordingly by taking the middle element if the size of the array is odd or the average of the middle two elements if the size is even. However, this approach is not optimal as sorting an array takes O(n log(n)) time complexity which is too slow for large inputs.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Find%20Median%20from%20Data%20Stream.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Find%20Median%20from%20Data%20Stream.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Find%20Median%20from%20Data%20Stream.java	https://youtu.be/fr6TAJ8hvw4
Coding Interviews	Priority Queue	Max Value of Equation	https://leetcode.com/problems/max-value-of-equation/	You are given an array points containing the coordinates of points on a 2D plane, sorted by the x-values, where points[i] = [xi, yi] such that xi < xj for all 1 <= i < j <= points.length. You are also given an integer k.    Return the maximum value of the equation yi + yj + |xi - xj| where |xi - xj| <= k and 1 <= i < j <= points.length.    It is guaranteed that there exists at least one pair of points that satisfy the constraint |xi - xj| <= k.	O(n log(n))	O(n)	We're expecting an optimized solution of time complexity O(n log(n)) and memory complexity O(n).	Brute Force With A Twist	Binary Search	Sliding Window + Priority Queue	Sliding Window Approach + Single Pointer	C	The twist? Well, you'll have horrible efficiency.  The approach would be to compare all possible pairs of points and calculate the value of the equation for each pair, and return the maximum value found.  This has a time complexity of O(n^2), which makes it inefficient for large inputs.	Incorrect!  This approach may miss some pairs of points that satisfy the constraint, leading to an incorrect output. It also has a time complexity of O(n*log(n)), which is slower than the optimal approach presented.  For each point, search for the point with the largest yi + xj - xi - yj that satisfies |xi - xj| <= k. Return the maximum value found.	Correct!  We use a sliding window technique to ensure that the absolute difference between xi and xj is less than or equal to k. We then use a priority queue to store the current yj-yi values in the window. We can then calculate the current maximum value of yi-yi+|xi-xj| by adding the current yi-xi value to the largest yj+yi value in the priority queue.  Time complexity: O(n log(n)), where n is the number of points in the input array. This is because we use a set or priority queue that has O(log(n)) insertion and deletion times.  Space complexity: O(n), where n is the number of points in the input array. This is because we need to store all points in the set or priority queue.	Not the most optimal way of doing things.  This approach still has a time complexity of O(n^2) in the worst case scenario, as it may still have to check all possible pairs of points.  How would it be done? First, initialize a pointer to the leftmost point and move it rightward until it is no longer possible to satisfy the constraint |xi - xj| <= k. While doing so, calculate the equation value for all pairs of points that satisfy the constraint and keep track of the maximum value. Then, move the pointer to the next point and repeat the process until all points have been considered.	A brute force approach for this problem would be to consider all pairs of points i and j such that |xi - xj| <= k and i < j. Then, compute the value of the equation yi + yj + |xi - xj| for each pair and return the maximum value found.  The time complexity of this approach is O(n^2), where n is the length of the array points, since we need to consider all pairs of points that satisfy the constraint. The space complexity is O(1), since we are not using any extra data structures to solve the problem. However, this approach is not optimal for large input sizes and can lead to a timeout error. A more efficient approach would require the use of a heap or priority queue to keep track of the maximum value of yi + yj + |xi - xj| seen so far as we iterate through the array points.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Max%20Value%20of%20Equation.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Max%20Value%20of%20Equation.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Max%20Value%20of%20Equation.java	https://youtu.be/YZ-naBf0ALU
Coding Interviews	Linked List	Middle Of the Linked List	https://leetcode.com/problems/middle-of-the-linked-list/	Given the head of a singly linked list, return the middle node of the linked list.    If there are two middle nodes, return the second middle node.	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	Count-based approach	Vector-based Approach	Binary Search	Floyd's Tortoise And Hare	D	Twice as complicated as it needs to be.  Traverse the linked list once to determine its length, then traverse it again to find the middle node by moving len/2 nodes from the head  This approach requires two traversals of the linked list, which makes it inefficient.	Inefficient compared to other methods.  First, create a vector and add each node of the linked list to it. Then, return the node at index size()/2. This approach uses extra memory to store the nodes of the linked list in a vector, which is unnecessary and inefficient.	Not a very good idea!	Correct!  In this algorithm, we use two pointers, a slow pointer and a fast pointer. The slow pointer moves one step at a time, while the fast pointer moves two steps at a time. By the time the fast pointer reaches the end of the linked list, the slow pointer will be at the middle of the linked list.  Time Complexity: O(n), where n is the length of the linked list, since we need to traverse the list once.  Space Complexity: O(1), since we are only using two pointers.	One simple brute force approach for finding the middle node of a linked list is to first traverse the entire list to count the number of nodes. Then, traverse the list again to find the middle node based on the number of nodes counted.  For example, we can first traverse the list to count the number of nodes n. Then, we can traverse the list again, this time stopping at the (n/2)+1-th node, which will be the middle node for odd n. For even n, we can stop at the n/2-th node, which will be the second middle node.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Middle%20of%20the%20Linked%20List.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Middle%20of%20the%20Linked%20List.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Middle%20of%20the%20Linked%20List.java	https://youtu.be/ciPwv1L4JOg
Coding Interviews	Linked List	Partition List	https://leetcode.com/problems/partition-list/	Given the head of a linked list and a value x, partition it such that all nodes less than x come before nodes greater than or equal to x.    You should preserve the original relative order of the nodes in each of the two partitions.	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	Two Pointers	Creating Two New Linked Lists	Quick Sort	Convert to Array + Sort	B	Incorrect!  This approach involves iterating over the linked list and using two pointers to keep track of the nodes less than x and the nodes greater than or equal to x.  Time Complexity: O(n^2), where n is the length of the linked list, since the list may need to be iterated over multiple times in the worst case.	Correct!  One holds all nodes less than x, and the other holds the remaining nodes, before combining them.   Note that this solution can be optimized further by not using the two new linked lists and instead rearranging the nodes in the original linked list itself.  Time Complexity: O(n).  Memory Complexity: O(1).	Ouch!  The time complexity of this approach is O(n^2) in the worst case, where n is the length of the linked list. Although this approach has an average-case time complexity of O(n*log(n)), the worst-case performance makes it inefficient for large inputs.	Conversion is usually best reserved as a brute-force method.  Here, you can convert the linked list into an array, sort the array, and then convert this sorted array back into a linked list.  The time complexity of this approach is O(n log(n)), where n is the length of the linked list, since sorting the array takes O(n log(n)) time using a standard sorting algorithm such as Quick Sort or Merge Sort. However, this approach requires O(n) extra memory to store the array, making it less space-efficient than the original solution.	One possible brute force approach to solving this problem is to traverse the linked list once and create two separate lists: one for nodes less than x and one for nodes greater than or equal to x. Then, concatenate the two lists and return the head.  Here are the steps for this approach:  Initialize two dummy nodes for the two partitions, one for nodes less than x and one for nodes greater than or equal to x.  Traverse the linked list, and for each node:  a. If the node's value is less than x, append it to the less than partition list.  b. Otherwise, append it to the greater than or equal to partition list.  Concatenate the two partition lists by setting the next pointer of the last node in the less than partition list to the first node in the greater than or equal to partition list.  Set the next pointer of the last node in the greater than or equal to partition list to null to terminate the list.  Return the head of the less than partition list.  This approach has a time complexity of O(n) and a space complexity of O(n), where n is the number of nodes in the linked list.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Partition%20List.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Partition%20List.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Partition%20List.java	https://youtu.be/5WSTShKzKLM
Coding Interviews	Linked List	Remove Nth Node From End of List	https://leetcode.com/problems/remove-nth-node-from-end-of-list/	Given the head of a linked list, remove the nth node from the end of the list and return its head.	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	Two-Pass Algorithm	Vector-based Approach	Hash Table	Count-based approach	A	Correct!  The first pass counts the length of the linked list. The second pass traverses the list to the node to be removed and removes it.  Time complexity: O(n), where n is the length of the linked list.  Space complexity: O(1), since the only extra space used is for the dummy node.	Poor space complexity.  We'd traverse the linked list once, and store the nodes in an array or a vector. After that, we remove the nth node from the end by deleting the corresponding node from the array/vector and rebuild the linked list from the remaining nodes.  Space Complexity: O(n), which may not be feasible if the linked list is very large.	Poor space complexity.  We'd traverse the linked list once, and store the nodes in a map or a hash table with their indices as keys. Then, we'd remove the nth node from the end by deleting the corresponding node from the map/hash table and rebuild the linked list from the remaining nodes.  This solution has a higher space complexity of O(n) and adds overhead to store the nodes in a map/hash table. Additionally, rebuilding the linked list can be more complex and error-prone.	Not Quite!  Traverse the linked list to count the length, and then traverse again to the (length - n)-th node and remove the next node.  This is similar to the correct solution, but the node to remove is the next node rather than the (length - n + 1)-th node, which will cause issues if n is equal to the length of the list.	Yes, here's a brute force approach to solve this problem:  1)Traverse the linked list to find its length.  2) Calculate the position of the node to remove by subtracting n from the length.  3) Traverse the linked list again until reaching the position calculated in step 2.  4) Remove the node at the calculated position by adjusting the pointers of its previous and next nodes.  5) Return the head of the modified linked list.  6) This approach requires two passes through the linked list and has a time complexity of O(n), where n is the length of the linked list. However, there are more efficient approaches that require only a single pass through the linked list.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Remove%20Nth%20Node%20From%20End%20of%20List.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Remove%20Nth%20Node%20From%20End%20of%20List.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Remove%20Nth%20Node%20From%20End%20of%20List.java	https://youtu.be/j6DtldLtNEc
Coding Interviews	Linked List	Linked List Cycle	https://leetcode.com/problems/linked-list-cycle/	Given head, the head of a linked list, determine if the linked list has a cycle in it.    There is a cycle in a linked list if there is some node in the list that can be reached again by continuously following the next pointer. Internally, pos is used to denote the index of the node that tail's next pointer is connected to. Note that pos is not passed as a parameter.    Return true if there is a cycle in the linked list. Otherwise, return false.	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	Floyd's Tortoise And Hare	Traversal + Visited Array	Reverse The Linked List	Floyd's Hash Table	A	Correct!  This algorithm is widely used because it is simple, efficient, and easy to implement.  The method works by creating two pointers, tortoise and hare, and advancing them through the linked list. The tortoise pointer moves one step at a time, while the hare pointer moves two steps at a time. If there is a cycle in the linked list, the hare pointer will eventually catch up to the tortoise pointer, and they will meet at a node in the cycle. If there is no cycle, the hare pointer will reach the end of the linked list.  Time Complexity: O(n), where n is the length of the linked list.  Space Complexity: O(1), as we only need to store two pointers regardless of the size of the linked list.	Suboptimal.  This approach requires us to traverse the entire linked list, even if there is no cycle. In the worst case, it takes O(n) time, which is the same as the correct answer. The approach itself? We traverse the linked list, keeping track of the number of nodes visited. If the number of visited nodes exceeds the length of the linked list, return true, as there must be a cycle.   Time complexity: O(n).  Space complexity: O(1).	Incorrect! We don't want to modify the structure!  Reverse the linked list, then traverse it while keeping track of the previous node. If we encounter a node that points to a previous node, return true, as there must be a cycle.  If we reverse the linked list and there is no cycle, we will end up with a completely different linked list, and our algorithm will return false even if the original list had a cycle.	No. Floyd isn't famous for his association with Hash Tables. An approach with a hash table is suboptimal because it requires O(n) extra space to store the hash table. Additionally, inserting and searching in the hash table can take O(1) time in the best case, but up to O(n) time in the worst case if there are hash collisions.	A brute force approach for detecting a cycle in a linked list would involve using a set. We could add the nodes to the set, and if we encounter the same node twice, we know we have a cycle	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Linked%20List%20Cycle.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Linked%20List%20Cycle.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Linked%20List%20Cycle.java	https://youtu.be/sdCwMuNBa08
Coding Interviews	Linked List	Linked List Cycle II	https://leetcode.com/problems/linked-list-cycle-ii/	Given a linked list, return the node where the cycle begins. If there is no cycle, return null. To represent a cycle in the given linked list, we use an integer pos which represents the position (0-indexed) in the linked list where tail connects to. If pos is -1, then there is no cycle in the linked list. Note: Do not modify the linked list.	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	Hash Table	Floyd's Tortoise And Hare	Simple Iteration	Sort-Based Approach	B	Simplistic.  Storing visited nodes in a hash table is a nice idea, but a not the most efficient use of space. If we find a node that already exists in the hash table, that means we have a cycle, and we can find the starting node by iterating over the linked list again from the head until the first node that appears in the hash table is found. However, this requires O(n) space to store the hash table.	Correct!  This approach works very well: it only requires a couple of pointers.  Be careful: this would require a slight modification of Floyd's Cycle Detection algorithm!	Highly inefficient for larger linked lists.  This would be O(n^2) in complexity, where n is the length of the linked list.	A rather bloated approach.  We traverse the linked list, and store the node's value in an array. Then, we traverse this array to find the node with the repeating value.  This is O(n log(n)) in time complexity, and requires extra space too!	Here's a simple brute force approach to solve this problem:  Traverse the linked list, keeping track of the nodes visited in a set or a hashmap.  For each node visited, check if it already exists in the set or hashmap. If it does, return that node as the start of the cycle.  If the traversal completes and no cycle is found, return null.  This approach has a time complexity of O(n), where n is the number of nodes in the linked list, and a space complexity of O(n) to store the set or hashmap of visited nodes.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Linked%20List%20Cycle%20II.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Linked%20List%20Cycle%20II.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Linked%20List%20Cycle%20II.java	https://youtu.be/y5aY9_RKXUQ
Coding Interviews	Linked List	Find the Duplicate Number	https://leetcode.com/problems/find-the-duplicate-number/	Given an array of integers nums containing n + 1 integers where each integer is in the range [1, n] inclusive.    There is only one repeated number in nums, return this repeated number.    You must solve the problem without modifying the array nums and uses only constant extra space.	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	Count Sort Variant	Binary Search	Floyd's Tortoise And Hare	Floyd's Hash Set	C	Nope.  Although this approach has a time complexity of O(n), it requires extra space to store the counts of each element. In the worst case, where all elements are unique, the space complexity can be O(n), which violates the requirement of using constant extra space.	Not optimal!  We would create a range from 1 to n, and count the number of elements in nums that are within the range. If the count is greater than the range, then the repeated number must be within that range. Otherwise, it is outside that range.  This approach has a time complexity of O(n log(n)), which is slower than the Floyd's Tortoise and Hare algorithm. Additionally, it requires extra space to store the range, which violates the requirement of using constant extra space.	Correct! Aka the (less commonly used) Cycle Detection Algorithm.  This algorithm works by using two pointers, the tortoise and the hare, where the hare moves twice as fast as the tortoise. If there is a cycle in the linked list, the hare will eventually catch up to the tortoise at some point. Once they meet, we reset the tortoise to the beginning and move both pointers at the same speed until they meet again, which is the start of the cycle.  In this case, we can apply the same concept to the array by treating each value as a pointer to the next index to move to. Since there is only one repeated number in the array, the repeated number is part of the cycle, and thus, the algorithm can find it.  Time Complexity: O(n).  Memory Complexity: O(1).	No algorithm has this name, although there are a large number of programmers in the California community overly reliant on hash sets!  This would yield a time complexity of O(n).  The space complexity becomes O(n) since we need to store the visited elements in the hash set. Not so groovy.	A brute force approach to solve this problem would be to use two nested loops. The outer loop would iterate through each element of the array, and the inner loop would check all the subsequent elements of the array to see if any of them match the current element. If a match is found, we have found the duplicate element, and we can return it.  However, this approach would have a time complexity of O(n^2), which is not very efficient for large arrays. A more optimal approach would be to use a hash set or a frequency array to keep track of the elements we have seen so far. We can iterate through the array and check if each element has already been seen before. If it has, then we have found the duplicate element, and we can return it.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Find%20the%20Duplicate%20Number.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Find%20the%20Duplicate%20Number.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Find%20the%20Duplicate%20Number.java	https://youtu.be/WZ0hBgceNgY
Coding Interviews	Linked List	Merge K Sorted Lists	https://leetcode.com/problems/merge-k-sorted-lists/	You are given an array of k linked-lists lists, each linked-list is sorted in ascending order.    Merge all the linked-lists into one sorted linked-list and return it.	O(n log(k))	O(k)	We're expecting an optimized solution of time complexity O(n log(k)) and memory complexity O(k), where n is nodes, and k the number of lists.	Flatten + Sort	Divide And Conquer	Traversal + Compare All Linked Lists	Min-Heap	D	Slower than the optimal solution.  This approach flattens all linked lists into a vector, which takes O(n) time and O(n) space, where n is the total number of nodes in all k lists. Sorting the vector takes O(n log(n)) time. Creating a new linked list takes O(n) time. Therefore, the overall time complexity is O(n log(n)).	An inefficient approach when the number of linked lists is large.  This approach repeatedly merges pairs of linked lists. The time complexity is O(nk log(k)), where n is the total number of nodes in all k lists, and k is the number of lists.	Unwise.  This approach traverses all linked lists to find the minimum element in each iteration.  The time complexity is O(nk^2), where n is the total number of nodes in all k lists, and k is the number of lists.	Correct!  Our method uses a min heap to merge the linked lists. Initially, all the first elements of the linked lists are added to the heap. Then, the minimum element is popped from the heap and added to the result list. If the popped element has a next element, it is added to the heap. This process is repeated until the heap is empty.  Time Complexity: O(n log(k)), where n is the total number of nodes in all k lists, and k is the number of lists. The log(k) factor comes from the time taken to heapify the min heap, which has a maximum size of k.  Space Complexity: O(k), as the min heap contains at most k elements.	The brute force approach to solving this problem would be to simply iterate through all the linked lists, extract their values into a list, sort the list, and then create a new linked list with the sorted values. This approach has a time complexity of O(n log(n)), where n is the total number of elements across all linked lists. The steps for this approach would be:  Initialize an empty list to store all the values from the linked lists.  Iterate through each linked list in the input array.  For each linked list, iterate through each node and append the node's value to the list.  Sort the list of values in ascending order.  Create a new linked list and add the sorted values as nodes in the correct order.  Return the head of the new linked list.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Merge%20k%20Sorted%20Lists.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Merge%20k%20Sorted%20Lists.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Merge%20k%20Sorted%20Lists.java	https://youtu.be/DLbUpbUctI4
Coding Interviews	Linked List	Insert Into a Sorted Circular Linked List	https://leetcode.com/problems/insert-into-a-sorted-circular-linked-list/	Given a Circular Linked List node, which is sorted in non-descending order, write a function to insert a value insertVal into the list such that it remains a sorted circular list. The given node can be a reference to any single node in the list and may not necessarily be the smallest value in the circular list.    If there are multiple suitable places for insertion, you may choose any place to insert the new value. After the insertion, the circular list should remain sorted.    If the list is empty (i.e., the given node is null), you should create a new single circular list and return the reference to that single node. Otherwise, you should return the originally given node.	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	Insertion Sort Variant	Simple Traversal	Traversal + Store Min/Max Values	Create + Append A New Node	A	Correct!  Our algorithm iterates through the circular linked list and checks the current node and the next node to determine the insertion position of the new node. It checks if the new node value lies between the current node value and the next node value. If not, it continues iterating until it finds a suitable position.  If the list is empty, it creates a new single circular list and returns a reference to that single node.  Time complexity: O(n), where n is the number of nodes in the circular linked list. In the worst case, the algorithm may have to traverse the entire list before finding the insertion position.  Space complexity: O(1), since only a constant amount of extra space is required to store temporary variables.	Not quite right.  Start from the head and traverse the linked list until we find the correct position to insert the new node. If we reach the head again without finding the position, insert the new node before the head.  This approach doesn't work because it only considers the head node and doesn't take into account the possibility that the insertion position may be between two nodes other than the head.	Suboptimal.  This requires an additional traversal of the linked list to find the minimum and maximum values, which is unnecessary. Also, it doesn't take into account the fact that the linked list is sorted, so the insertion position can be found by comparing values as we traverse the list.	Incorrect.  This approach doesn't preserve the original circular structure of the linked list. Also, it requires an additional traversal of the linked list to find the minimum node, which is unnecessary.	One brute force approach to solve this problem is to traverse the entire circular linked list to find the appropriate position to insert the new value. The position to insert the new value will be between two nodes where the new value is greater than the value of the previous node and less than or equal to the value of the next node. If the new value is less than the minimum or greater than the maximum value of the linked list, then insert it at the beginning or end, respectively.  To handle the special case where the circular linked list is empty, we can create a new node with the value insertVal and make it point to itself.  Once we find the appropriate position to insert the new value, we can create a new node with the value insertVal and insert it between the two nodes by adjusting their pointers.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Insert%20into%20a%20Sorted%20Circular%20Linked%20List.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Insert%20into%20a%20Sorted%20Circular%20Linked%20List.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Insert%20into%20a%20Sorted%20Circular%20Linked%20List.java	https://youtu.be/WMcC3orUcVY
Coding Interviews	Linked List	Copy List with Random Pointer	https://leetcode.com/problems/copy-list-with-random-pointer/	A linked list of length n is given such that each node contains an additional random pointer, which could point to any node in the list, or null.  Construct a deep copy of the list. The deep copy should consist of exactly n brand new nodes, where each new node has its value set to the value of its corresponding original node. Both the next and random pointer of the new nodes should point to new nodes in the copied list such that the pointers in the original list and copied list represent the same list state. None of the pointers in the new list should point to nodes in the original list.    For example, if there are two nodes X and Y in the original list, where X.random --> Y, then for the corresponding two nodes x and y in the copied list, x.random --> y.    Return the head of the copied linked list. The linked list is represented in the input/output as a list of n nodes. Each node is represented as a pair of [val, random_index] where:  val: an integer representing Node.val  random_index: the index of the node (range from 0 to n-1) that the random pointer points to, or null if it does not point to any node.  Your code will only be given the head of the original linked list.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Recursive Approach	Linear Iteration + Hash Table	Recursion + Hashing	Triple Pass of the Linked List	B	This could potentially cause a stack overflow error for large linked lists.  Using a recursive function to create the new linked list by traversing the original one and copying each node and its random pointer recursively.  This approach has a time complexity of O(n^2).	Correct!  With this approach, you first iterate through the linked list and create a new node for each node in the original list, copying its value and random pointer. You also store a mapping between the original nodes and their corresponding new nodes in an hash table.  After creating the new nodes, you iterate through the list again and update the random pointers of each new node to point to the corresponding new node that the original random pointer points to, using the mapping in the hash table.  Time Complexity: O(n), where n is the length of the linked list. We perform two iterations over the linked list of length n.  Space Complexity: O(n), we use extra space to store the mapping between original nodes and new nodes in the hash table.	Stack overflow is a possibility, and not in a good way!  In this approach, you use a recursive function to create a deep copy of the linked list, and store the mapping between the original and new nodes in an hash table. The recursive function takes a node as input and returns the corresponding new node. If a node has already been created, the function retrieves it from the hash table, otherwise it creates a new node and stores it in the hash table.  This approach has a time complexity of O(n), but it uses recursion and may lead to stack overflow for very long linked lists.	Ouch  !This approach has a time complexity of O(n^2), which is very inefficient as the list gets larger.	One brute force approach to solving this problem is to iterate through the original linked list and for each node, create a new node with the same value and set its random pointer to null. Then, create a hash table where the keys are the original nodes and the values are the corresponding new nodes.  Next, iterate through the original linked list again and for each node, set the next pointer of its corresponding new node to the new node corresponding to its next node in the original list, and set the random pointer of its corresponding new node to the new node corresponding to its random node in the original list, using the hash table.  Finally, return the new node corresponding to the head of the original list, which should be the head of the copied linked list. This approach has a time complexity of O(n), where n is the length of the original linked list, and a space complexity of O(n), where n is the length of the original linked list.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Copy%20List%20with%20Random%20Pointer.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Copy%20List%20with%20Random%20Pointer.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Copy%20List%20with%20Random%20Pointer.java	https://youtu.be/GADp4eGLaFE
Coding Interviews	Binary Tree	Binary Tree Right Side View	https://leetcode.com/problems/binary-tree-right-side-view/	Given the root of a binary tree, imagine yourself standing on the right side of it, return the values of the nodes you can see ordered from top to bottom.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Inorder Traversal + Right-most Node Stored	Inorder Traversal + Left-most Node Stored	BFS (Breadth-First Search) + Level Traversal	Recursive Traversal + Hash Table	C	Incorrect!  The right-most node at each level may not necessarily be visible from the right side of the tree.	An all-round incorrect approach to solving the problem.	Correct!  Method: we traverse the binary tree level-by-level, keeping track of the rightmost node at each level. At the end of each level, we add the rightmost node to our result vector. Personally, we use a queue to both process the nodes, and keep track of the nodes to be processed.  Time Complexity: O(n) since each node is visited at least once.  Space Complexity: O(n), in the worst case, we would store all nodes in the queue simultaneously.	Not quite!  This is slower than the queue-based BFS approach due to the overhead of map operations.  Method: we recursively add the nodes at each depth to a map of <depth, value>, then return the values from the map for the maximum depth.  Weakness: this approach is suboptimal because it uses extra space to store the map.	The brute-force approach:  Perform a level-order traversal of the binary tree. For each level, add the value of the rightmost node to the result list, then return the result list.  This approach works because during level-order traversal, the nodes at each level are processed from left to right. Therefore, the rightmost node at each level is the last one to be processed, and hence it is the node that is visible from the right side of the tree. This is nearly identical to the correct answer to this problem!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Binary%20Tree%20Right%20Side%20View.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Binary%20Tree%20Right%20Side%20View.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Binary%20Tree%20Right%20Side%20View.java	https://youtu.be/UBCdPWidc6o
Coding Interviews	Binary Tree	Convert Binary Search Tree to Sorted Doubly Linked List	https://leetcode.com/problems/convert-binary-search-tree-to-sorted-doubly-linked-list/description/	Convert a Binary Search Tree to a sorted Circular Doubly-Linked List in place.    You can think of the left and right pointers as synonymous to the predecessor and successor pointers in a doubly-linked list. For a circular doubly linked list, the predecessor of the first element is the last element, and the successor of the last element is the first element.    We want to do the transformation in place. After the transformation, the left pointer of the tree node should point to its predecessor, and the right pointer should point to its successor. You should return the pointer to the smallest element of the linked list.	O(n)	O(h)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(h).	Inorder Traversal + Sorting	Breadth-First traversal + Queue	Recursive Inorder Traversal + Doubly Linked List	Recursive Depth-First Search + Doubly Linked List	D	The idea is to use a queue to keep track of the current level of nodes. We traverse the tree and update the pointers to create the doubly linked link!  This approach has a time complexity of O(n log(n)) due to sorting and a memory complexity of O(n) for storing the node values.	Due to the use of the queue, this approach would have a memory complexity of O(n), which isn't quite optimal here.	Highly inefficient.  The idea?   Recursive Inorder Traversal of the tree, creating the doubly linked list as we go. Traverse the tree in in-order and create a doubly linked list as you go. For each node, traverse its left subtree, append the node to the end of the linked list, and then traverse its right subtree  This approach has a time complexity of O(n^2) for traversing the linked list each time a node is appended and a memory complexity of O(1) for not using any additional data structures. O(1) memory complexity is achieved much more efficiently using Morris traversal (which was not listed among this set of answers)!	Correct...for this set of answers!  Here, the recursion stack has a maximum depth of the height of the tree, which is O(h) in memory. The combination is done by combining the left and right subtrees with the current node in the middle, forming the final doubly linked list.It is possible to use different algorithms paradigms or data structures. For example, one could use a stack or queue to simulate the recursive call stack, but this would not improve the space or time complexity!   Morris traversal (not LISTED here) is even better in terms of space complexity, but it is a little obscure for most people!	A brute force approach to solving this problem would involve performing an in-order traversal of the binary search tree to get the nodes in sorted order, and then creating a circular doubly-linked list from the sorted nodes. The steps for this approach are as follows:  1) Traverse the left subtree recursively and then visit the current node.  2) Store the previous node visited during the traversal in a variable.  3) Set the left pointer of the current node to the previous node.  4) If the previous node is not null, set its right pointer to the current node.  5) Set the current node as the previous node for the next iteration.  6) Traverse the right subtree recursively.  7) Once the traversal is complete, set the left pointer of the first node to the last node, and the right pointer of the last node to the first node to make it a circular doubly-linked list.  This approach has a time complexity of O(n) since it visits each node in the binary search tree exactly once, and a space complexity of O(n) for the recursive call stack.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Convert%20Binary%20Search%20Tree%20to%20Sorted%20Doubly%20Linked%20List.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Convert%20Binary%20Search%20Tree%20to%20Sorted%20Doubly%20Linked%20List.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Convert%20Binary%20Search%20Tree%20to%20Sorted%20Doubly%20Linked%20List.java	https://youtu.be/Qx3xMofMhWw
Coding Interviews	Binary Tree	Diameter of Binary Tree	https://leetcode.com/problems/diameter-of-binary-tree/	Given the root of a binary tree, return the length of the diameter of the tree.    The diameter of a binary tree is the length of the longest path between any two nodes in a tree. This path may or may not pass through the root.    The length of a path between two nodes is represented by the number of edges between them.	O(n)	O(h)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(h).	In-order Traversal Of All Nodes	Recursive Approach	Two Pair	Traversal + Hash Set	B	Inefficient!  METHOD: traverse all nodes and find the longest path between any two nodes.  WEAKNESS: this approach is suboptimal because it has a time complexity of O(n^2) - wildly inefficient, especially for larger trees.	Correct!   METHOD: recursive calculation of the height of the binary tree, with the base case being when the root is null (returning 0). The height of a node is the maximum of the height of its left and right subtrees plus one (for the root itself). During the height calculation, the maximum diameter is updated by taking the sum of left and right heights of the current node, then checking if it's greater than the current maximum diameter.  Time Complexity: O(n) because each node is visited once  Memory Complexity: O(h), where h is the height of the tree, due to the recursive calls.	No!  Two Pair is frequently a winning hand in poker, but isn't used in computer science yet.	Suboptimal.  This would require storing the maximum diameter of each node, which would be more efficiently done during the height calculation itself. The main weakness? This requires much more memory than the optimal approach.	Brute-force approach: calculate the longest path between any two nodes in the tree using a recursive function that computes the height of the left and right subtrees for each node, returning the maximum diameter found so far.  1) Initialize a variable max_diameter to 0.  2) Define a recursive function (height) that takes a node as input and returns its height - updating max_diameter if the diameter that includes the node is greater than max_diameter.  3) In the height function, compute the height of the left and right subtrees recursively by calling height on each child of the node.  4) Update max_diameter with the maximum of its current value and the sum of the heights of the left and right subtrees plus one (which represents the length of the path that goes through the current node).  5) Return the height of the node (which is the maximum of the heights of its left and right subtrees plus one).  6) Call the height function on the root node.  7) Return max_diameter.  Time Complexity: O(n^2), where n is the number of nodes in the tree, and we compute the heights of each node's left and right subtrees.  Space Complexity: O(h), where h is the height of the tree.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Diameter%20of%20Binary%20Tree.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Diameter%20of%20Binary%20Tree.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Diameter%20of%20Binary%20Tree.java	https://youtu.be/89cX0qitMxw
Coding Interviews	Binary Tree	Subtree of Another Tree	https://leetcode.com/problems/subtree-of-another-tree/	Given the roots of two binary trees root and subRoot, return true if there is a subtree of root with the same structure and node values of subRoot and false otherwise.    A subtree of a binary tree tree is a tree that consists of a node in tree and all of this node's descendants. The tree tree could also be considered as a subtree of itself.	O(m*n)	O(m+n)	We're expecting an optimized solution of time complexity O(m*n) and memory complexity O(m+n).	Tree Serialization + Pattern Matching	Convert Into Preorder Strings + Compare	Pre-Order Traversal + Comparison	Depth-First Search	A	Correct!  Alternative approaches may include traversing both trees in a recursive manner to compare node values and subtrees, which can have a better time complexity depending on the specific implementation.  The time complexity of this approach is O(m*n), where m and n are the number of nodes in root and subRoot, respectively, since we need to serialize both trees and then search for the subRoot string in the root string. The space complexity is O(m+n), since we need to store the serialized strings of both trees.	This approach is suboptimal because it does not consider the tree structure and only relies on the node values.	Suboptimal because it may miss cases where the subRoot is a subtree of root but not a contiguous part of the tree.  This approach traverses both trees simultaneously in pre-order and compare each node of subRoot to the corresponding node of root.	This approach is suboptimal because it checks each possible subtree, even if it is clear that the current node cannot be the root of the subRoot tree. Use DFS to traverse the tree and check if each subtree rooted at the current node is equal to subRoot.	A brute force approach for this problem could be to traverse the entire root tree, and for each node encountered, check if its subtree is identical to the subRoot tree. To check if two subtrees are identical, we can traverse them recursively, comparing each corresponding node's value and children.  Here is a more detailed algorithm:  1) Define a recursive function isIdentical(node1, node2) that returns true if the two trees rooted at node1 and node2 are identical, and false otherwise.  2) The base case for the recursion is when either node1 or node2 is null. In this case, return true if both node1 and node2 are null, and false otherwise.  3) If the values of node1 and node2 are different, return false.  4) Recursively call isIdentical on the left and right subtrees of node1 and node2, and return true if both calls return true.  5) Traverse the root tree recursively. For each node encountered, call isIdentical with its subtree and the subRoot tree. If the call returns true, return true.  6) If no subtree of root is identical to the subRoot tree, return false.  The time complexity: O(n * m), where n is the number of nodes in root and m is the number of nodes in subRoot. In the worst case, we will have to compare every node in the root tree to the subRoot tree. In space, it's O(max(n, m)), the space used by the recursive call stack.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Subtree%20of%20Another%20Tree.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Subtree%20of%20Another%20Tree.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Subtree%20of%20Another%20Tree.java	https://youtu.be/V257XySpES4
Coding Interviews	Binary Tree	Flip Equivalent Binary Trees	https://leetcode.com/problems/flip-equivalent-binary-trees/	For a binary tree T, we can define a flip operation as follows: choose any node, and swap the left and right child subtrees.    A binary tree X is flip equivalent to a binary tree Y if and only if we can make X equal to Y after some number of flip operations.    Given the roots of two binary trees root1 and root2, return true if the two trees are flip equivalent or false otherwise.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Inorder and Postorder Traversal + Comparison	HSO Algorithm	Canonical Parenthesis Representation	Parallel Traversal + Comparison	C	This approach is incorrect because two trees can have different shapes but still be flip equivalent!	D'oh!  This stands for 'Homer Simpson Optimization' Algorithm, which looks for the laziest and most inefficient solution to the problem. Perfect for those who look at a problem and think 'Can't someone else do it?'	Correct!  The method constructs a string representation of the binary tree for each input tree by performing a canonical parenthetical traversal. This traversal ensures that subtrees with the same structure are represented in the same way. The method then compares the two string representations for equality to determine if the two trees are flip equivalent.  Time Complexity: O(n), where n is the number of nodes in the larger tree, as each node is visited once during the traversal.  Memory Complexity: O(n) for storing the string representation of the tree.	This approach is inferior because it is more complex than the solution provided, which simply compares the canonical representations of the trees. Additionally, this approach may not terminate if the trees are not flip equivalent.	A simple brute force approach to solve this problem would be to check if the roots of the two given trees are either equal or symmetric. If they are, then we recursively check if their left and right subtrees are flip equivalent. If both subtrees are flip equivalent, then we return true, otherwise we return false.  This approach has a time complexity of O(n^2) in the worst case, where n is the number of nodes in the tree. This is because we may need to check each node of one tree against each node of the other tree.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Flip%20Equivalent%20Binary%20Trees.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Flip%20Equivalent%20Binary%20Trees.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Flip%20Equivalent%20Binary%20Trees.java	https://youtu.be/Ke0MaLGpjHs
Coding Interviews	Binary Tree	Lowest Common Ancestor of a Binary Tree	https://leetcode.com/problems/lowest-common-ancestor-of-a-binary-tree/	Given a binary tree, find the lowest common ancestor (LCA) of two given nodes in the tree.    According to the definition of LCA on Wikipedia: “The lowest common ancestor is defined between two nodes p and q as the lowest node in T that has both p and q as descendants (where we allow a node to be a descendant of itself).”	O(n)	O(h)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(h).	Traversal + Two Arrays	Recursive Approach	Node-by-node calculation	Convert into BST	B	Inferior.  This approach has a time complexity of O(n^2) in the worst case (when the two nodes are the farthest apart in the tree) and requires O(n) extra space to store the two arrays.	Correct!  It checks if the root is NULL or if the root is either p or q, and if so, it returns the root. Then it recursively checks for the lowest common ancestor in the left and right subtrees of the root. The final result is the root itself if both left and right subtrees have non-null values, left subtree if left is non-null, and right subtree if right is non-null.  Time Complexity: O(n), where n is the number of nodes in the binary tree.	Rather inefficient.  For each node in the tree, calculate the distance from the node to both p and q, and keep track of the node with the smallest sum of distances  This approach has a time complexity of O(n^2) in the worst case (when the two nodes are the farthest apart in the tree), as for each node in the tree we need to traverse to p and q, and a space complexity of O(1) as we are not storing any additional data structures.	Incorrect!  This can modify the structure of the original binary tree, which would result in an incorrect lowest common ancestor. Additionally, it's not always possible to convert a binary tree to a binary search tree without changing the structure of the tree.	A simple brute force approach to finding the lowest common ancestor of two nodes in a binary tree is to perform a post-order traversal of the tree. For each node encountered during the traversal, check if the node is either p or q. If the node is either p or q, return the node. If the node is not p or q, check if both p and q are descendants of the node. If so, return the node. If not, continue traversing the tree until a node is found that satisfies one of the previous conditions. If no such node is found, return null.  The time complexity of this approach is O(n^2) in the worst case, where n is the number of nodes in the tree. This is because in the worst case, the algorithm may traverse the entire tree for each node encountered during the traversal. The space complexity of this approach is O(h), where h is the height of the tree, due to the use of a call stack during the traversal.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Lowest%20Common%20Ancestor%20of%20a%20Binary%20Tree.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Lowest%20Common%20Ancestor%20of%20a%20Binary%20Tree.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Lowest%20Common%20Ancestor%20of%20a%20Binary%20Tree.java	https://youtu.be/mNgpdLgGR_E
Coding Interviews	Binary Tree	Lowest Common Ancestor of a Binary Tree III	https://leetcode.com/problems/lowest-common-ancestor-of-a-binary-tree-iii/	Given two nodes of a binary tree p and q, return their lowest common ancestor (LCA).    Each node will have a reference to its parent node. The definition for Node is below:    class Node {  public int val;  public Node left;  public Node right;  public Node parent;  }  According to the definition of LCA on Wikipedia: The lowest common ancestor of two nodes p and q in a tree T is the lowest node that has both p and q as descendants (where we allow a node to be a descendant of itself).	O(h)	O(1)	We're expecting an optimized solution of time complexity O(h) and memory complexity O(1).	Traversal + Set	Level-order Traversal	Recursive Traversal	Traversal Of Parent Pointers	D	Too much space required!  This approach requires additional memory to store the visited nodes, and may have a worst-case time complexity of O(n) when the binary tree is degenerate, resulting in a long parent chain for one of the nodes.  Starting from one of the nodes, traverse up the parent node chain until reaching the root node, and store all visited nodes in a set. Then, starting from the other node, traverse up the parent node chain, checking if any of the visited nodes have already been encountered. Return the first encountered node.	Not ideal.  Perform a level-order traversal on the binary tree, and check each level to see if both nodes are present. When the nodes are not present on the same level, return the previously seen level's common ancestor.  This approach is inefficient, as it requires checking all nodes in the binary tree, and it may also require additional storage to keep track of the previously seen level's common ancestor.	Quite inefficient.  Starting from the root node, recursively traverse the binary tree while maintaining a path for both nodes. Return the deepest common node in the path.  This approach has a time complexity of O(n^2) in the worst case scenario where the binary tree is unbalanced, and the two nodes are on opposite sides of the tree.	Correct!  Starting from the given nodes, we traverse their respective parent pointers until we reach a common node, which is the lowest common ancestor (LCA) of the two nodes.  Time Complexity: O(h) where h is the height of the tree.  Space complexity: O(1).	A simple brute force approach for finding the lowest common ancestor of two nodes in a binary tree with parent pointers is as follows:  Start from node p and traverse up to its root by following its parent pointers, and store all the nodes visited in a hash set.  Start from node q and traverse up to its root by following its parent pointers. At each step, check if the current node is present in the hash set of nodes visited in step 1. If it is, then this is the lowest common ancestor.  If the root is reached in step 2 without finding the lowest common ancestor, repeat the same process by starting from node q and traversing up to its root, and storing the visited nodes in a hash set. Then, start from node p and traverse up to its root, checking at each step if the current node is present in the hash set of nodes visited in step 2.  This approach has a time complexity of O(h) where h is the height of the tree, since in the worst case we may need to traverse up to the root twice. The space complexity is also O(h) since we store the visited nodes in hash sets.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Lowest%20Common%20Ancestor%20of%20a%20Binary%20Tree%20III.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Lowest%20Common%20Ancestor%20of%20a%20Binary%20Tree%20III.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Lowest%20Common%20Ancestor%20of%20a%20Binary%20Tree%20III.java	https://youtu.be/t5hs2wnUJJs
Coding Interviews	Binary Tree	Intersection of Two Linked Lists	https://leetcode.com/problems/intersection-of-two-linked-lists/	Given the heads of two singly linked-lists headA and headB, return the node at which the two lists intersect. If the two linked lists have no intersection at all, return null.    The test cases are generated such that there are no cycles anywhere in the entire linked structure. Note that the linked lists must retain their original structure after the function returns. The inputs to the judge are given as follows (your program is not given these inputs):    intersectVal - The value of the node where the intersection occurs. This is 0 if there is no intersected node.  listA - The first linked list.  listB - The second linked list.  skipA - The number of nodes to skip ahead in listA (starting from the head) to get to the intersected node.  skipB - The number of nodes to skip ahead in listB (starting from the head) to get to the intersected node.  The judge will then create the linked structure based on these inputs and pass the two heads, headA and headB to your program. If you correctly return the intersected node, then your solution will be accepted.	O(n+m)	O(1)	We're expecting an optimized solution of time complexity O(n+m) and memory complexity O(1) (**NOTE** There are multiple alternatives too!).	Traversal + Two Arrays	Traversal + Direct Comparison	Traversal + Hash Set	Two Pointers	D	No! The solution can be achieved in O(1) space.  In this approach, we create two pointers, one for each linked list, and iterate over them until they meet. To ensure that both pointers have traveled the same distance when they collide, we reset a pointer to the head of the opposite list when it reaches the end of its own list. This allows us to find the intersection point of the two linked lists in O(m + n) time, where m and n are the lengths of the linked lists.  This approach requires O(n) space complexity and is suboptimal compared to the O(1) space complexity required by the correct solution.	This approach has a time complexity of O(n^2), which is inefficient for large linked lists.	Unfortunately, no.  This approach requires O(n) space complexity and is suboptimal compared to the O(1) space complexity required by the correct solution. Additionally, it also requires two passes through the linked lists, whereas the correct solution only requires one pass.  The idea is that we traverse linked list A and store all the node addresses in a hash set. Then, traverse linked list B and for each node, check if it exists in the hash set.	Correct!  In this approach, we create two pointers, one for each linked list, and iterate over them until they meet. To ensure that both pointers have traveled the same distance when they collide, we reset a pointer to the head of the opposite list when it reaches the end of its own list.  This allows us to find the intersection point of the two linked lists in O(m + n) time, where m and n are the lengths of the linked lists.	One simple brute force approach to solving this problem is to traverse through both linked lists and compare each node of the first list with every node of the second list until a matching node is found. However, this approach would have a time complexity of O(mn) where m and n are the lengths of the two linked lists, which is not efficient.  A more optimized approach would be to first find the lengths of both linked lists, and then traverse the longer linked list by the difference in lengths between the two lists to reach the starting point of the intersection. Then, both linked lists can be traversed together until a matching node is found. This approach has a time complexity of O(m+n), which is more efficient.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Intersection%20of%20Two%20Linked%20Lists.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Intersection%20of%20Two%20Linked%20Lists.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Intersection%20of%20Two%20Linked%20Lists.java	https://youtu.be/qXImyldVZA0
Coding Interviews	Binary Tree	Lowest Common Ancestor of a Binary Search Tree	https://leetcode.com/problems/lowest-common-ancestor-of-a-binary-search-tree/	Given a binary search tree (BST), find the lowest common ancestor (LCA) node of two given nodes in the BST.    According to the definition of LCA on Wikipedia: “The lowest common ancestor is defined between two nodes p and q as the lowest node in T that has both p and q as descendants (where we allow a node to be a descendant of itself).”	O(h)	O(h)	We're expecting an optimized solution of time complexity O(h) and memory complexity O(h).	Simple Parent Pointer Usage	Inorder Traversal While Storing Path	Iterative Approach	Recursive Approach	D	Suboptimal.  This approach involves storing parent pointers for each node in the tree and then traversing from p and q to their respective root nodes, and then finding the first common node between the two paths.  This is suboptimal because it requires additional space to store the parent pointers, making it less memory-efficient. It also has a time complexity of O(h), where h is the height of the tree, which is similar to the recursive approach but with the additional space requirement.	Not ideal.  This approach involves storing the paths from root to p and root to q, then finding the last common node in the paths.  It's suboptimal because it has a time complexity of O(n) and a space complexity of O(n), making it less efficient than the recursive approach.	The complexity is too high here.  This approach involves iterating through the tree and checking each node to find the LCA. This approach has a higher time complexity than the recursive approach and is also more complex to implement.  This approach is suboptimal because the time complexity can be as high as O(n) in the worst case scenario where the tree is skewed, making the algorithm inefficient.	Correct!  The method traverses the tree recursively to find the LCA of the given two nodes in the BST. The algorithm starts at the root of the tree and checks if the root is either p or q or null. If any of these conditions are true, the root is returned.  Next, the algorithm searches for p and q in the left and right subtrees of the root. If both p and q are found in the left and right subtrees, then the current root is the LCA. If only p or q is found, the search continues in that subtree.  Time Complexity: O(h), where h is the height of the tree  Space Complexity: O(h) due to the recursion stack.	A brute force approach to finding the lowest common ancestor of two nodes in a binary search tree is to traverse the tree from the root node to each of the given nodes to find their paths, then compare the paths to find the lowest common ancestor. Here are the steps:  1) Traverse the tree from the root node to the first given node, storing the path in an array or a stack.  2) Traverse the tree from the root node to the second given node, storing the path in another array or stack.  3) While the two paths have the same node at the top, pop the top node from both paths.  4) The last common node is the lowest common ancestor.  5) This approach has a time complexity of O(n), where n is the number of nodes in the tree.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Lowest%20Common%20Ancestor%20of%20a%20Binary%20Search%20Tree.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Lowest%20Common%20Ancestor%20of%20a%20Binary%20Search%20Tree.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Lowest%20Common%20Ancestor%20of%20a%20Binary%20Search%20Tree.java	https://youtu.be/XM8X3GnXoXg
Coding Interviews	Binary Tree	Binary Search Tree Iterator	https://leetcode.com/problems/binary-search-tree-iterator/	Implement the BSTIterator class that represents an iterator over the in-order traversal of a binary search tree (BST):    BSTIterator(TreeNode root) Initializes an object of the BSTIterator class. The root of the BST is given as part of the constructor. The pointer should be initialized to a non-existent number smaller than any element in the BST.  boolean hasNext() Returns true if there exists a number in the traversal to the right of the pointer, otherwise returns false.  int next() Moves the pointer to the right, then returns the number at the pointer.  Notice that by initializing the pointer to a non-existent smallest number, the first call to next() will return the smallest element in the BST.    You may assume that next() calls will always be valid. That is, there will be at least a next number in the in-order traversal when next() is called.	O(1)	O(n)	We're expecting an optimized solution of time complexity O(1) and memory complexity O(n).	Depth-first Traversal	Breadth-first Traversal	Inorder Traversal + Stack	Pre-order Traversal + Stack	C	No, we can do this in O(1) time.  Traverses a Binary Search Tree in O(n) time complexity and O(h) space complexity using recursion or a stack.	No, we can do this in O(1) time.  Traverses a Binary Search Tree in O(n) time complexity and O(n) space complexity using a queue to visit the nodes level by level.	Correct!  Our algorithm initializes the iterator with the root of the BST and pushes all the nodes of the left chain of the root onto the stack. The next() function pops the top node from the stack, and if its right child is not null, pushes all the nodes of its left chain onto the stack. It then returns the value of the popped node. The hasNext() function checks whether the stack is empty or not.  Time Complexity: O(1) for both next() and hasNext() operations in the average case.  Space Complexity: O(h), where h is the height of the BST, because at most h nodes can be in the stack at any given time. However, in the worst case scenario, where the tree is completely unbalanced, the space complexity can go up to O(n), where n is the number of nodes in the BST.	Slightly confused approach.  This iterates over a Binary Search Tree in O(1) time complexity and O(h) space complexity using a stack, but does not follow the order of the BST.  Although it works for traversing the entire tree, it does not provide the values in sorted order as required by the problem.	The brute force approach for implementing the BSTIterator class would be to perform an in-order traversal of the given binary search tree (BST) and store the elements in a list. The hasNext() method would then simply check if there are any remaining elements in the list, and the next() method would return the next element in the list and move the pointer to the right.  However, this approach would not satisfy the requirement of O(h) memory, where h is the height of the BST. To meet this requirement, we can use a stack to simulate the in-order traversal of the BST. We start by pushing all the left nodes onto the stack. The hasNext() method would then check if the stack is empty, and the next() method would pop the top node from the stack, add its right subtree's left nodes onto the stack, and return the popped node's value.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Binary%20Search%20Tree%20Iterator.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Binary%20Search%20Tree%20Iterator.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Binary%20Search%20Tree%20Iterator.java	https://youtu.be/88cSv9rOqrE
Coding Interviews	Binary Tree	Recover Binary Search Tree	https://leetcode.com/problems/recover-binary-search-tree/	You are given the root of a binary search tree (BST), where the values of exactly two nodes of the tree were swapped by mistake. Recover the tree without changing its structure.	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	In-order Traversal + Stack	Morris Traversal	Breadth-first Traversal	Depth-first Traversal	B	A little suboptimal.  Traverses the tree in O(n) time complexity and O(h) space complexity using a stack to keep track of visited nodes. Although it is a valid approach, it uses extra space and is not optimal when the tree is very large.	Correct!  Morris Traversal is a space-efficient method to traverse a binary tree without using a stack or recursion. The algorithm uses threading, which means it modifies the structure of the tree temporarily to reduce the space complexity of the traversal. To recover the Binary Search Tree, you can use Morris Traversal to traverse the tree and check if the current node's value is less than the previous node's value.  Time Complexity: O(n) because you're traversing each node of the tree only once.  Space Complexity: O(1) because you're not using any extra space except for three pointers.	Not quite.  Although it works for traversing the entire tree, it does not efficiently identify the two swapped nodes and thus requires further processing.  Traverses the tree in O(n) time complexity and O(n) space complexity using a queue to visit the nodes level by level.	Less efficient than other approaches.  Although it is a common approach for tree traversal, it does not allow us to identify the two swapped nodes without additional processing. Traverses the tree in O(n) time complexity and O(h) space complexity using recursion or a stack.	A brute force approach to recover the BST would be to perform an in-order traversal of the tree and store the nodes in an array. Since in-order traversal of a BST returns the nodes in non-descending order, any two swapped nodes would appear in the array in an incorrect order. We can then identify the misplaced nodes by comparing each node with its next node in the array, and swap their values to recover the tree.  However, this approach requires O(n) space to store the nodes in the array, where n is the number of nodes in the tree. A better approach would be to perform the in-order traversal of the tree recursively and keep track of the previous node visited. If at any point, the value of the current node is less than the value of the previous node, we have found a misplaced pair of nodes. We can then swap their values to recover the tree.  This approach has O(1) space complexity and O(n) time complexity, where n is the number of nodes in the tree.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Recover%20Binary%20Search%20Tree.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Recover%20Binary%20Search%20Tree.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Recover%20Binary%20Search%20Tree.java	https://youtu.be/vVjHJC8lkHo
Coding Interviews	Binary Tree	Binary Tree Maximum Path Sum	https://leetcode.com/problems/binary-tree-maximum-path-sum/	A path in a binary tree is a sequence of nodes where each pair of adjacent nodes in the sequence has an edge connecting them. A node can only appear in the sequence at most once. Note that the path does not need to pass through the root.    The path sum of a path is the sum of the node's values in the path.    Given the root of a binary tree, return the maximum path sum of any non-empty path.	O(n)	O(h)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(h).	DFS + Memoization	Recursive Traversal	BFS	Recursion + Kadane's Algorithm	D	Not quite optimal, although a solid suggestion!   We can traverse the tree with DFS, and memoize the maximum path sum for each node. While the time complexity is nice at O(n), we need additional space for the memoization - which would be O(n) rather than the optimal O(h).	Overly naïve.  It would mean traversing each node multiple times!   The time complexity would be O(n^2) in this case.  Technically, the Kadane's Algorithm for Trees is recursive too…but it's really important to name it.	Not quite optimal!  BFS can also yield a time complexity of O(n), but needs additional space to store the maximum sum path for each level.	Correct!   Method: we take a recursive approach that traverses the tree, while calculating the maximum sum path passing through each node AND the maximum sum path starting from each node.  We use Kadane's algorithm to calculate the maximum sum path starting from each node, and combine it with the tree diameter problem to calculate the maximum sum path passing through each node.  Time Complexity: O(n), where n is the number of nodes in the tree  Space Complexity: O(h), where h is the height of the tree, due to the recursion stack.	A brute force approach to solving this problem would be to traverse every possible path in the binary tree and compute its path sum, keeping track of the maximum path sum seen so far.  We can traverse every possible path in the binary tree using a recursive function that starts at the root node and explores both its left and right subtrees. At each node, we compute the path sum of the path that includes that node by adding its value to the maximum path sum of its left and right subtrees. We then compare this path sum to the maximum path sum seen so far and update it if necessary.  The time complexity of this approach is O(n^2), where n is the number of nodes in the binary tree, because we potentially visit every node in the tree for every other node in the tree. However, we can optimize this approach by computing the maximum path sum for each node in a bottom-up manner and keeping track of the maximum path sum seen so far during the traversal. This reduces the time complexity to O(n), where n is the number of nodes in the binary tree.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Binary%20Tree%20Maximum%20Path%20Sum.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Binary%20Tree%20Maximum%20Path%20Sum.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Binary%20Tree%20Maximum%20Path%20Sum.java	https://youtu.be/2mtSr-pQ0f8
Coding Interviews	Hash Tables	Minimum Area Rectangle	https://leetcode.com/problems/minimum-area-rectangle/	You are given an array of points in the X-Y plane points where points[i] = [xi, yi].    Return the minimum area of a rectangle formed from these points, with sides parallel to the X and Y axes. If there is not any such rectangle, return 0.	O(n^2)	O(n)	We're expecting an optimized solution of time complexity O(n^2) and memory complexity O(n) (***Our suggested solution is by no means the most efficient for this problem***).	Hash Table + Nested Loop	Hash Table + HSO Algorithm	Sorting + Two Pointers	Set	A	Correct!  Method: we use a hash table to group the points by their x-coordinates, and then iterate through every pair of distinct points to check if they can form a rectangle. This involves using a nested loop over all pairs of points.  Time Complexity: O(n^2) where n is the number of points.  Space Complexity: O(n) for the hash table.	No, this is a joke algorithm!  The 'Homer Simpson Optimization Algorithm' will always look for the laziest (yet most inefficient solution) to any problem, solving it (and causing another problem) with alcohol. Perfect for those who look at a problem and think 'Can't someone else do it?'  Not so ideal for an interview!	Inefficient!  Method: sort the points first by x-coordinate, and then by y-coordinate. Then, for each pair of points (p1, p2) that have the same x-coordinate, check if there exists two other points (p3, p4) with the same y-coordinate. If such points exist, calculate the area of the rectangle formed by (p1, p2, p3, p4) and update the minimum area.  Weakness: the time complexity is horrendous, being O(n^2 log(n)) due to sorting and loops.	Very inefficient!  Method: we store all the given points in a set of pairs. Then, for each pair of points (p1, p2), we check if there exists two other points (p3, p4) such that (p1, p4) and (p2, p3) are diagonals of a rectangle. Finally, calculate the area of the rectangle formed by (p1, p2, p3, p4) and update the minimum area.  Weakness: this approach has a time complexity of O(n^3), which is very rarely viable for a LeetCode problem!	The brute-force here is to iterate through all pairs of points in the array and calculate the distance between them. If these two points share the same x-coordinate or y-coordinate, they could be opposite corners of a rectangle. If they share a coordinate, look for two more points in the array that share the other coordinate with them and form a rectangle.  Calculate the area of this rectangle and keep track of the minimum area found so far. Then, repeat the steps above for ALL pairs of points in the array, returning the minimum area found.  The simplest version of this approach has a time complexity of O(n^4), where n is the length of the input array, since it requires four nested loops to iterate through all possible pairs of points. It is not an efficient solution, but it can be a starting point for developing a more optimized algorithm.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Minimum%20Area%20Rectangle.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Minimum%20Area%20Rectangle.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Minimum%20Area%20Rectangle.java	https://youtu.be/GbiPbnSRHHQ
Coding Interviews	Hash Tables	All O'one Data Structure	https://leetcode.com/problems/all-oone-data-structure/	Design a data structure to store the strings' count with the ability to return the strings with minimum and maximum counts.    Implement the AllOne class:    AllOne() Initializes the object of the data structure.  inc(String key) Increments the count of the string key by 1. If key does not exist in the data structure, insert it with count 1.  dec(String key) Decrements the count of the string key by 1. If the count of key is 0 after the decrement, remove it from the data structure. It is guaranteed that key exists in the data structure before the decrement.  getMaxKey() Returns one of the keys with the maximal count. If no element exists, return an empty string .  getMinKey() Returns one of the keys with the minimum count. If no element exists, return an empty string .  Note that each function must run in O(1) average time complexity.	O(1)	O(n)	We're expecting an optimized solution of time complexity O(1) and memory complexity O(n).	Doubly-Linked List + Hash Table	Single Hash Table	Single Doubly-Linked List	Two Hash Tables	A	Correct! In this suggested solution, which is a little tricky to implement, you need to design a data structure to store key-value pairs and efficiently support the following operations in constant time: insert a new key-value pair, remove an existing key-value pair, and get a key-value pair with the maximum or minimum key.  The data structure can be implemented using a doubly linked list of buckets with a hash table that maps each key to its corresponding bucket. Each bucket contains a set of keys that share the same value. To support the three operations, the implementation updates the linked list and hash table accordingly, moving keys between buckets as necessary.	Incorrect!  One possible solution is to use a single hash table to store both the key-value pairs and the frequency count of each key. When a key is accessed, the frequency count is incremented and the hash table is scanned to find the key with the highest/lowest frequency count. However, this approach would require scanning the entire hash table each time a key is accessed, making it inefficient for large datasets.	Not quite!  It's true that one could use a linked list to store the key-value pairs and the frequency count of each key. Each node in the linked list would contain a key, a value, and a frequency count. When a key is accessed, its frequency count is incremented and the linked list is scanned to find the key with the highest/lowest frequency count. However, this approach would require scanning the entire linked list each time a key is accessed - this makes it inefficient for large datasets.	Suboptimal!  It is possible to use two separate hash tables to store the key-value pairs and the frequency count of each key. When a key is accessed, the frequency count is incremented and the hash table with the frequency counts is scanned to find the key with the highest/lowest frequency count. While this approach is quite efficient, it still requires scanning the entire frequency count hash table each time a key is accessed. However, in the worst case, all keys could have the same hash code, and the hash table could end up having all keys collide in the same bucket, resulting in O(n) time complexity for accessing an element in the hash table. However, this is a rare case, and the hash table data structure has been designed to minimize collisions, making the expected time complexity O(1) for most cases.	One possible brute force approach for implementing the AllOne data structure is to use two hash tables, one to store the count of each string and another to store the set of strings for each count. Here are the steps for each operation:  1) Initialization: Initialize both hash tables to empty.  2) Increment (inc): If the key is not in the count map, add it with count 1. If it's already in the count map, increment its count. Then, remove the key from the set corresponding to the old count (if it exists) and add it to the set corresponding to the new count.  3) Decrement (dec): Decrement the count of the key in the count map. If the count becomes 0, remove the key from the count map and from the set corresponding to its old count. If the count is positive, remove the key from the set corresponding to the old count and add it to the set corresponding to the new count.  4) Get maximum key (getMaxKey): Return one of the keys in the set corresponding to the maximum count (which can be found in constant time by keeping track of the maximum count).  5) Get minimum key (getMinKey): Return one of the keys in the set corresponding to the minimum count (which can be found in constant time by keeping track of the minimum count).  This approach has a time complexity of O(1) for all operations on average, as required by the problem statement. However, it requires O(n) space in the worst case, where n is the number of distinct strings that have been inserted into the data structure.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/All%20O%60one%20Data%20Structure.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/All%20O%60one%20Data%20Structure.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/All%20O%60one%20Data%20Structure.java	https://youtu.be/n0DIr2_TxYM
Coding Interviews	Hash Tables	LRU Cache	https://leetcode.com/problems/lru-cache/	Design a data structure that follows the constraints of a Least Recently Used (LRU) cache.    Implement the LRUCache class:    LRUCache(int capacity) Initialize the LRU cache with positive size capacity.  int get(int key) Return the value of the key if the key exists, otherwise return -1.  void put(int key, int value) Update the value of the key if the key exists. Otherwise, add the key-value pair to the cache. If the number of keys exceeds the capacity from this operation, evict the least recently used key.  The functions get and put must each run in O(1) average time complexity.	O(1)	O(n)	We're expecting an optimized solution of time complexity O(1) and memory complexity O(n) (For the get and put functions. N.B. there are alternative 'better' approaches to investigate for this famous interview problem!).	Linear Search	Doubly-Linked List + Hash Table	Single Hash Table	Two Arrays	B	Hmm…not really!  The idea is to store the timestamp of each key-value pair in a separate hash table, and then perform a linear search over it to find the least recently used element when the cache is full. However, this approach would also have a time complexity of O(n) for both get and put operations, where n is the number of elements in the cache, making it unsuitable for high-performance systems.	Correct!  One implementation can maintain a doubly linked list to keep track of the order in which the elements were accessed, with the most recently accessed element at the front and the least recently accessed element at the back. It also could use an hash table to store key-value pairs - and keep the corresponding iterators in the doubly linked list.	Incorrect!  Using a hash table will store the key-value pairs, but lacks a mechanism to track the order in which the elements were accessed. This would make it impossible to determine the least recently used element when the cache is full, and the implementation would require a linear scan of the hash table to evict an element. As a result, this approach would also have a time complexity of O(n) for both get and put operations, where n is the size of the hash table.	Incorrect!  It's true that one approach could be to use an array to store the key-value pairs, with a separate array to store the timestamps of each key-value pair. However, this approach would require a linear search over the timestamp array to find the least recently used element when the cache is full, resulting in a time complexity of O(n) for the eviction operation. There's no way to do an O(1) 'lazy deletion' for this!	Here, my own suggested implementation is almost identical to the Brute Force approach…my apologies!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/LRU%20Cache.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/LRU%20Cache.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/LRU%20Cache.java	https://youtu.be/aqhi8CcFkQM
Coding Interviews	Hash Tables	LFU Cache	https://leetcode.com/problems/lfu-cache/	Design and implement a data structure for a Least Frequently Used (LFU) cache.  Implement the LFUCache class:    LFUCache(int capacity) Initializes the object with the capacity of the data structure.  • int get(int key) Gets the value of the key if the key exists in the cache. Otherwise, returns -1.  • void put(int key, int value) Update the value of the key if present, or inserts the key if not already present. When the cache reaches its capacity, it should invalidate and remove the least frequently used key before inserting a new item. For this problem, when there is a tie (i.e., two or more keys with the same frequency), the least recently used key would be invalidated.  To determine the least frequently used key, a use counter is maintained for each key in the cache. The key with the smallest use counter is the least frequently used key.  When a key is first inserted into the cache, its use counter is set to 1 (due to the put operation). The use counter for a key in the cache is incremented either a get or put operation is called on it.  The functions get and put must each run in O(1) average time complexity.	O(1)	O(n)	We're expecting an optimized solution of time complexity O(1) and memory complexity O(n).	Sorted List	Frequency Count	Hash Table + Min-Heap	Hash Table + Doubly Linked List	D	Not the best answer from the selection. This CAN be a correct implementation of the LFU cache, but a sorted list results in O(n) time complexity for both get and put operations, violating the O(1) time complexity requirement.	Not ideal due to the space complexity.  The method here would incorporate separate counters that tell us the frequency of use AND time of last access for each key in the cache.  This requires additional space complexity, and can result in incorrect behavior when there is a tie between keys with the same frequency.	Not quite!   The approach involves using a hash table to store the key-value pairs, and a min-heap to keep track of the least frequently used entries. Each entry in the min-heap is a key-value pair, maintained along with its frequency of use. This approach would have O(log(n)) complexity for accessing and updating the min-heap. When a key is accessed or updated, update its frequency of use in the hash table and update its position in the min-heap. When a new key is inserted, add it to the hash table with a frequency of 1 and insert it into the min-heap. When the cache reaches its capacity, evict the least frequently used key by removing it from the min-heap and hash table.	Correct!  Each node in the linked list can stores a frequency, and a LIST of keys with that frequency. When a key is accessed (via get or put), its frequency is incremented, and it is moved to the correct position in the linked list. When the cache reaches its capacity, the least frequently used key is removed from the linked list and the hash table. This yields an O(1) approach for both get and put operations.	A brute-force approach to achieve O(1) complexity is probably not possible.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/LFU%20Cache.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/LFU%20Cache.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/LFU%20Cache.java	https://youtu.be/Fk3DPdhYQoQ
Coding Interviews	Hash Tables	Insert Delete GetRandom O(1)	https://leetcode.com/problems/insert-delete-getrandom-o1/	Implement the RandomizedSet class:    RandomizedSet() Initializes the RandomizedSet object.  bool insert(int val) Inserts an item val into the set if not present. Returns true if the item was not present, false otherwise.  bool remove(int val) Removes an item val from the set if present. Returns true if the item was present, false otherwise.  int getRandom() Returns a random element from the current set of elements (it's guaranteed that at least one element exists when this method is called). Each element must have the same probability of being returned.  You must implement the functions of the class such that each function works in average O(1) time complexity.	O(1)	O(n)	We're expecting an optimized solution of time complexity O(1) and memory complexity O(n).	Vector + Hash Table	Doubly Linked List + Hash Table	Two Arrays	Binary Search Tree + Vector	A	Correct!  The vector stores the actual elements in the set, while the map simply maps a value to its corresponding index in the values vector. The insert operation will add a new element to the values vector, but only if it doesn't already exist in the set. The remove operation will first check if the element exists in the set, and will then remove it from the values vector by swapping it with the last element in a lazy deletion. It's important that the swapped element's entry is updated in the map at the same time. Finally, in the C++/Java implementations of the solution, the getRandom operation returns a random element from the values vector using the rand() function. Python can use random/choice for the same purpose.	Not the best from the options available due to the additional space.  This linked list would be used to store the elements, with the hash table recording the index of each element. The insertion and removal operations would take O(n) time complexity in the worst case due to the need to traverse the linked list to find the element. However, this solution would have O(n) space complexity.	Incorrect, due to the additional space complexity!  Presumably, we use one to store the elements, while the other flags if an element is present in the set.  The insertion and removal operations would take O(n) time complexity in the worst case due to the need to shift elements in the array. However, this solution would have O(n) space complexity.	Incorrect!  This idea uses a binary search tree to store the elements and a vector to store the elements in the order they were inserted. Insertion and removal operations would take O(log(n)) time complexity in the worst case due to the binary search tree's lookup time complexity.	A brute-force approach to achieve O(1) complexity is probably not possible.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/Insert%20Delete%20GetRandom%20O(1).cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/Insert%20Delete%20GetRandom%20O(1).py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/Insert%20Delete%20GetRandom%20O(1).java	https://youtu.be/Tj1SZyakWQo
Coding Interviews	Hash Tables	First Unique Number	https://leetcode.com/problems/first-unique-number/	You have a queue of integers, you need to retrieve the first unique integer in the queue.    Implement the FirstUnique class:    FirstUnique(int[] nums) Initializes the object with the numbers in the queue.  int showFirstUnique() returns the value of the first unique integer of the queue, and returns -1 if there is no such integer.  void add(int value) insert value to the queue.	O(1)	O(n)	We're expecting an optimized solution of time complexity O(1) and memory complexity O(n) ***Note, these refer to the add() and get() functions***	Single Hash Table	Queue + Set	Doubly-Linked List + Hash Table	Sorted List	C	Not quite!  This idea uses a single hash table to keep track of the frequency of each number in the input array, and returns the first number with a frequency of 1. The time complexity is O(n) for preprocessing the input array, although O(1) for all subsequent queries.	Not quite: combining a queue and set would result in O(n) time complexity!  We can use a queue to maintain the order of the unique numbers, while the set to keeps track of the numbers that have been added to the data structure. For the add operation, we simply add the number to the queue - and to the set if it is not already there. If it's already in the set, we can remove it from the queue! For the showFirstUnique operation, we iterate through the queue until we find the first number that is NOT in the set. However, this approach has a time complexity of O(n) for both add and showFirstUnique operations.	Correct!  This approach is very efficient compared to the others for the following reasons. The doubly linked list maintains the order of the unique numbers that have been added to the data structure, while the hash table maps the value of a number to its corresponding node in the uniqueNumbers list.  Essentially, the list maintains the order of the unique numbers, while the map helps with efficient lookup of nodes in the list.	Sorting ruins the complexity!  This approach involves using a sorted list to maintain the order of the unique numbers. We can insert a number in the sorted list if it is not already in the list, and remove it from the list if it is already present. For the showFirstUnique operation, we can simply return the first number in the sorted list. This approach has a time complexity of O(n log(n)) for the add operation and O(1) for the showFirstUnique operation.	Our brute force follows these steps:  1) Initialize an empty list to keep track of unique integers.  2) For each integer in the queue, check if it exists in both the queue and the unique list.  3) If the integer is not in the unique list, check if it is unique in the queue by checking the remaining integers after it.  4) If the integer is unique, add it to the unique list.  5) If the integer is not unique, ignore it.  6) After iterating through the queue, return the first element in the unique list as the first unique integer. If the unique list is empty, return -1.  7) Note that this approach has a time complexity of O(n^2) since it requires nested iterations through the queue to check for uniqueness. It is a brute force approach that is not optimized for time complexity but is simple to understand and implement.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPackSolutionsCPP/First%20Unique%20Number.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsPythonSolutions/First%20Unique%20Number.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/CodingInterviewsJavaSolutions/First%20Unique%20Number.java	https://youtu.be/8qM72PPR-mY
