Pack Name	Topic	Problem Name	Problem Link	Question Statement	Time Complexity	Memory Complexity	Question	A	B	C	D	Correct	Message Response For A	Message Response For B	Message Response For C	Message Response For D	Brute Force Approach	C++ Answer	Python Answer	Java Answer	English Solution Video
Algorithms Pack	Sorting	Sort An Array	https://leetcode.com/problems/sort-an-array/	Given an array of integers nums, sort the array in ascending order and return it.    You must solve the problem without using any built-in functions in O(nlog(n)) time complexity and with the smallest space complexity possible.	O(n log(n))	O(1)	We're expecting an optimized solution of time complexity O(n log(n)) and memory complexity O(1).	Selection Sort	Counting Sort	Insertion Sort	Shell Sort	B	Count sort is better.  Selection Sort also has a time complexity of O(n^2), which makes it inefficient for large arrays.  Time Complexity: O(n^2).	Correct!  Counting sort is a sorting algorithm that works by counting the number of occurrences of each value in the input array and then using this information to reconstruct the sorted array. In this approach, we first add a large number to all elements in the input array to ensure that all values are non-negative. We then find the maximum element in the array to determine the size of the counting array. We create a counting array of size (maxValue+1) and use it to count the number of occurrences of each element in the input array. We then update the input array by iterating over the counting array and reconstructing the sorted array.  Time Complexity: O(n+maxValue), where n is the size of the input array and maxValue is the maximum value in the input array.  Memory Complexity: O(maxValue), as we need to create a counting array of size (maxValue+1).	Count sort would be better here.  Insertion Sort has a time complexity of O(n^2) in the worst case, and it requires shifting many elements to insert an element in its correct position, which makes it less efficient than other sorting algorithms.  Time Complexity: O(n^2)	It's not as efficient as the counting sort.  Although Shell Sort has a better average case time complexity than other O(n^2) algorithms, its worst-case time complexity is still O(n^2), which makes it less efficient than other sorting algorithms with better worst-case time complexity.  Time Complexity: O(n^(3/2)) in the worst case.	The brute-force approach is the bubble sort. Obviously, this is almost never the optimal answer for any solution!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Sort%20an%20Array.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Sort%20an%20Array.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Sort%20an%20Array.java	https://youtu.be/rEBj4QtGbZg
Algorithms Pack	Sorting	Minimum Absolute Difference	https://leetcode.com/problems/minimum-absolute-difference/	Given an array of distinct integers arr, find all pairs of elements with the minimum absolute difference of any two elements.    Return a list of pairs in ascending order(with respect to pairs), each pair [a, b] follows    a, b are from arr  a < b  b - a equals to the minimum absolute difference of any two elements in arr	O(n log(n))	O(n)	We're expecting an optimized solution of time complexity O(n log(n)) and memory complexity O(n).	Greedy	Bubble Sort	Divide And Conquer	Sorting + Linear Scanning	D	Not ideal. We'd start with the first element in the array and compare it to all other elements to find the minimum difference. Then, move to the next element and repeat the process. This approach has a time complexity of O(n^2) and is suboptimal because it doesn't guarantee that we will find all pairs with the minimum absolute difference.	No. This approach is inefficient, with a time complexity of O(n^2) in the worst case. Additionally, sorting the array is unnecessary for this problem, and may result in incorrect answers if there are duplicate elements in the array.	This is suboptimal. D&C may not necessarily give the correct answer, because the minimum absolute difference could be between two elements in different subarrays. For example, consider the input [1, 3, 5, 9, 12]. If we divide this array into subarrays [1, 3] and [5, 9, 12], we will miss the minimum absolute difference between 3 and 5.	Correct! However, counting sort is also an excellent way of solving this problem!  We can sort the array in ascending order and then compare adjacent elements to find the minimum absolute difference. We can then iterate through the sorted array again and return all pairs that have this difference.	The brute-force is quite simple.  We can iterate through all possible pairs of elements in the array and calculate their absolute difference. We can then find the minimum absolute difference and return all pairs that have this difference.  Why it's suboptimal: This approach has a time complexity of O(n^2), which is not efficient for large arrays.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Minimum%20Absolute%20Difference.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Minimum%20Absolute%20Difference.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Minimum%20Absolute%20Difference.java	https://youtu.be/EAmEH3ZpRMw
Algorithms Pack	Sorting	Largest Perimeter Triangle	https://leetcode.com/problems/largest-perimeter-triangle/	Given an integer array nums, return the largest perimeter of a triangle with a non-zero area, formed from three of these lengths. If it is impossible to form any triangle of a non-zero area, return 0.	O(n log(n))	O(n)	We're expecting an optimized solution of time complexity O(n log(n)) and memory complexity O(1).	Sorting + Greedy	Hash Table	Binary Search	Dynamic Programming	A	Correct!  We can sort the input array in decreasing order and then traverse the sorted array from left to right, considering each element as the longest side of a potential triangle. For a triangle to be formed, the sum of the other two sides must be greater than the longest side. Therefore, if we find three consecutive elements that satisfy this condition, we can return their sum as the largest perimeter. If no such triplet is found, we return 0.  Time Complexity: O(n log(n))  Memory Complexity: O(n).	This is quite weak.  Create a hash table to store the counts of each integer in the input array. Iterate through all possible pairs of integers and check if there exists a third integer such that they form a valid triangle. Return the maximum perimeter found.  The approach has a time complexity of O(n^2), which is better than the brute-force approach but still worse than the optimal solution. It also requires extra space for the hash table.	Binary search doesn't work as well for this problem.  This approach has a time complexity of O(n^2 log(n)), which is worse than the optimal solution. It also requires sorting and binary search operations.	Not ideal. This is far too expensive an approach, both in terms of time and memory complexity!  To briefly explain a DP approach: we create a 2D array dp[i][j] where dp[i][j] represents the largest valid triangle that can be formed using the values from index i to index j in the input array. Initialize the diagonal values to 0, and then fill in the rest of the array by checking each possible triangle that can be formed from values in the subarray. Return the largest value in the array.	Generate all possible triplets of integers from the input array and check if they form a valid triangle. Keep track of the maximum perimeter found so far.  Why it's suboptimal: This approach has a time complexity of O(n^3), which is very inefficient for large input sizes.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Largest%20Perimeter%20Triangle.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Largest%20Perimeter%20Triangle.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Largest%20Perimeter%20Triangle.java	https://youtu.be/qklQggMJANk
Algorithms Pack	Sorting	Array Partition	https://leetcode.com/problems/array-partition/	Given an integer array nums of 2n integers, group these integers into n pairs (a1, b1), (a2, b2), ..., (an, bn) such that the sum of min(ai, bi) for all i is maximized. Return the maximized sum.	O(n log(n))	O(n)	We're expecting an optimized solution of time complexity O(n log(n)) and memory complexity O(1).	Prefix Sum	Trie	Sorting	Divide And Conquer	C	Not really recommended here!	Trie? Try again!	Correct! However, counting sort is also a good approach here.  Our approach is to first sort the given array 'nums' in non-decreasing order using the STL function 'sort' from C++. We can then group the numbers in pairs (a, b) by iterating over the sorted array with a step of 2, and adding up the minimum of each pair to a running sum. Since the array is sorted, the smaller number of each pair will always be the first element of the pair (i.e., ai = nums[i] and bi = nums[i+1]). Finally, we return the sum.  This approach maximizes the sum of the smaller numbers since we pair them with larger numbers in the array. We can see that any other pairing strategy would result in a smaller sum of minimums since we would be pairing larger numbers with each other, resulting in larger minimums.  Time Complexity: O(n log(n)).  Memory Complexity: O(n).	Sorting is a more intuitive approach. D&C may fail to consider the possibility of larger minimums that can be obtained by pairing elements from different halves.	The brute-force here is truly terrible. We generate all possible pairings of the given 2n integers, compute the sum of minimums for each pairing, and return the maximum sum.  Why it's suboptimal: The brute-force approach is suboptimal because it generates all possible pairings of the given integers, which takes O((2n)!) time. This is prohibitively slow for large values of n, making this approach impractical.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Array%20Partition.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Array%20Partition.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Array%20Partition.java	https://youtu.be/B7zV0naH16s
Algorithms Pack	Sorting	Wiggle Sort	https://leetcode.com/problems/wiggle-sort/	Given an integer array nums, reorder it such that nums[0] <= nums[1] >= nums[2] <= nums[3]....    You may assume the input array always has a valid answer.	O(n log(n))	O(n)	We're expecting an optimized solution of time complexity O(n log(n)) and memory complexity O(1).	Careful Sort + Swap Operations	Bubble Sort	Selection Sort	Insertion Sort	A	Correct!  METHOD: we can sort the input array in ascending order, and swap all adjacent elements (starting from index 1!) with their right neighbor. This will produce the wiggled order expected.  The reason why this works is: if we start with a sorted array in ascending order, the smallest elements will be placed in the even positions (0, 2, 4,...) and the largest elements will be placed in the odd positions (1, 3, 5,...). When we swap the adjacent elements, we are essentially swapping the smallest element in an even position with its neighboring larger element in an odd position, which guarantees the wiggle pattern.  Time Complexity: O(n log(n)), which is dominated by the sorting step. The swapping step takes O(n) time, which is negligible compared to the sorting.  Memory Complexity: O(n).	Inefficient compared to sort and swap!  Bubble Sort can be used to create the wiggled pattern. This would take O(n^2) time complexity in the worst case, and the space complexity would be O(1). However, it takes more time to sort the input array than the optimal solution in most cases, except when the input array is already sorted in ascending order.  Additionally, the problem constraints do not require a fully sorted array, but rather a wiggled array. This sort it more general-purpose and sorts the array completely, so it is not as efficient for the specific problem.	Inefficient compared to sorting and swapping!  Selection Sort can be used to create the wiggled pattern. This would take O(n^2) time complexity in the worst case, and the space complexity would be O(1).   However, it takes more time to sort the input array than the optimal solution in most cases, except when the input array is already sorted in ascending order.  Additionally, the problem constraints do not require a fully sorted array, but rather a wiggled array. This sort it more general-purpose and sorts the array completely, so it is not as efficient for the specific problem.	Inefficient compared to sorting and swapping!  Insertion Sort can be used to create the wiggled pattern. This would take O(n^2) time complexity in the worst case, and the space complexity would be O(1).   However, it takes more time to sort the input array than the optimal solution in most cases, except when the input array is already sorted in ascending order.  Additionally, the problem constraints do not require a fully sorted array, but rather a wiggled array. This sort it more general-purpose and sorts the array completely, so it is not as efficient for the specific problem.	Brute-force approach: one possible naive approach is to generate all possible permutations of the input array and check each permutation to see if it satisfies the wiggle condition.  Time Complexity: O(n!) to generate all permutations, with O(n) time complexity to check each permutation.   Space Complexity: O(n!) to store all permutations.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Wiggle%20Sort.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Wiggle%20Sort.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Wiggle%20Sort.java	https://youtu.be/yejhNBoG2fg
Algorithms Pack	Sorting	Eliminate Maximum Number of Monsters	https://leetcode.com/problems/eliminate-maximum-number-of-monsters/	You are playing a video game where you are defending your city from a group of n monsters. You are given a 0-indexed integer array dist of size n, where dist[i] is the initial distance in kilometers of the ith monster from the city.    The monsters walk toward the city at a constant speed. The speed of each monster is given to you in an integer array speed of size n, where speed[i] is the speed of the ith monster in kilometers per minute.    You have a weapon that, once fully charged, can eliminate a single monster. However, the weapon takes one minute to charge.The weapon is fully charged at the very start.    You lose when any monster reaches your city. If a monster reaches the city at the exact moment the weapon is fully charged, it counts as a loss, and the game ends before you can use your weapon.    Return the maximum number of monsters that you can eliminate before you lose, or n if you can eliminate all the monsters before they reach the city.	O(n log(n))	O(n)	We're expecting an optimized solution of time complexity O(n log(n)) and memory complexity O(n).	Binary Search	Count Sort	Greedy + Sorting	Brute-Force	C	Unfortunately, this BS approach won't help us with these monsters.	Not quite! Well, if you mean 'sort' after calculating the arrival time, then 'count' the monsters, this is a good answer! However, 'Greedy + Sorting' is better here.	Correct!  We first calculate the arrival time for each monster based on their distance and speed, and then sort the arrival times in ascending order. We then iterate through the sorted array and keep count of how many monsters we can eliminate before any one of them reaches the city. We stop counting when either we have eliminated all the monsters or when the number of monsters we have encountered exceeds their earliest arrival time. The idea is to prioritize eliminating the monsters with the earliest arrival time.  Time Complexity: O(nlogn), due to the sorting operation, where n is the number of monsters.  Space Complexity: O(n), because of the extra vector used to store the arrival times. A *priority queue* is a good alternative answer for this problem.	Not quite! It works, but there is at least one better approach.	Brute-force approach: Check every possible elimination combination with loops  DESCRIPTION: Try eliminating each monster one by one, keeping track of the remaining monsters' positions and arrival times. Stop when any monster reaches the city.  Time Complexity: O(n^2)  Space Complexity: O(n).	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Eliminate%20Maximum%20Number%20of%20Monsters.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Eliminate%20Maximum%20Number%20of%20Monsters.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Eliminate%20Maximum%20Number%20of%20Monsters.java	https://youtu.be/GYZ0OC98KPo
Algorithms Pack	Sorting	Maximize Sum Of Array After K Negations	https://leetcode.com/problems/maximize-sum-of-array-after-k-negations/	Given an integer array nums and an integer k, modify the array in the following way:    choose an index i and replace nums[i] with -nums[i].  You should apply this process exactly k times. You may choose the same index i multiple times.    Return the largest possible sum of the array after modifying it in this way.	O(n log(n))	O(n)	We're expecting an optimized solution of time complexity O(n log(n)) and memory complexity O(1).	Divide & Conquer	Sorting + Greedy	Priority Queue	Dynamic Programming	B	This approach would likely fail when k is large enough to completely negate the smaller part, leaving the remaining part with only positive numbers.	Correct!  We first sort the given array to easily apply the negation operation to the smallest values in the array. Then, we iterate through the array and negate the values with a negative sign k times. In each iteration, we decrement k and negate the current number if it's negative. While iterating, we also keep track of the minimum value that has been negated so far. After the iterations, if k is an odd number, we subtract twice the minimum value from the sum to ensure that the remaining k negations are applied to a positive number in the array, thus maximizing the final sum.  Time Complexity: O(n log(n)) due to the sorting operation.  Memory Complexity: O(n).	A little expensive in terms of space complexity. We can use a priority queue to keep track of the smallest values in the array, negate them k times and add them to the sum. This approach is suboptimal because it does not consider the fact that negating a larger positive number can contribute more to the sum than negating a smaller negative number.	Not ideal. We can use dynamic programming to keep track of the maximum sum that can be obtained by applying a certain number of negations to a certain index in the array. We can then use this information to compute the maximum sum by applying k negations to the appropriate indices. This approach is suboptimal because it has a high time complexity.	The brute-force here? We can try all possible combinations of k negations and calculate the sum of each combination. Finally, we can return the maximum sum obtained among all the combinations. This approach is suboptimal because the time complexity is O(2^nk) and thus impractical for even moderate values of n and k.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Maximize%20Sum%20Of%20Array%20After%20K%20Negations.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Maximize%20Sum%20Of%20Array%20After%20K%20Negations.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Maximize%20Sum%20Of%20Array%20After%20K%20Negations.java	https://youtu.be/_5gN7lQJ3UE
Algorithms Pack	Sorting	Shortest Unsorted Continuous Subarray	https://leetcode.com/problems/shortest-unsorted-continuous-subarray/	Given an integer array nums, you need to find one continuous subarray such that if you only sort this subarray in non-decreasing order, then the whole array will be sorted in non-decreasing order.    Return the shortest such subarray and output its length.	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	Sorting + Greedy	Hash Table	Stack	Linear Scanning	D	This isn't the most efficient approach. It's entirely possible to use linear scanning instead for this problem.	Suboptimal due to memory complexity. The approach has a time complexity of O(n), but requires extra memory to store the hash table.  The approach itself is relatively easy to understand: use a hash table to keep track of the indices of each element in the original array, and then find the leftmost and rightmost indices that need to be sorted by comparing the original array with the sorted array.	Not as good as linear scanning due to the extra space required. A stack-based approach would have a time complexity of O(n), but requires extra memory to store the stack - its implementation can be tricky!	Correct!  In our C++ solution, we can call this a 'Two-pass Linear Scan' approach. We scan the array twice, once from left to right, and once from right to left, to find the leftmost and rightmost indices that need to be sorted.  During the first pass, we keep track of the maximum value seen so far. If the current element is less than the maximum, it means that the current element needs to be sorted, so we update the right index.  Similarly, during the second pass, we keep track of the minimum value seen so far. If the current element is greater than the minimum, it means that the current element needs to be sorted, so we update the left index.  Finally, we return the length of the subarray by computing the difference between the right and left indices plus one. If right is still -1, it means that the array is already sorted, so we return 0.  Time Complexity: O(n) since we only need to scan the array twice.  Memory Complexity: O(1) since we only use a constant amount of extra memory to store the left and right indices, and the maximum and minimum values seen so far.	The brute-force approach here is to check every possible subarray to see if it's sorted, and then return the length of the shortest such subarray.  Why it's suboptimal: This approach has a time complexity of O(n^3), which is very inefficient and would not be able to handle large input sizes.  Time Complexity: O(n^3)	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Shortest%20Unsorted%20Continuous%20Subarray.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Shortest%20Unsorted%20Continuous%20Subarray.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Shortest%20Unsorted%20Continuous%20Subarray.java	https://youtu.be/DjMdSYUJLB4
Algorithms Pack	Sorting	Most Profit Assigning Work	https://leetcode.com/problems/most-profit-assigning-work/	You have n jobs and m workers. You are given three arrays: difficulty, profit, and worker where:    difficulty[i] and profit[i] are the difficulty and the profit of the ith job, and  worker[j] is the ability of jth worker (i.e., the jth worker can only complete a job with difficulty at most worker[j]).  Every worker can be assigned at most one job, but one job can be completed multiple times.    For example, if three workers attempt the same job that pays $1, then the total profit will be $3. If a worker cannot complete any job, their profit is $0.  Return the maximum profit we can achieve after assigning the workers to the jobs.	O((n+m) log(n))	O(n)	We're expecting an optimized solution of time complexity O((n+m) log(n)) and memory complexity O(n).	Dynamic Programming	Binary Search	Sorting + Greedy	Union Find	C	Not ideal.  This approach is suboptimal because the time complexity is high (O(nm^2)). It requires filling in a 2D array, which can be computationally expensive for large inputs.	Incorrect! Binary search won't work for this problem - at least, not as optimally as sorting+greedy.	Correct!  We first create a vector of pairs of difficulty and profit, representing each job. Then, we sort the jobs by their difficulty in ascending order, as we want to assign the jobs starting from the easiest ones. We also sort the workers' ability in ascending order.  Then, we iterate through the workers' ability, and for each worker, we assign the job(s) that they can complete with the highest profit. We keep track of the maximum profit seen so far, and add it to the total profit. We repeat this process for all workers.  At each iteration, we advance the job pointer only if the current job difficulty is smaller or equal to the worker's ability. This ensures that we assign jobs with the smallest difficulty to workers that can complete them, and if a worker can complete a harder job, they will still have the option to complete an easier job later on.  Time Complexity: O(n log(n)), where n is the length of the difficulty/profit arrays, due to the sorting step. The loop over the workers takes O(m), where m is the length of the worker array, so the overall time complexity is O((n+m) log(n)).  Memory Complexity: O(n), for the storage of the pairs of difficulty and profit.	Honestly, even if this manages to somehow work, there is clear over-engineering at play here. Sorting+Greedy is the best approach.	In the brute-force approach, we can try every possible combination of job assignments and calculate the total profit. Return the maximum profit.  Why it's suboptimal: The time complexity is very high (O(m^n)), where n is the number of jobs and m is the number of workers. This approach is not feasible for large inputs.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Most%20Profit%20Assigning%20Work.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Most%20Profit%20Assigning%20Work.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Most%20Profit%20Assigning%20Work.java	https://youtu.be/Jf__9kVxhBY
Algorithms Pack	Sorting	Reduction Operations To Make The Array Elements Equal	https://leetcode.com/problems/reduction-operations-to-make-the-array-elements-equal/	Given an integer array nums, your goal is to make all elements in nums equal. To complete one operation, follow these steps:    Find the largest value in nums. Let its index be i (0-indexed) and its value be largest. If there are multiple elements with the largest value, pick the smallest i.  Find the next largest value in nums strictly smaller than largest. Let its value be nextLargest.  Reduce nums[i] to nextLargest.  Return the number of operations to make all elements in nums equal.	O(n log(n))	O(n)	We're expecting an optimized solution of time complexity O(n log(n)) and memory complexity O(n).	Backtracking	Recursion	Sorting + Counting	Binary Search	C	Backtracking would be very inefficient, especially if the array has many duplicate values or if it is already close to being equal. Moreover, it is not guaranteed to find the optimal solution.	Wildly suboptimal.  In this approach, we can recursively try all possible pairs of elements in the array and reduce them to the smaller value. We repeat this process until all elements in the array become equal.  However, this is even slower than the naive brute-force approach since it involves recursion. The time complexity of this approach is O(n^n), where n is the length of the input array. Therefore, it is not a feasible solution for any input size.	Correct!  In our approach, we first sort the given array in descending order. Then, we traverse the array from the second element to the end, and for each element, we compare it with the previous element. If they are not equal, we add the index of the current element to the running count of operations. We add the index of the current element because when we reduce the current element to the value of the previous element, we are actually increasing the number of elements that have the value of the previous element. Thus, we need to keep track of how many elements we have processed so far.  Time Complexity: O(n log(n)), where n is the length of the input array. This is because the sorting operation takes O(n log(n)) time, and the loop that traverses the array takes O(n) time.  Memory Complexity: O(n), where n is the length of the input array. This is because we sort the input array in place, and we only need to store the running count of operations, which takes constant space.	No. This would look and sound like a wrong guess in a real interview.	The brute-force approach here is to check all values sequentially.  We start with the first element of the array and reduce all other elements to its value. Then we move to the next element and repeat the process until all elements in the array are equal.  However, this approach takes O(n^2) time because for each element, we have to compare it with every other element in the array.  Time Complexity: O(n^2).	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Reduction%20Operations%20to%20Make%20the%20Array%20Elements%20Equal.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Reduction%20Operations%20to%20Make%20the%20Array%20Elements%20Equal.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Reduction%20Operations%20to%20Make%20the%20Array%20Elements%20Equal.java	https://youtu.be/IDNxRt1TGYI
Algorithms Pack	Binary Search	Binary Search	https://leetcode.com/problems/binary-search/	Given an array of integers nums which is sorted in ascending order, and an integer target, write a function to search target in nums. If target exists, then return its index. Otherwise, return -1.    You must write an algorithm with O(log(n)) runtime complexity.	O(log(n))	O(1)	We're expecting an optimized solution of time complexity O(log(n)) and memory complexity O(1).	Linear Search	Kruskal's Algorithm	Binary Search	Count Sort	C	The clue is in the name. Use a Binary Search!	No, binary search!	I really hope you got this! Of course, 'binary search' is going to be the correct answer for a problem named 'binary search!'	No! Binary Search!	The brute-force here is very easy. We can simply traverse the array and check if the current element is equal to the target. If it is, we return the current index. Otherwise, we move to the next element. We continue this process until we reach the end of the array.  Why it's suboptimal: This approach has a linear time complexity of O(n), which is not optimal for large arrays.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Binary%20Search.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Binary%20Search.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Binary%20Search.java	https://youtu.be/iUUiAwlPb1M
Algorithms Pack	Binary Search	Find First And Last Position Of Element In Sorted Array	https://leetcode.com/problems/find-first-and-last-position-of-element-in-sorted-array/	Given an array of integers nums sorted in non-decreasing order, find the starting and ending position of a given target value.    If target is not found in the array, return [-1, -1].    You must write an algorithm with O(log(n)) runtime complexity.	O(log(n))	O(1)	We're expecting an optimized solution of time complexity O(log(n)) and memory complexity O(1).	Two Pointers	Binary Search	Hash Table	Linear Search	B	This approach has a time complexity of O(n), which does not meet the requirement of O(log(n)) in the problem statement.  How? Initialize two pointers at the start of the array, and move them towards the middle until the target element is found. Once found, move one pointer left and the other right to find the first and last occurrences of the element.	Correct!  In our approach, we use the built-in STL function equal_range that finds the first occurrence of an element in a sorted range of elements. This function returns a pair of iterators, which can be converted to indices.  We start by applying equal_range on the given vector of integers nums and the target value. If the first iterator of the returned pair is pointing to the end of the vector, we return [-1, -1] as the target value is not present in the given vector. Otherwise, we extract the indices of first and second iterators from the nums vector.  Finally, we check if the value at the first index is equal to the target value. If it's not, we return [-1, -1] as the target value is not present in the given vector. Otherwise, we return the indices of the first and the last occurrence of the target value in the nums vector.  Time Complexity: O(log(n)) - in C++, the equal_range function performs binary search to find the first and the last occurrence of the target value in the given vector.  Memory Complexity: O(1), using constant extra memory to store the indices and the result vector.	This approach requires creating a hash table of the entire array, which has a space complexity of O(n). Additionally, finding the first and last occurrences of the element has a time complexity of O(n), making the overall approach suboptimal with a time complexity of O(n).  How? Create a hash table of the array with element values as keys and indices as values. Find the index of the target element in the hash table, and then expand the search to find the first and last occurrences of the element.	No. The brute-force approach has a time complexity of O(n), which does not meet the requirement of O(log(n)) in the problem statement.	The brute-force here is to simply iterate through the array and find the first and last occurrence of the target element.  However, this approach has a time complexity of O(n), which does not meet the requirement of O(log(n)) in the problem statement.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Find%20First%20and%20Last%20Position%20of%20Element%20in%20Sorted%20Array.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Find%20First%20and%20Last%20Position%20of%20Element%20in%20Sorted%20Array.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Find%20First%20and%20Last%20Position%20of%20Element%20in%20Sorted%20Array.java	https://youtu.be/vzHQBpMyU3o
Algorithms Pack	Binary Search	Find Right Interval	https://leetcode.com/problems/find-right-interval/	You are given an array of intervals, where intervals[i] = [starti, endi] and each starti is unique.    The right interval for an interval i is an interval j such that startj >= endi and startj is minimized. Note that i may equal j.    Return an array of right interval indices for each interval i. If no right interval exists for interval i, then put -1 at index i.	O(n log(n))	O(n)	We're expecting an optimized solution of time complexity O(n log(n)) and memory complexity O(n).	Binary Search + Hash Table	Single Hash Table	Sorting + Linear Scan	Nested Loops	A	Correct! (A heap approach is also possible, with similar complexity!)  Our approach uses a map to store the start index of each interval, where the key is the start point, and the value is the index of the interval in the original array. Then we iterate through each interval in the array and use a binary search to find the index of the smallest start point that is greater than or equal to the end point of the current interval. If such an interval is found, its index is stored in the resulting array at the corresponding index of the current interval. If not, the resulting array is set to -1 at the index of the current interval.  Time Complexity: the map insertion operation takes O(log(n)) time, and we insert n intervals, so the total time is O(n log(n)). The binary search also takes O(log(n)) time, and we perform it n times, so the total time is O(n log(n)). Therefore, the time complexity of our approach is O(n log(n)).  Memory Complexity:  We store the start index of each interval in the map, so the memory required for the map is O(n). We also store the resulting array of size n. Therefore, the memory complexity of our approach is O(n).	No, not quite.  Although this approach can be faster than the nested loop approach in some cases, it still has a worst-case time complexity of O(n^2) if all intervals have the same start point.	A binary search and hash table is better. This still has a time complexity of O(n^2) in the worst case because the linear search may have to examine all the intervals in the sorted array.	Very rarely optimal - and not optimal here! This is basically the brute-force approach.	Iterate through each interval and, for each interval, iterate through all subsequent intervals to find the right interval (if it exists).  This approach has a time complexity of O(n^2), which is very inefficient for large values of n. For example, if n = 10^4, this approach would require 100 million iterations.  Time Complexity: O(n^2).	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Find%20Right%20Interval.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Find%20Right%20Interval.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Find%20Right%20Interval.java	https://youtu.be/erDK63JK14A
Algorithms Pack	Binary Search	Valid Triangle Number	https://leetcode.com/problems/valid-triangle-number/	Given an integer array nums, return the number of triplets chosen from the array that can make triangles if we take them as side lengths of a triangle.	O(n^2)	O(n)	We're expecting an optimized solution of time complexity O(n^2) and memory complexity O(1).	Binary Search	Dynamic Programming	Hash Table	Sorting + Two Pointers	D	Not ideal here.  This approach would have a time complexity of O(n^2 log(n)), which is better than brute force but still not optimal. It requires sorting the array and binary search for every combination of two sides, which can take a lot of time for larger inputs.  Time Complexity: O(n^2 log(n))	Such an approach would have a time complexity of O(n^2), but it requires additional memory to store the dynamic programming table, which can be problematic for large input sizes.	Not optimal due to the space complexity.  The idea? Use a hash table to count the frequency of each number in the array, and then for every possible pair of sides, check if the sum of those sides is less than or equal to any other number in the array. Although this approach has a time complexity of O(n^2), it requires additional memory to store the hash table, which can be problematic for large input sizes.	Correct!  The two-pointer approach, also known as the two-sum approach, is a common technique to solve problems that involve sorted arrays. In this approach, we first sort the array and iterate through each element in the array. For each element, we initialize two pointers, one pointing to the next element and the other pointing to the end of the array. We then move the pointers inwards, comparing the sum of the elements at the two pointers with the current element. If the sum is greater than the current element, we decrement the right pointer. If the sum is smaller than the current element, we increment the left pointer. If the sum is equal to the current element, we increment the count and move both pointers inwards. We repeat this process until the two pointers meet.  In the given problem, we use the two-pointer approach to find the number of triplets that can make triangles. We sort the array and iterate through each element, setting two pointers, one pointing to the next element and the other pointing to the end of the array. We then move the pointers inwards, comparing the sum of the elements at the two pointers with the current element. If the sum is greater than the current element, we increment the count by the difference between the right pointer and left pointer, move the right pointer inward, and repeat the process. If the sum is not greater than the current element, we move the left pointer inward and repeat the process. We return the final count.  Time Complexity: O(n^2), where n is the size of the array. Sorting the array takes O(n log(n)) time, and the nested loop takes O(n^2) time.  Memory Complexity: O(1). We are only using a constant amount of extra memory for the pointers and the count.	The brute-force approach is very simple: iterate through all possible triplets of numbers in the array and check if they can form a triangle.  Why it's suboptimal: This approach has a time complexity of O(n^3) which makes it inefficient for larger input sizes.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Valid%20Triangle%20Number.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Valid%20Triangle%20Number.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Valid%20Triangle%20Number.java	https://youtu.be/z0MrIl0HM7Q
Algorithms Pack	Binary Search	Arranging Coins	https://leetcode.com/problems/arranging-coins/	You have n coins and you want to build a staircase with these coins. The staircase consists of k rows where the ith row has exactly i coins. The last row of the staircase may be incomplete.    Given the integer n, return the number of complete rows of the staircase you will build.	O(log(n))	O(1)	We're expecting an optimized solution of time complexity O(log(n)) and memory complexity O(1).	Binary Search	Linear Search	Hash Table	Topological Sort	A	Correct! However, there IS a formula-based approach too (not mentioned here).  Our approach is to use binary search to find the number of rows that can be fully constructed using the given coins. We first define a helper function possible that takes in the number of coins and the number of rows and returns whether it's possible to construct row number of rows with coins number of coins. Using this helper function, we start with the entire range of possible rows i.e., 0 to coins, and then perform binary search. For each iteration, we calculate the mid row and check if it's possible to construct mid rows using the given number of coins. If it is possible, we update the answer and move to the right half of the range. Otherwise, we move to the left half of the range.  The binary search approach allows us to find the answer in O(log(n)) time complexity. Since we are using only constant extra memory, the space complexity is O(1).	Suboptimal. This brute-force approach has a time complexity of O(n), which is not optimal for large values of n.	Suboptimal. This would be a very expensive approach in terms of space complexity.	No chance!	The brute-force approach is to iterate over all possible rows and return the maximum number of rows such that the total number of coins in the rows is less than or equal to n.  Why it's suboptimal: This approach has a time complexity of O(n), which is not optimal for large values of n.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Arranging%20Coins.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Arranging%20Coins.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Arranging%20Coins.java	https://youtu.be/xiF8dy67tRM
Algorithms Pack	Binary Search	Find The Smallest Divisor Given A Threshold	https://leetcode.com/problems/find-the-smallest-divisor-given-a-threshold/	Given an array of integers nums and an integer threshold, we will choose a positive integer divisor, divide all the array by it, and sum the division's result. Find the smallest divisor such that the result mentioned above is less than or equal to threshold.    Each result of the division is rounded to the nearest integer greater than or equal to that element. (For example: 7/3 = 3 and 10/2 = 5).    The test cases are generated so that there will be an answer.	O(n log(max(nums)))	O(1)	We're expecting an optimized solution of time complexity O(n log(max(nums))) and memory complexity O(1).	Binary Search	Priority Queue	Sorting	Hash Table + Linear Scan	A	Correct!  We use binary search to find the smallest divisor which satisfies the condition mentioned in the problem statement. We first find the maximum element in the array, which is an upper bound on the possible divisors. We then use binary search to find the smallest divisor. For a given divisor, we find the sum of the ceiling of each element divided by the divisor. If this sum is less than or equal to the threshold, we update our answer and move to a smaller divisor. Otherwise, we move to a larger divisor.  Time Complexity: O(n log(max(nums))) where n is the size of the nums vector, and max(nums) is the maximum element in the nums vector.  Memory Complexity: O(1), as we use constant extra space in the form of a few variables to store our intermediate results.	Not quite!  Method: Use a priority queue to keep track of the elements whose ceiling when divided by the divisor is the largest, and keep popping elements until the sum of the ceiling of each element divided by the divisor is less than or equal to the threshold.  Weakness: Less efficient than a binary search!   Time Complexity: O(n log(n) + n log(max(nums)))  Memory Complexity: O(n).	Less efficient than other solutions.  Method: Sort the array in descending order and try all possible divisors from 1 to the maximum element in the array.  Weakness: less efficient than binary search  Time Complexity: O(n log(n) + n log(max(nums)))  Memory Complexity: O(1).	This is not at all an optimal approach for this problem.	Brute-Force Approach: iterate over all possible divisors from 1 to max(nums) and compute the sum for each divisor. Return the smallest divisor which satisfies the condition mentioned in the problem statement.  Time Complexity: O(max(nums) * n) where n is the size of the nums vector.  Memory Complexity: O(1).	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Find%20the%20Smallest%20Divisor%20Given%20a%20Threshold.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Find%20the%20Smallest%20Divisor%20Given%20a%20Threshold.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Find%20the%20Smallest%20Divisor%20Given%20a%20Threshold.java	https://youtu.be/oCaiNNOcVLs
Algorithms Pack	Binary Search	Minimum Number Of Days To Make M Bouquets	https://leetcode.com/problems/minimum-number-of-days-to-make-m-bouquets/	You are given an integer array bloomDay, an integer m and an integer k.    You want to make m bouquets. To make a bouquet, you need to use k adjacent flowers from the garden.    The garden consists of n flowers, the ith flower will bloom in the bloomDay[i] and then can be used in exactly one bouquet.    Return the minimum number of days you need to wait to be able to make m bouquets from the garden. If it is impossible to make m bouquets return -1.	O(n log(n))	O(1)	We're expecting an optimized solution of time complexity O(n log(n)) and memory complexity O(1).	Sorting + Linear Search	Binary Search	Sliding Window	DFS	B	Binary search is possible, and would be much better.  This approach does not consider the possibility that there may be no k adjacent flowers available to make a bouquet. It also does not guarantee the minimum number of days required to make m bouquets. Its time complexity is O(n^2) due to the sorting process and the inner loop to check for previous bouquets.	Correct!  We can indeed use a binary search approach to find the answer. First, we set the lower bound to 1 and the upper bound to 10^9, which represents the maximum possible value of days. Then, for each iteration, we calculate the mid value of the range and check if it is possible to make m bouquets in k adjacent flowers in mid days. If it is possible, we set the upper bound to mid-1 and save the mid value as the new answer. Otherwise, we set the lower bound to mid+1.  To check if it is possible to make m bouquets in k adjacent flowers in mid days, we iterate over the bloomDay array and count the number of adjacent flowers bloomed within mid days. We keep track of the number of bouquets made and the number of adjacent flowers used for each bouquet. If the number of bouquets made is less than m, we return false; otherwise, we return true.  Time Complexity: O(n * log(max(bloomDay))), where n is the size of the bloomDay array, and max(bloomDay) is the maximum value in the array. The while loop runs log(max(bloomDay)) times, and for each iteration, we check if it is possible to make m bouquets, which takes O(n) time.  Memory Complexity: O(1), as we are not using any additional data structure to solve the problem.	Not optimal.  This approach does not guarantee the minimum number of days required to make m bouquets, as the minimum bloomDay for each bouquet may not be optimal. It also does not consider the possibility that there may be no k adjacent flowers available to make a bouquet. Its time complexity is O(nmk) due to the nested loops for the sliding window and checking for previous bouquets.	Not the best here.  While possible, a DFS approach would be time-consuming, especially when k and m are large.	The brute-force solution we are offering here would be particularly wasteful. We can enumerate all possible combinations of bouquets using k adjacent flowers from the garden and check if each combination is valid to make a bouquet. We can start by choosing the first k flowers as the first bouquet, the next k flowers as the second bouquet, and so on, until we have m bouquets. Then, we can check if each bouquet is valid by comparing the bloomDay of the flowers used in the bouquet.  This approach has an exponential time complexity of O(n^k) since we are enumerating all possible combinations of k adjacent flowers.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Minimum%20Number%20of%20Days%20to%20Make%20m%20Bouquets.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Minimum%20Number%20of%20Days%20to%20Make%20m%20Bouquets.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Minimum%20Number%20of%20Days%20to%20Make%20m%20Bouquets.java	https://youtu.be/dHyF855ezUA
Algorithms Pack	Binary Search	Heaters	https://leetcode.com/problems/heaters/	Winter is coming! During the contest, your first job is to design a standard heater with a fixed warm radius to warm all the houses.    Every house can be warmed, as long as the house is within the heater's warm radius range.     Given the positions of houses and heaters on a horizontal line, return the minimum radius standard of heaters so that those heaters could cover all houses.    Notice that all the heaters follow your radius standard, and the warm radius will the same.	O(n log(n) + m log m)	O(1)	We're expecting an optimized solution of time complexity O(n log(n)) + O(m log m) and memory complexity O(1).	Greedy	Hash Table	Sorting + Binary Search	Dynamic Programming	C	Not quite!  This approach may not produce the optimal result. It may not consider heaters that are not the closest but still cover more houses. For example, if two heaters can each cover half the houses, but are farther away than the closest heater, this approach will miss them.  The approach? Sort the heater positions in increasing order. For each house, find the closest heater that is to the right of the house. If there is no such heater, find the closest heater that is to the left of the house. Calculate the distance between the house and the heater, and take the maximum distance as the minimum radius required to cover all houses.	Suboptimal.  This would be far too expensive in terms of space complexity.	Correct!  This approach involves binary search to find the minimum radius required to cover all the houses using the heaters. First, we sort the houses and heaters in ascending order. Then, we initialize the range of radius as 0 to 1e9 (a very large value). We check if it is possible to cover all the houses with the given radius using the possible() function. If yes, then we update the end of the range to mid-1 and update the radius to mid. Else, we update the start of the range to mid+1. We repeat this process until start becomes greater than end. Finally, we return the minimum radius that can cover all the houses.  Time Complexity: O(n log(n)), where n is the maximum of the size of houses and heaters. This is because of the sorting and binary search involved.  Memory Complexity: O(1), since we are using constant extra space.	Nope!  This approach does not work for the given problem because the optimal placement of heaters is not dependent on the previous placements. Each heater's placement is independent of the previous heaters' placement.	The brute-force approach here involves checking ALL possible heater radii, from 0 to the maximum distance between the leftmost and rightmost houses. We can then find the minimum radius that covers all the houses.  Why it's suboptimal: This approach has a time complexity of O(n^2), where n is the number of houses. As n can be as large as 10^4, this approach can take a long time to run and is not feasible for large inputs.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Heaters.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Heaters.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Heaters.java	https://youtu.be/XodnzXl_--A
Algorithms Pack	Binary Search	Missing Element in Sorted Array	https://leetcode.com/problems/missing-element-in-sorted-array/	Given an integer array nums which is sorted in ascending order and all of its elements are unique and given also an integer k, return the kth missing number starting from the leftmost number of the array.	O(log(n))	O(1)	We're expecting an optimized solution of time complexity O(log(n)) and memory complexity O(1).	Binary Search + Linear Search	Hash Table	Two Binary Searches	Binary Search	D	Convoluted!  Method: use binary search to find the index where the kth missing number should be, then perform a linear search from that index to find the kth missing number.  Weakness: while binary search can help us find the index where the kth missing number should be, performing a linear search from that index to find the kth missing number can be inefficient, especially if k is large. O(log(n)+ k) in time complexity.	Method: create a hash table to store all the numbers in the array, then iterate through the array, checking for missing numbers in the hash table until we find the kth missing number.  Weakness: both time and space complexity are worse, at O(n).	Unnecessary complexity involved!  Method: use binary search to find the index where the kth missing number should be, then use binary search again to find the kth missing number starting from that index.  Weakness: performing Binary Search twice is a BS approach for this problem. It gives a time complexity of O(log(n) * log k), which is significantly worse than the optimal approach.	Correct! First, we found it useful to define two helper functions - 'missing' and 'kth'. The 'missing' function calculates the number of missing elements from the leftmost element of the array up to a given index. The 'kth' function returns the k-th missing element starting from the given index.  Then, we check if the number of missing elements from the last element of the array is less than k. If it is, we can find the k-th missing element starting from the last element. Otherwise, we perform binary search on the array to find the index where the k-th missing element would be located. We update the start and end indices of the search based on whether the number of missing elements up to the mid index is less than or greater than k. We keep track of the latest index where the number of missing elements is less than or equal to k. Finally, we use the 'kth' function to return the k-th missing element starting from the latest index where the number of missing elements is less than or equal to k.  Time Complexity: O(log(n)) because of the binary search, where n is the size of the input array.  Space Complexity: O(1), because we only use a constant amount of additional memory for our solution.	Brute-Force Approach: Linear search  Description: Iterate through the array, counting the missing numbers until we find the kth missing number.  Time complexity: O(n), where n is the length of the array.  Space complexity: O(1), since we don't need to store any extra data.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Missing%20Element%20in%20Sorted%20Array.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Missing%20Element%20in%20Sorted%20Array.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Missing%20Element%20in%20Sorted%20Array.java	https://youtu.be/Ucvaef4ySnc
Algorithms Pack	Binary Search	Kth Smallest Number in Multiplication Table	https://leetcode.com/problems/kth-smallest-number-in-multiplication-table/	Nearly everyone has used the Multiplication Table. The multiplication table of size m x n is an integer matrix mat where mat[i][j] == i * j (1-indexed).    Given three integers m, n, and k, return the kth smallest element in the m x n multiplication table.	O(m log(nm))	O(1)	We're expecting an optimized solution of time complexity O(m log(nm)) and memory complexity O(1).	Min-Heap	Binary Search	Sorting	Optimized Count Sort	B	Suboptimal. The idea is to use a heap to store the elements in the multiplication table in increasing order. We would then pop k elements from the heap and return the kth smallest element. This would have a time complexity of O(mnlog(m*n)) which is the same as the brute force approach.	Correct - well done!  Our approach is to use binary search to find the kth smallest element in the m x n multiplication table. We first set the start to 1 and the end to m x n, then we compute the mid value, and use a helper function 'lessEqCount' to count the number of elements in the table less than or equal to mid. If the count is less than k, then we update the start to mid + 1. Otherwise, we update the end to mid - 1 and update the answer to mid. We repeat this process until the start and end meet.	This apporach would yield a poor time complexity, especially when compared to the binary search.	Not a good option: binary search is the best approach by far.	Brute-force: we could solve this problem by simply iterating over all elements in the table and sorting them, and then returning the kth smallest element. This would have a time complexity of O(mnlog(m*n)) which is too slow for large values of m and n.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Kth%20Smallest%20Number%20in%20Multiplication%20Table.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Kth%20Smallest%20Number%20in%20Multiplication%20Table.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Kth%20Smallest%20Number%20in%20Multiplication%20Table.java	https://youtu.be/pwv3di65RJY
Algorithms Pack	Binary Search	Sqrt(x)	https://leetcode.com/problems/sqrtx/	Given a non-negative integer x, return the square root of x rounded down to the nearest integer. The returned integer should be non-negative as well.    You must not use any built-in exponent function or operator.    For example, do not use pow(x, 0.5) in c++ or x ** 0.5 in python.	O(log(n))	O(1)	We're expecting an optimized solution of time complexity O(log(n)) and memory complexity O(1).	Linear Search	Binary Search	Inbuilt Exponent Function	Binary Search + Linear Search	B	Method: Use a linear search: We can start with a guess of 0 and increment it until we find the smallest square greater than x, and then return the previous integer value as the square root of x. This approach has a time complexity of O(sqrt(x)), which is slower than binary search.    Weakness: Using a linear search is slower than binary search, especially for large values of x. It also requires checking each integer value up to the square root of x, which can be computationally expensive for large values of x.	Correct! A very nice solution, only surpassed by 'Newton's Method', which is also interesting to look up! Our solution uses the binary search algorithm to find the square root of a non-negative integer x, rounded down to the nearest integer. We start with a lower bound of 0 and an upper bound of x, and then repeatedly calculate the midpoint between these bounds. We then check whether the square of the midpoint is greater than or less than x. If the square of the midpoint is less than x, we update the lower bound to be the midpoint; otherwise, we update the upper bound to be the midpoint. We continue this process until the difference between the upper and lower bounds is less than a small epsilon value, at which point we return the lower bound plus the epsilon value as the square root of x rounded down to the nearest integer. This approach is commonly known as the binary search method for finding square roots, or the 'Babylonian method' since it is an ancient algorithm that was used by Babylonian mathematicians.   Time complexity: O(log(x)), since we use binary search to converge on the correct answer.  Space complexity is O(1), since we only need to store a few variables for the algorithm.	Oops! Read the question carefully!	Combining these two approaches here is going to be way less efficient than simply using one or the other.	Brute-force: We can start with a guess of 0 and increment it until we find the largest square less than or equal to x. This approach has a time complexity of O(sqrt(x)), since we need to try all possible integers up to the square root of x.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Sqrt(x).cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Sqrt(x).py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Sqrt(x).java	https://youtu.be/NHnl8iqG4O8
Algorithms Pack	Depth-First Search	Kill Process	https://leetcode.com/problems/kill-process/	You have n processes forming a rooted tree structure. You are given two integer arrays pid and ppid, where pid[i] is the ID of the ith process and ppid[i] is the ID of the ith process's parent process.    Each process has only one parent process but may have multiple children processes. Only one process has ppid[i] = 0, which means this process has no parent process (the root of the tree).    When a process is killed, all of its children processes will also be killed.    Given an integer kill representing the ID of a process you want to kill, return a list of the IDs of the processes that will be killed. You may return the answer in any order.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Topological Sorting	Linear Search	Inorder Traversal	DFS	D	Incorrect!  We don't have a guaranteed DAG here, so topological sort is not a realistic option.	Too slow. The idea is okay. We iterate over the list of processes and check if each process is a descendant of the process to be killed. If yes, we add it to the result.  Why it's suboptimal: This approach is inefficient for large trees since it checks all processes, even if they are not descendants of the process to be killed. Time Complexity: O(n^2), where n is the total number of processes.	Suboptimal.  First of all, inorder traversal assumes a binary tree, which is not guaranteed in a process tree. Hence, the inorder traversal can be incorrect in some cases.	Correct!  DFS can traverse the process tree starting from the process to be killed. First, we build a graph using a hash table where each process ID maps to a list of its children's IDs. Then, starting from the process to be killed, we perform a DFS and add all visited nodes to a set. The set contains all the processes that need to be killed.  Time Complexity: O(n), where n is the total number of processes. The algorithm builds a graph by iterating over all the processes once, which takes O(n) time. The DFS function traverses the tree, visiting each node at most once. Hence, it takes O(n) time. The set construction from visited nodes takes O(n) time in the worst case.  Memory Complexity: O(n), where n is the total number of processes.	A brute-force here?  : Traverse the process tree for every process to identify which processes need to be killed.  Why it's suboptimal: This approach is suboptimal because it requires traversing the entire tree for each process. Hence, the time complexity is O(n^2), where n is the total number of processes.  Time Complexity: O(n^2)	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Kill%20Process.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Kill%20Process.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Kill%20Process.java	https://youtu.be/bG4VPzYB1Uo
Algorithms Pack	Depth-First Search	Employee Importance	https://leetcode.com/problems/employee-importance/	You have a data structure of employee information, including the employee's unique ID, importance value, and direct subordinates' IDs.    You are given an array of employees employees where:    employees[i].id is the ID of the ith employee.  employees[i].importance is the importance value of the ith employee.  employees[i].subordinates is a list of the IDs of the direct subordinates of the ith employee.  Given an integer id that represents an employee's ID, return the total importance value of this employee and all their direct and indirect subordinates.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	DFS + Hash Table	Recursive Traversal	BFS Without Hash Table	Union Find	A	Correct!  In our approach, we use depth-first search (DFS) to traverse the hierarchy of employees starting from the given employee ID. We use an hash table to map each employee's ID to the corresponding Employee object for quick access.  We start by fetching the Employee object for the given ID and add its importance value to the result. We then recursively call the dfs() function on each of its subordinates and add their importance values to the result.  We repeat this process for each subordinate, continuing to traverse the hierarchy of employees until we have visited every direct and indirect subordinate. We return the final result, which is the total importance value of the given employee and all their direct and indirect subordinates.  Time Complexity: O(n), where n is the total number of employees in the organization. We traverse each employee at most once.  Memory Complexity: O(n), where n is the total number of employees in the organization. We store the Employee objects in an hash table for quick access.	Suboptimal. This is identical to the brute-force approach.	Better to include a hash table. Starting from the employee with the given ID, use BFS to traverse the hierarchy and add up the importance values of all visited employees. This approach is suboptimal because it can result in duplicate work, since the same employee can be visited multiple times from different paths in the hierarchy. Its time complexity is O(n^2) in the worst case, where n is the number of employees.	Union Find is overkill for this problem, and there are simpler solutions that don't require building a graph and using a data structure like Union Find to solve it. This solution also requires some additional setup time to build the graph and perform the Union Find operations, which could be avoided with a simpler solution.	Brute-Force Approach:  For each employee, recursively traverse its direct and indirect subordinates, adding up their importance values.  Time Complexity: O(n^2) in the worst case, where n is the number of employees.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Employee%20Importance.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Employee%20Importance.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Employee%20Importance.java	https://youtu.be/V1FvJHDfdaw
Algorithms Pack	Depth-First Search	Number Of Connected Components in an Undirected Graph	https://leetcode.com/problems/number-of-connected-components-in-an-undirected-graph/	You have a graph of n nodes. You are given an integer n and an array edges where edges[i] = [ai, bi] indicates that there is an edge between ai and bi in the graph.    Return the number of connected components in the graph.	O(n+m)	O(n+m)	We're expecting an optimized solution of time complexity O(n+m) and memory complexity O(n+m).	Hash Set of Hash Sets	Kruskal's Algorithm	DFS	Matrix Multiplication	C	As older people sometimes say in English, this is really making a complete hash of things….use DFS instead!	A little inefficient! This approach involves using Kruskal's algorithm for finding the minimum spanning tree of the graph, and then counting the number of connected components in the tree. This approach has a time complexity of O(m log m), where m is the number of edges in the graph. However, it is not guaranteed to work correctly if the graph is not connected, and it may also be less efficient than the other approaches in practice.	Correct!  To do this, we first create an undirected graph using the edges given in the input. Then, we traverse the graph using a depth-first search algorithm to count the number of connected components in the graph. We do this by starting from an unvisited node and traversing all of its neighbors, marking them as visited as we go. We continue until all nodes in the connected component have been visited, then we increment our count of connected components and move on to the next unvisited node. Once we have visited all nodes in the graph, our count of connected components will be the final answer.  Time Complexity: O(n + m)  Memory Complexity: O(n + m) because we store the graph as an adjacency list with one vector for each node and a vector of pairs for each edge.	Highly inefficient! Build an adjacency matrix from the given edges and perform matrix multiplication with the matrix raised to the power of the number of nodes in the graph. Count the number of non-zero entries in the resulting matrix.  Time Complexity: O(n^3 log(n))  Memory Complexity: O(n^2).	Brute-force: we can start by generating all possible subsets of the nodes in the graph, and for each subset, check if all the nodes are connected. This can be done by performing a depth-first search or breadth-first search on the subset and checking if all the nodes are visited.  Time complexity: O(2^n * n^2), where n is the number of nodes in the graph.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Number%20of%20Connected%20Components%20in%20an%20Undirected%20Graph.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Number%20of%20Connected%20Components%20in%20an%20Undirected%20Graph.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Number%20of%20Connected%20Components%20in%20an%20Undirected%20Graph.java	https://youtu.be/RZYpd3ZlfAM
Algorithms Pack	Depth-First Search	Flood Fill	https://leetcode.com/problems/flood-fill/	An image is represented by an m x n integer grid image where image[i][j] represents the pixel value of the image.    You are also given three integers sr, sc, and color. You should perform a flood fill on the image starting from the pixel image[sr][sc].    To perform a flood fill, consider the starting pixel, plus any pixels connected 4-directionally to the starting pixel of the same color as the starting pixel, plus any pixels connected 4-directionally to those pixels (also with the same color), and so on. Replace the color of all of the aforementioned pixels with color.    Return the modified image after performing the flood fill.	O(mn)	O(mn)	We're expecting an optimized solution of time complexity O(mn) and memory complexity O(mn).	Dynamic Programming	DFS	Topological Sort	Rabin-Karp Algorithm	B	Not needed here.	Correct! Look up the flood fill algorithm if you're not already familiar with it! The memory complexity here is far from optimal, but this problem is included to give a basic demonstration of the flood fill itself. In this approach, we start from the given starting point (sr, sc) and perform a DFS traversal. During traversal, we mark all the pixels which are connected to the starting pixel and have the same color as the starting pixel. We replace the color of all such pixels with the new color.  We use a helper function named isValid() to check if the given pixel is a valid pixel or not. We also use a vector visited to mark the visited pixels.  Time Complexity: O(m * n), where m is the number of rows and n is the number of columns of the image. In the worst case, we might have to visit all the pixels of the image.  Memory Complexity: O(m * n), where m is the number of rows and n is the number of columns of the image. We use a boolean visited array of size m * n to keep track of the visited pixels.	No, not used at all!	Not used here.	The brute-force solution is very similar to the one we offer as a demonstration here.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Flood%20Fill.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Flood%20Fill.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Flood%20Fill.java	https://youtu.be/s6_NsNHCEOw
Algorithms Pack	Depth-First Search	Count Sub Islands	https://leetcode.com/problems/count-sub-islands/	You are given two m x n binary matrices grid1 and grid2 containing only 0's (representing water) and 1's (representing land). An island is a group of 1's connected 4-directionally (horizontal or vertical). Any cells outside of the grid are considered water cells.    An island in grid2 is considered a sub-island if there is an island in grid1 that contains all the cells that make up this island in grid2.    Return the number of islands in grid2 that are considered sub-islands.	O(mn)	O(mn)	We're expecting an optimized solution of time complexity O(mn) and memory complexity O(mn).	Flood Fill	Union Find	DFS	Brute-Force + Memoization	C	Suboptimal because it still checks every cell of *each island* in grid2, which is unnecessary since we can use DFS to visit all cells of an island in linear time.  Approach: Perform a flood-fill algorithm for each island in grid2 to find all the cells that belong to the island, then check if all the cells also belong to the corresponding island in grid1.  Time complexity: O(mn*(m+n)) where m and n are the number of rows and columns in the matrix respectively. This is because for each cell in grid2 that belongs to an island, we perform a flood-fill algorithm that traverses at most m+n cells.  Memory complexity: O(mn)	Suboptimal because union-find has a time complexity of O(α(mn)), which is slower than DFS in the worst-case scenario where α is the inverse Ackermann function.  Approach: Create disjoint sets for each island in grid2, then merge cells in the same island. For each set of cells in grid2, check if it is a subset of any set in grid1.  Time complexity: O(mnα(mn)) where m and n are the number of rows and columns in the matrix respectively, and α is the inverse Ackermann function. This is because we use a union-find data structure to merge cells into sets, and the worst-case time complexity of union-find is O(α(mn)).  Memory complexity: O(mn).	Correct!  We traverse through each cell of grid2 that contains a 1 and perform a DFS to check if it is a sub-island of grid1. To do this, we simply check if the current cell is a valid cell and if it contains a 1. If it does, we check if the corresponding cell in grid1 also contains a 1. If it does not, the current island is not a sub-island, and we return. If it does, we mark the current cell as visited in grid2 and recursively check its neighboring cells. We continue until we have visited all the cells of the current island. If at any point during the traversal, we encounter a cell that is not a part of the corresponding island in grid1, we mark the island as false and return.  Time Complexity: O(mn), where m and n are the number of rows and columns in grid2, respectively.  Memory Complexity: O(mn), where m and n are the number of rows and columns in grid2, respectively.  Additionally, the DFS recursion stack also uses O(mn) space in the worst case scenario, where all cells are part of a single island.	Highly inefficient!  Approach: For each cell in grid2, memoize whether the cell belongs to an island, and whether that island is a sub-island of grid1. To memoize, use a hash table to store visited islands of grid2, and check if an island in grid2 is a subset of any visited island in grid1.  Time complexity: O(m^2n^2) where m and n are the number of rows and columns in the matrix respectively. This is because we need to check every cell of every island in grid2 and compare it to every visited island in grid1.  Memory complexity: O(mn) for the hash table.	Brute-force approach: check every island in grid2 to see if it is a sub-island of grid1 by comparing each cell in the island to the corresponding cell in the island of grid1.  Time complexity: O(m^2n^2) where m and n are the number of rows and columns in the matrix respectively. This is because we need to check every cell of every island in grid2 and compare it to every cell of the corresponding island in grid1.  Memory complexity: O(1)...but look at the time complexity!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Count%20Sub%20Islands.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Count%20Sub%20Islands.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Count%20Sub%20Islands.java	https://youtu.be/WDsH02DSRUQ
Algorithms Pack	Depth-First Search	Coloring A Border	https://leetcode.com/problems/coloring-a-border/	You are given an m x n integer matrix grid, and three integers row, col, and color. Each value in the grid represents the color of the grid square at that location.    Two squares belong to the same connected component if they have the same color and are next to each other in any of the 4 directions.    The border of a connected component is all the squares in the connected component that are either 4-directionally adjacent to a square not in the component, or on the boundary of the grid (the first or last row or column).    You should color the border of the connected component that contains the square grid[row][col] with color.    Return the final grid.	O(mn)	O(mn)	We're expecting an optimized solution of time complexity O(mn) and memory complexity O(mn).	Union-Find	Flood Fill	DFS	Edge-Detection	C	Not quite optimal!  Method: Use a union-find data structure to find the connected component containing the specified cell. During the union-find, keep track of which cells are on the border of the component. Finally, color all the border cells with the specified color.   Weakness: Time complexity: O(m * n * α(m * n)) where α is the inverse Ackermann function. Space complexity: O(m * n).	This adds a little unnecessary complexity. A more general DFS is better.	Correct!  Method: our solution uses a depth-first search (DFS) algorithm to identify the connected component that contains the given square at (row, col) and mark it as visited. Then, we iterate over the entire grid and color in the border of the visited component with the given color. To do this, we check if a cell is adjacent to an unvisited cell or is on the border of the grid, and if so, we color it in.  Time Complexity: O(mn), since we visit every cell of the grid at most once.  Memory Complexity: O(mn), since we use a visited matrix of the same size as the grid.	Controversially, no!    Method: iterate through the border of the grid, and for each cell on the border, check if it is part of a connected component with the same color as the specified cell. If it is, color that cell with the specified color. Time complexity: O(m + n) where m and n are the dimensions of the grid. Space complexity: O(1).  Weakness: the edge-detection approach is efficient in terms of both time and space complexity, but it only works if the specified cell is on the border of the grid. If the cell is in the interior of the grid, this approach will not work.	Brute-force approach: Iterate through every cell in the grid, and for each cell, check if it is on the border of a connected component. If it is, color that cell with the specified color. Time complexity: O(m * n * max(m, n)) where m and n are the dimensions of the grid. Space complexity: O(1).	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Coloring%20A%20Border.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Coloring%20A%20Border.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Coloring%20A%20Border.java	https://youtu.be/u_ss6h5ycf4
Algorithms Pack	Depth-First Search	Number of Closed Islands	https://leetcode.com/problems/number-of-closed-islands/	Given a 2D grid consists of 0s (land) and 1s (water). An island is a maximal 4-directionally connected group of 0s and a closed island is an island totally (all left, top, right, bottom) surrounded by 1s.    Return the number of closed islands.	O(mn)	O(mn)	We're expecting an optimized solution of time complexity O(mn) and memory complexity O(mn).	Union-Find	DFS	Floyd-Warshall	Rabin-Karp	B	Plausible, but not best here!  Method: initialize a union-find data structure where each cell is a separate set. For each 0 cell, check its neighbors and union their sets if they're also 0s. Finally, count the number of sets that are closed islands.  Weakness: this solution has a time complexity of O(m*n) since union-find operations take O(1) time on average. However, it requires extra space to store the union-find data structure, making it less memory-efficient than the other solutions.	Correct! (BFS also works fine!)  Method: Our solution uses DFS to find the closed islands in a 2D grid. The DFS function traverses through the 2D grid, checking if each cell is a connected component or not. If the cell is a connected component, the DFS assigns a unique ID to the connected component. The DFS also checks if the connected component is touching the boundary, and if so, sets a boolean flag 'touches_boundary' to true. The 'onBoundary' function checks if the cell is on the boundary of the grid. The main function increments the count variable if the connected component is not touching the boundary.  Time Complexity: O(mn), where m and n are the number of rows and columns in the grid, respectively.  Memory Complexity: O(mn), as we are using an extra matrix of size m*n to store the connected component IDs.	Incorrect! This is not even close, as Floyd-Warshall is a shortest path algorithm.	Incorrect! This is used for string-based problems.	Iterate through each cell in the grid and for each 0 cell, perform a DFS to find all connected 0 cells, then check if it's a closed island. Count the number of closed islands found.  The initial brute force solution has a time complexity of O(mn(m+n)) since it performs a DFS for each 0 cell, leading to a worst-case scenario where we visit every cell twice (once for the DFS and once for the check). This is not optimal for larger grids.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Number%20of%20Closed%20Islands.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Number%20of%20Closed%20Islands.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Number%20of%20Closed%20Islands.java	https://youtu.be/D8GYZiLZKPU
Algorithms Pack	Depth-First Search	Detect Cycles in 2D Grid	https://leetcode.com/problems/detect-cycles-in-2d-grid/	Given a 2D array of characters grid of size m x n, you need to find if there exists any cycle consisting of the same value in grid.    A cycle is a path of length 4 or more in the grid that starts and ends at the same cell. From a given cell, you can move to one of the cells adjacent to it - in one of the four directions (up, down, left, or right), if it has the same value of the current cell.    Also, you cannot move to the cell that you visited in your last move. For example, the cycle (1, 1) -> (1, 2) -> (1, 1) is invalid because from (1, 2) we visited (1, 1) which was the last visited cell.    Return true if any cycle of the same value exists in grid, otherwise, return false.	O(nm)	O(nm)	We're expecting an optimized solution of time complexity O(nm) and memory complexity O(nm).	Floyd's Cycle-Finding Algorithm	Floyd-Warshall	Dynamic Programming	DFS	D	This approach may not work for all cases - the approach being where we apply Floyd's cycle-finding algorithm to find a cycle of length 4 or more in the grid. Additionally, it has a time complexity of O(m^2*n), which is less efficient than the DFS or union-find approach for most inputs.	Almost never optimal, unfortunately! The Floyd Warshall algorithm has a time complexity of O(n^3), which is not feasible for large inputs. Additionally, this approach is overkill for finding cycles of length 4 or more, as it computes the shortest path between every pair of cells in the grid.	Using DP+Memoization would be a very time-consuming exercise here. DFS is a better approach.	Correct!  We start by iterating through each cell of the grid and performing DFS from that cell if it hasn't been visited yet. During DFS, we mark visited cells and check if the current cell matches the starting cell of the cycle (if it does, then we've found a cycle). We also avoid revisiting the parent cell that was last visited. If we find a cycle, we terminate the search immediately and return true. If we traverse the entire grid and do not find any cycles, we return false.  Time Complexity: O(m*n), where m and n are the dimensions of the grid. This is because we need to traverse the entire grid, and each cell is visited at most once.  Memory Complexity: O(m*n), where m and n are the dimensions of the grid. This is because we need to keep track of visited cells using a 2D boolean array, which has the same dimensions as the grid.	In this case, the brute-force approach is a little similar to the suggested approach to offer a hint.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Detect%20Cycles%20in%202D%20Grid.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Detect%20Cycles%20in%202D%20Grid.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Detect%20Cycles%20in%202D%20Grid.java	https://youtu.be/YpbGXs5fl9s
Algorithms Pack	Depth-First Search	Restore the Array From Adjacent Pairs	https://leetcode.com/problems/restore-the-array-from-adjacent-pairs/	There is an integer array nums that consists of n unique elements, but you have forgotten it. However, you do remember every pair of adjacent elements in nums.    You are given a 2D integer array adjacentPairs of size n - 1 where each adjacentPairs[i] = [ui, vi] indicates that the elements ui and vi are adjacent in nums.    It is guaranteed that every adjacent pair of elements nums[i] and nums[i+1] will exist in adjacentPairs, either as [nums[i], nums[i+1]] or [nums[i+1], nums[i]]. The pairs can appear in any order.    Return the original array nums. If there are multiple solutions, return any of them.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Greedy	Sorting	Topological Sort	DFS	D	Not the best approach. This approach may not work if there are multiple possible solutions, as it may get stuck in a cycle or a dead end. For example, if there are two adjacent pairs (a, b) and (b, c), but also another pair (c, d), then starting with a or d may lead to a dead end. Time complexity is O(n^2).	This might work, but is not the best approach  We'd construct the graph from the given adjacent pairs and sort the nodes in increasing order, giving us the original array.  Why it's suboptimal: While this approach yields the correct answer, it is inefficient since it requires sorting the entire array of nodes. This can be time-consuming and memory-intensive, especially for larger graphs.  Time Complexity: O(n log(n)), where n is the number of nodes in the graph.	Not for this problem.  Topological sort requires a directed acyclic graph, and it's not guaranteed that the graph created from adjacentPairs is a DAG.	Correct!  We can build an undirected graph from the given adjacent pairs. Since the graph is connected, any connected component of the graph can represent a valid ordering of the original array. We can perform depth-first search (DFS) starting from any node of the graph and traverse all the connected nodes to form a connected component. The sequence of nodes visited in DFS forms a valid ordering of the original array.  Time Complexity: O(n), where n is the length of the array.  Memory Complexity: O(n), for the hash table, unordered set, and the call stack for the DFS function.	Our brute-force approach? We can generate all possible permutations of the unique elements in the array and check which permutation satisfies the adjacent pairs.  Why it's suboptimal: This approach has an exponential time complexity of O(n!), which is not practical for large values of n.  Time Complexity: O(n!), where n is the length of the array.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Restore%20the%20Array%20From%20Adjacent%20Pairs.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Restore%20the%20Array%20From%20Adjacent%20Pairs.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Restore%20the%20Array%20From%20Adjacent%20Pairs.java	https://youtu.be/ab-iNgozKxw
Algorithms Pack	Depth-First Search	Smallest String With Swaps	https://leetcode.com/problems/smallest-string-with-swaps/	You are given a string s, and an array of pairs of indices in the string pairs where pairs[i] = [a, b] indicates 2 indices(0-indexed) of the string.    You can swap the characters at any pair of indices in the given pairs any number of times.    Return the lexicographically smallest string that s can be changed to after using the swaps.	O(n log(n))	O(n)	We're expecting an optimized solution of time complexity O(n log(n)) and memory complexity O(n).	Greedy Swapping	Sort + Swap	DFS	Permutation Sorting	C	Incorrect!  This approach may lead to a wrong result because swapping letters based on the first pair that can be swapped does not necessarily lead to the lexicographically smallest string. There may be other pairs that can be swapped to obtain a smaller string. Time complexity: O(n^2) for applying swaps.	Not ideal.   Sort all pairs and apply swaps one by one.  Why it's suboptimal: This approach fails in some cases because sorting pairs does not guarantee that all pairs form a connected component. It's possible that some pairs are not connected, and applying swaps on these pairs may lead to a wrong result. Time complexity: O(n log(n)) for sorting pairs, and O(n^2) for applying swaps.	Correct, although Union Find would also be a sound approach!  Our approach is to use the concept of connected components to group all the characters that are connected by swaps. We first create an undirected graph where the nodes are the indices of the string, and the edges are the pairs of indices given. We then traverse the graph using Depth First Search (DFS) and find all the connected components. For each connected component, we sort the corresponding characters in lexicographical order and replace the corresponding characters in the original string s. Finally, we return the modified string.  Time Complexity: O(n log(n)), where n is the length of the input string s. The time complexity of DFS traversal and sorting is O(n log(n)). We traverse the graph once, so the time complexity is O(n log(n)).  Memory Complexity: O(n), where n is the length of the input string s. We create an adjacency list representation of the graph, which takes O(n) space. We also use a visited array and a vector to store the connected components, which takes O(n) space. Therefore, the overall memory complexity is O(n).	No: this is a very complex brute-force approach.	Generate all possible permutations of the string using the given pairs and return the lexicographically smallest one.  Why it's suboptimal: The time complexity is factorial, which is not feasible for large inputs.  Time Complexity: O(n!)	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Smallest%20String%20With%20Swaps.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Smallest%20String%20With%20Swaps.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Smallest%20String%20With%20Swaps.java	https://youtu.be/wWDO8JRpeWE
Algorithms Pack	Depth-First Search	Is Graph Bipartite	https://leetcode.com/problems/is-graph-bipartite/	There is an undirected graph with n nodes, where each node is numbered between 0 and n - 1. You are given a 2D array graph, where graph[u] is an array of nodes that node u is adjacent to. More formally, for each v in graph[u], there is an undirected edge between node u and node v. The graph has the following properties:    There are no self-edges (graph[u] does not contain u).  There are no parallel edges (graph[u] does not contain duplicate values).  If v is in graph[u], then u is in graph[v] (the graph is undirected).  The graph may not be connected, meaning there may be two nodes u and v such that there is no path between them.  A graph is bipartite if the nodes can be partitioned into two independent sets A and B such that every edge in the graph connects a node in set A and a node in set B.    Return true if and only if it is bipartite.	O(n+m)	O(n)	We're expecting an optimized solution of time complexity O(n+m) and memory complexity O(n).	Union-Find	Coloring With Flood Fill Algorithm	Kahn's Algorithm	DFS	D	Incorrect!  Method: We can use the union-find algorithm to group nodes into two sets. For each edge, we check if the nodes are already in the same set. If they are, the graph is not bipartite. If they are not, we add them to different sets. If we run out of edges to check and the graph has not been determined to be non-bipartite, we assume it is bipartite.  Weakness: DFS has better complexity. This has time complexity O(nlogn).	Not the best approach!	This is used for topological sorting, and isn't really a viable approach here.	Correct! Our approach is based on performing a Depth-First Search (DFS) traversal of the given graph, marking nodes as either '1' or '2' depending on the color of their adjacent nodes, and checking for conflicts.  We start by initializing a vector of colors with all nodes colored as '0', meaning they haven't been visited yet. Then, we iterate through each node of the graph, and for each unvisited node, we start a DFS traversal, marking the node as either '1' or '2' based on the color of its parent node. We pass the opposite color to the child nodes, i.e., if the parent node is colored as '1', we assign '2' to its child nodes and vice versa. If a conflict is detected while coloring the nodes, we mark the colorConflict variable as true and immediately return false. Otherwise, if all nodes are colored without any conflicts, we return true.  Time Complexity: O(n + m), where n is the number of nodes in the graph, and m is the number of edges.  Space Complexity: O(n), which is the space used by the colors vector.	Approach: Try all possible ways to partition the nodes into two sets, A and B, and check if all edges in the graph connect nodes from different sets. Time complexity: O(2^n * n), where n is the number of nodes in the graph.  Note: This approach is very inefficient!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Is%20Graph%20Bipartite.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Is%20Graph%20Bipartite.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Is%20Graph%20Bipartite.java	https://youtu.be/n93ifQRBD5o
Algorithms Pack	Depth-First Search	Reorder Routes To Make All Paths Lead To The City Zero	https://leetcode.com/problems/reorder-routes-to-make-all-paths-lead-to-the-city-zero/	There are n cities numbered from 0 to n - 1 and n - 1 roads such that there is only one way to travel between two different cities (this network form a tree). Last year, The ministry of transport decided to orient the roads in one direction because they are too narrow.    Roads are represented by connections where connections[i] = [ai, bi] represents a road from city ai to city bi.    This year, there will be a big event in the capital (city 0), and many people want to travel to this city.    Your task consists of reorienting some roads such that each city can visit the city 0. Return the minimum number of edges changed.    It's guaranteed that each city can reach city 0 after reorder.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	DFS	Kruskal's Algorithm	Greedy	Topological Sort	A	Correct!  In this approach, we first create an undirected graph by adding edges in both directions. Then we perform a DFS (Depth-First Search) starting from node 0, and keep track of whether we need to reorder an edge in order to visit the node 0.    We start with node 0, mark it as visited, and for each of its neighbors, we mark the edge as 'built' (since we can travel from 0 to that neighbor directly), and then recursively explore that neighbor's neighbors. We add the number of edges that need to be reordered to a counter variable.  After we have traversed the entire tree, we will have counted all the edges that need to be reordered in order to visit the node 0 from all the nodes. We then return the reorder count.  Time Complexity: O(n), where n is the number of nodes in the graph. This is because we perform a DFS on each node at most once.  Memory Complexity: O(n), where n is the number of nodes in the graph. This is because we store the visited nodes, and the graph is represented as an adjacency list.	Not quite!  Kruskal's Algorithm is not guaranteed to produce the minimum spanning tree of the original tree, which means that the solution may not be optimal. Additionally, the algorithm may require a large number of changes to be made to the orientation of the edges, leading to a higher reorder count.  Approach: Build a new tree rooted at the capital city by greedily selecting edges in ascending order of their weights. Keep track of the orientation of each edge and the number of changes required to make every city reachable from the capital city.	Greed is not good, in this instance.  This approach does not consider the global optimal solution, and may get stuck in a local optimal solution. There can be cases where choosing a different edge at one node could lead to fewer edge reversals overall.  Time Complexity: O(n^2)	Incorrect approach.  This approach assumes that the graph is a Directed Acyclic Graph (DAG) and does not work for general trees. Additionally, the algorithm requires computing the indegree of each node, which takes O(n^2) time for a tree.	The brute-force here is to generate all possible combinations of the orientations of the edges, and count the number of changes required to make every city reachable from the capital city.  Why it's suboptimal: This approach has an extremely high time complexity, as the number of possible edge orientations grows exponentially with the number of edges. It is not feasible for large inputs.  Time Complexity: O(2^n * n^2), where n is the number of edges.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Reorder%20Routes%20to%20Make%20All%20Paths%20Lead%20to%20the%20City%20Zero.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Reorder%20Routes%20to%20Make%20All%20Paths%20Lead%20to%20the%20City%20Zero.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Reorder%20Routes%20to%20Make%20All%20Paths%20Lead%20to%20the%20City%20Zero.java	https://youtu.be/Ls5g9Youotc
Algorithms Pack	Depth-First Search	Path With Minimum Effort	https://leetcode.com/problems/path-with-minimum-effort/	You are a hiker preparing for an upcoming hike. You are given heights, a 2D array of size rows x columns, where heights[row][col] represents the height of cell (row, col). You are situated in the top-left cell, (0, 0), and you hope to travel to the bottom-right cell, (rows-1, columns-1) (i.e., 0-indexed). You can move up, down, left, or right, and you wish to find a route that requires the minimum effort.    A route's effort is the maximum absolute difference in heights between two consecutive cells of the route.    Return the minimum effort required to travel from the top-left cell to the bottom-right cell.	O(mn log K)	O(mn)	We're expecting an optimized solution of time complexity O(mn log K) and memory complexity O(mn).	Greedy	DFS + Binary Search	Dijkstra's Algorithm	Floyd-Warshall	B	Not the best approach. This approach doesn't guarantee that we will reach the bottom-right cell with the minimum effort. We might get stuck in a local minimum and miss a better path. Also, it's possible to construct a counter-example where this algorithm fails.	We use a binary search approach to search for the minimum effort required to travel from the top-left cell to the bottom-right cell. Initially, we set the lower bound to 0 and the upper bound to 1e9 (the maximum height difference). We then repeatedly perform a binary search on the bounds until we find the smallest value for which a path from (0,0) to (rows-1,columns-1) with no steps greater than mid exists.  In the possible() function, we use depth-first search (DFS) to search for a path from (0,0) to (rows-1,columns-1) with no steps greater than diff. We mark visited cells using a 2D boolean array to ensure we do not visit cells twice. If we find a path that reaches the destination cell, we return true, indicating that there is a path with no steps greater than diff. Otherwise, we return false.  Time Complexity: O(mn log K), where m and n are the number of rows and columns in the heights matrix, respectively, and K is the range of the heights. We perform a binary search on K, which takes O(log K) time. For each search, we perform DFS, which takes O(mn) time in the worst case.  Memory Complexity: O(mn), where m and n are the number of rows and columns in the heights matrix, respectively. We use a 2D boolean array to mark visited cells.	Incorrect! This approach doesn't guarantee that we will find the minimum effort required to travel from the top-left cell to the bottom-right cell. We might miss a better path. Also, the algorithm is not suitable for this problem because the edge weights are not non-negative.	With a minimum time complexity of O(n^3), it's clear there are more optimal approaches than Floyd-Warshall to solve this problem.	The brute-force approach would be to generate all possible paths from the top-left cell to the bottom-right cell and compute their maximum absolute difference. Return the minimum of all such maximum absolute differences.  Why it's suboptimal: This approach is very inefficient since it generates all possible paths, which could be exponential in the worst case. The time complexity is O(2^(m+n)), where m and n are the dimensions of the heights matrix.  Time Complexity: O(2^(m+n)).	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Path%20With%20Minimum%20Effort.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Path%20With%20Minimum%20Effort.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Path%20With%20Minimum%20Effort.java	https://youtu.be/f9CRNdrA0Tc
Algorithms Pack	Breadth-First Search	Graph Valid Tree	https://leetcode.com/problems/graph-valid-tree/	You have a graph of n nodes labeled from 0 to n - 1. You are given an integer n and a list of edges where edges[i] = [ai, bi] indicates that there is an undirected edge between nodes ai and bi in the graph.    Return true if the edges of the given graph make up a valid tree, and false otherwise.	O(n+m)	O(n+m)	We're expecting an optimized solution of time complexity O(n+m) and memory complexity O(n+m).	Floyd-Warshall	Topological Sort	Dynamic Programming	BFS	D	Suboptimal.  The Floyd-Warshall Algorithm is typically used to find the shortest path between all pairs of vertices in a weighted graph, but may be overkill for detecting cycles in an undirected graph since it has a time complexity of O(V^3) where V is the number of vertices.	A risky strategy, for a non-DAG not ideal here.   Topological Sort is primarily used for directed acyclic graphs (the aforementioned DAG) and may not work for undirected graphs.	An unlucky guess?  There are many more viable approaches than DP.	Correct! DFS also works well, but BFS is a little more intuitive to code.  The idea is to do a Breadth-First Search (BFS) traversal of the graph and check for the presence of a cycle. For each node, we perform a BFS traversal starting from that node. During the traversal, we maintain two vectors: visited and parent. visited vector stores the level at which a node is visited, and parent vector stores the parent of the node. If a node is visited more than once, then there is a cycle in the graph.  We initialize the visited vector with a very large value OO to indicate that a node is not yet visited. During BFS, if we visit a node that is already visited, then we check whether the parent of the node is the same as the current node's parent. If they are the same, then we skip that edge, as it is the one connecting the current node to its parent. Otherwise, we have found a cycle in the graph.  If there is no cycle in the graph, we check whether the number of edges is n-1, where n is the number of nodes in the graph. If this condition is not satisfied, then the graph is disconnected and cannot be a tree.  Time Complexity: O(V+E), where V is the number of nodes and E is the number of edges in the graph.  Memory Complexity: O(V+E), where V is the number of nodes and E is the number of edges in the graph.	Generate all possible permutations of edges and check if the resulting graph is a valid tree.  Why it's suboptimal: The number of possible permutations of edges grows factorially with the number of edges, making this approach highly inefficient for large graphs.  Time Complexity: O(n!)	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Graph%20Valid%20Tree.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Graph%20Valid%20Tree.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Graph%20Valid%20Tree.java	https://youtu.be/ffvp3Mw6LG8
Algorithms Pack	Breadth-First Search	Shortest Path To Get Food	https://leetcode.com/problems/shortest-path-to-get-food/	You are starving and you want to eat food as quickly as possible. You want to find the shortest path to arrive at any food cell.    You are given an m x n character matrix, grid, of these different types of cells:    '*' is your location. There is exactly one '*' cell.  '#' is a food cell. There may be multiple food cells.  'O' is free space, and you can travel through these cells.  'X' is an obstacle, and you cannot travel through these cells.  You can travel to any adjacent cell north, east, south, or west of your current location if there is not an obstacle.    Return the length of the shortest path for you to reach any food cell. If there is no path for you to reach food, return -1.	O(nm)	O(nm)	We're expecting an optimized solution of time complexity O(nm) and memory complexity O(nm).	BFS	Dijkstra's Algorithm	Recursive Backtracking	Greedy	A	Correct!  Our solution uses Breadth-First Search (BFS) to find the shortest path from the starting cell to any food cell in the given grid.  We first find the starting cell, marked with '*'. We then use BFS to explore all possible cells reachable from the starting cell, which are not blocked by an obstacle ('X') or already visited. We use a queue to store the cells to visit next. We visit each cell in the queue and check if it is a food cell ('#'), if it is, we return the level at which the food cell was found. Otherwise, we add all reachable, unvisited cells marked as free space ('O') to the queue, and mark them as visited.  If all cells have been visited and no food cells have been found, we return -1 to indicate that there is no path to any food cell.	Suboptimal. While it would work, this approach involves calculating the shortest path to all cells in the grid, which is not necessary for this problem. This makes the time complexity of the algorithm much higher than the BFS approach.	This is the brute-force approach, and isn't as good as BFS for this problem.	The greedy algorithm may not always find the shortest path since it does not consider the overall path length, but only the next step. It may also get stuck in local minima and miss shorter paths to other food cells. It could be done by choosing the nearest food cell at each step and follow the shortest path to that food cell, and repeat until we reach a food cell. However, the time complexity is still suboptimal.	The brute-force approach is to generate all possible paths from the starting cell to each food cell and select the shortest path. This can be done using depth-first search (DFS) or recursive backtracking. However, the time complexity of this approach is very high, as it involves exploring all possible paths in the grid, which can take exponential time.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Shortest%20Path%20to%20Get%20Food.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Shortest%20Path%20to%20Get%20Food.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Shortest%20Path%20to%20Get%20Food.java	https://youtu.be/ajddnybnPug
Algorithms Pack	Breadth-First Search	Jump Game III	https://leetcode.com/problems/jump-game-iii/	Given an array of non-negative integers arr, you are initially positioned at start index of the array. When you are at index i, you can jump to i + arr[i] or i - arr[i], check if you can reach to any index with value 0.    Notice that you can not jump outside of the array at any time.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Dynamic Programming	Greedy	BFS	Binary Search	C	Not so efficient.  Dynamic programming can be inefficient if the jumps have large values or if the array is very large, because it requires computing and storing many intermediate results. In some cases, it may also not be possible to find a solution using dynamic programming.  Time Complexity: O(n^2) or O(n) (depending on the implementation)  We can use dynamic programming to keep track of the minimum number of jumps required to reach each index with value 0, starting from the left or right end of the array. This approach can be implemented using a memoization table or a bottom-up approach.	Incorrect!  The greedy algorithm is not guaranteed to find an optimal solution, because it may get stuck in a local optimum or a loop that prevents reaching an index with value 0. In some cases, it may also not be possible to find a solution using the greedy algorithm.	Our approach starts at the starting vertex and explores all the neighboring vertices before moving to the next level. In this approach, we use a queue to maintain the order of nodes to be visited. We start from the start index and push it into the queue. We then process the nodes in the queue level by level until we reach an index with value 0 or we have traversed all nodes.  Time Complexity: O(n), where n is the length of the input array. In the worst case, we will visit every node in the array.  Memory Complexity: O(n), where n is the length of the input array. We use a boolean array to keep track of the visited nodes, which requires O(n) space. We also use a queue to store the nodes to be visited, which can hold up to n nodes.	Incorrect!   Binary search assumes that the array is sorted, which is not the case here.	As usual, brute-force would be highly inefficient. We can try all possible jumps from the current index, and repeat this process for each new index we reach until we find an index with value 0. This approach can be implemented recursively or using a loop.  Why it's suboptimal: The brute force approach is very inefficient because it explores all possible paths, even those that are clearly not optimal. This can lead to an exponential time complexity in the worst case, and it's not practical for large arrays.  Time Complexity: O(2^n) or O(n!) (exponential or factorial, depending on the implementation).	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Jump%20Game%20III.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Jump%20Game%20III.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Jump%20Game%20III.java	https://youtu.be/uGnlCuUzSJU
Algorithms Pack	Breadth-First Search	Minimum Operations To Convert Number	https://leetcode.com/problems/minimum-operations-to-convert-number/	You are given a 0-indexed integer array nums containing distinct numbers, an integer start, and an integer goal. There is an integer x that is initially set to start, and you want to perform operations on x such that it is converted to goal. You can perform the following operation repeatedly on the number x:    If 0 <= x <= 1000, then for any index i in the array (0 <= i < nums.length), you can set x to any of the following:    x + nums[i]  x - nums[i]  x ^ nums[i] (bitwise-XOR)  Note that you can use each nums[i] any number of times in any order. Operations that set x to be out of the range 0 <= x <= 1000 are valid, but no more operations can be done afterward.    Return the minimum number of operations needed to convert x = start into goal, and -1 if it is not possible.	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	BFS	Dijkstra's Algorithm	Greedy	Dynamic Programming	A	Correct!  We start by initializing a queue with the starting value and marking it as visited. Then, we perform BFS on this queue by processing its front element and adding its neighbors to the queue.  For each element in the queue, we iterate through the given array of distinct numbers and check if we can reach the goal by adding, subtracting, or XORing it with the current element. If we can reach the goal using any of these operations, we return the current level plus one, which represents the minimum number of operations needed to reach the goal.  If we can't reach the goal using any of the operations, we mark the current element as visited and add its neighbors (obtained by performing any of the operations) to the queue.  We continue this process until we find the goal or the queue becomes empty. If we can't find the goal, we return -1.  Time Complexity: O(n * (1000^2)), where n is the length of the given array. This is because we can perform any operation on x that results in a value between 0 and 1000, and we iterate through the array n times.  Memory Complexity: O(1).	Not optimal at all, since the edges in any graph we generate are not weighted uniformly. Some edges may require more or fewer operations than others, depending on the value of the number being operated on. Therefore, using Dijkstra's algorithm will not guarantee that we find the minimum number of operations required.	Not the best approach here.   This approach is suboptimal because it does not consider all possible combinations of operations that can be performed on x. It may be the case that using smaller elements first leads to a shorter sequence of operations.	Not the best approach due to the space complexity! The idea is okay, but can be improved upon significantly.  The idea here? Create a 2D array DP where DP[i][j] represents the minimum number of operations needed to convert i to j. We can fill the array using a recursive formula that considers all possible operations on i.	The brute force here? Well, we can iterate over all possible sequences of operations that can be performed on x until we reach the goal. However, the number of possible sequences of operations is very large, and we would have to check all of them to guarantee that we find the minimum number of operations required. This approach is very inefficient and not practical for larger inputs.  Time Complexity: O(3^n), where n is the number of elements in the array.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Minimum%20Operations%20to%20Convert%20Number.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Minimum%20Operations%20to%20Convert%20Number.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Minimum%20Operations%20to%20Convert%20Number.java	https://youtu.be/2rwXiFl-Ac0
Algorithms Pack	Breadth-First Search	Open The Lock	https://leetcode.com/problems/open-the-lock/	You have a lock in front of you with 4 circular wheels. Each wheel has 10 slots: '0', '1', '2', '3', '4', '5', '6', '7', '8', '9'. The wheels can rotate freely and wrap around: for example we can turn '9' to be '0', or '0' to be '9'. Each move consists of turning one wheel one slot.    The lock initially starts at '0000', a string representing the state of the 4 wheels.    You are given a list of deadends dead ends, meaning if the lock displays any of these codes, the wheels of the lock will stop turning and you will be unable to open it.    Given a target representing the value of the wheels that will unlock the lock, return the minimum total number of turns required to open the lock, or -1 if it is impossible.	O(n^2 ∗A^n + d)	O(a^n+d)	We're expecting an optimized solution of time complexity O(n^2 ∗A^n + d) and memory complexity O(a^n+d). (Explained in full in the correct answer!).	Greedy	BFS	Simulated Annealing	Bucket Sort	B	Not optimal.  Simply, it doesn't consider the fact that turning one wheel may affect the other wheels. For example, turning the first wheel to '9' and the second wheel to '1' may not be the best approach, since it might be better to turn the second wheel to '9' instead. This approach may result in the incorrect minimum number of turns required to reach the target combination.	Correct!  We use a BFS algorithm to explore all possible combinations of the lock. We start with the initial combination '0000' and explore all its neighbors, which are obtained by changing one digit at a time. For each neighbor, we check if it has been visited before or if it is a deadend. If it hasn't been visited and it is not a deadend, we add it to the queue and mark it as visited. We repeat this process until we reach the target combination or until we have explored all possible combinations.  Why it's a good approach: BFS guarantees that we find the shortest path to the target. In this problem, the target may not be directly reachable from the initial combination, so we need to explore all possible combinations to find the shortest path. BFS allows us to do this efficiently by exploring all neighbors of the current combination before moving on to the next level.  Time Complexity: O(n^2a^n + d), where n is the number of wheels (4 in this case), a is the number of digits per wheel (10 in this case), and d is the number of deadends. The worst case is when we explore all possible combinations of the lock, which is a^n. For each combination, we need to check if it is a deadend, which takes O(d) time. Therefore, the overall time complexity is O(n^2a^n + d).  Memory Complexity: O(a^n + d), where a is the number of digits per wheel (10 in this case), n is the number of wheels (4 in this case), and d is the number of deadends. We need to store all possible combinations of the lock, which is a^n. We also need to store the set of deadends, which takes O(d) space. Therefore, the overall memory complexity is O(a^n + d).	No.  Simulated annealing may not always find the optimal solution since it relies on randomness and may get stuck in a local minimum.  Time Complexity: O(k*n), where k is the number of iterations and n is the number of digits in the combination.	If you chose this option without it being a mouse-slip or finger-slip, I suggest finding an actual, physical bucket, then sitting down on it facing the corner of the room, donning a dunce cap. On the other hand, if you know how to make this work efficiently, please let us know!	Brute-force approach: turn the lock by hand….well, basically! Try every possible combination of rotations for the wheels of the lock until we find the target code or exhaust all possible combinations.  Why it's suboptimal: The number of possible combinations is 10^4, which is prohibitively large to search exhaustively.  Time Complexity: O(10^4)	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Open%20the%20Lock.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Open%20the%20Lock.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Open%20the%20Lock.java	https://youtu.be/A5B46qkZzd4
Algorithms Pack	Breadth-First Search	Walls And Gates	https://leetcode.com/problems/walls-and-gates/	You are given an m x n grid rooms initialized with these three possible values.    -1 A wall or an obstacle.  0 A gate.  INF Infinity means an empty room. We use the value 231 - 1 = 2147483647 to represent INF as you may assume that the distance to a gate is less than 2147483647.  Fill each empty room with the distance to its nearest gate. If it is impossible to reach a gate, it should be filled with INF.	O(nm)	O(nm)	We're expecting an optimized solution of time complexity O(nm) and memory complexity O(nm).	Dijkstra's Algorithm	Floyd-Warshall	BFS	A* Search	C	No. Dijkstra's algorithm is not suitable for this problem because it is designed for finding the shortest path between two specific nodes, not for computing the distance of all nodes from a set of source nodes.	This algorithm has a time complexity of O(m^3), which is not optimal for large grids and may also cause overflow errors due to the use of infinity.	Correct!  In our approach, which FLIPS the brute-force approach suggested, we use BFS to traverse the grid from all the gate cells (cells with value 0) in the grid. We start by pushing all the gate cells into a queue and mark them as visited. For each level, we check all the cells that are reachable from the cells at the current level (the cells that are adjacent to the cells in the current level) and add them to the queue, marking them as visited. We also fill the distance to the nearest gate for each of the cells visited.  The algorithm takes advantage of the fact that BFS guarantees that we will visit all the cells in the grid in order of their distance from the starting cells. So, we can be sure that by the time we visit a cell, we have found the shortest distance to that cell from one of the starting cells.  Time Complexity: O(mn), since we traverse each cell in the grid only once in our BFS algorithm.  Memory Complexity: O(mn), as we use two extra vectors of size m x n to keep track of the visited cells and starting cells in the grid.	Suboptimal compared to a BFS approach. While A* search is an improvement over Dijkstra's algorithm, it still involves calculating the shortest path from each gate to every other point in the grid, which can result in redundant calculations and slower performance for larger grids.	We can consider every empty cell in the grid as a starting point, and then use Breadth-First Search (BFS) to traverse the entire grid and compute the distance of each empty cell from the nearest gate.  Why it's suboptimal: This approach has a time complexity of O((mn)^2), which is very inefficient and impractical for large grids.  Time Complexity: O((mn)^2)	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Walls%20and%20Gates.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Walls%20and%20Gates.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Walls%20and%20Gates.java	https://youtu.be/8zvjsNRQtnM
Algorithms Pack	Breadth-First Search	Pacific Atlantic Water Flow	https://leetcode.com/problems/pacific-atlantic-water-flow	There is an m x n rectangular island that borders both the Pacific Ocean and Atlantic Ocean. The Pacific Ocean touches the island's left and top edges, and the Atlantic Ocean touches the island's right and bottom edges.    The island is partitioned into a grid of square cells. You are given an m x n integer matrix heights where heights[r][c] represents the height above sea level of the cell at coordinate (r, c).    The island receives a lot of rain, and the rain water can flow to neighboring cells directly north, south, east, and west if the neighboring cell's height is less than or equal to the current cell's height. Water can flow from any cell adjacent to an ocean into the ocean.    Return a 2D list of grid coordinates result where result[i] = [ri, ci] denotes that rain water can flow from cell (ri, ci) to both the Pacific and Atlantic oceans.	O(nm)	O(nm)	We're expecting an optimized solution of time complexity O(nm) and memory complexity O(nm).	BFS	Greedy	Flood-Fill Algorithm	Dijkstra's Algorithm	A	Correct!  We perform two BFS searches, one from the Pacific Ocean border and the other from the Atlantic Ocean border. We mark all visited cells reachable from each border, respectively, in two separate visited arrays. Finally, we iterate through each cell and check whether it was visited by both BFSes, which means that it is reachable by both oceans.  Time Complexity: O(mn), where m is the number of rows and n is the number of columns in the input matrix. This is because we perform two separate BFS searches, which takes O(mn) time each, and iterate through the matrix once, which also takes O(m*n) time.  Memory Complexity: O(mn), where m is the number of rows and n is the number of columns in the input matrix. We use two separate visited arrays of size mn each to mark the cells visited by both BFSes. We also use a queue of cells for each BFS search, which can contain up to mn cells. Therefore, the total memory used is O(mn) + O(mn) + O(mn) = O(m*n).	Incorrect!  This approach does not guarantee that we will find all the cells that can flow to both oceans, as some lower cells may not be reachable from the highest cell. It also does not guarantee that we will find the optimal paths to the oceans.	An ironic name, given the problem, but an approach that might scuttle you in an interview.  We can flood fill the grid from every cell that is adjacent to the Pacific and Atlantic ocean to mark all the reachable cells. Then we can iterate through the grid and check if a cell is reachable from both oceans.  This approach can be slow and memory-intensive, especially for large grids, as it requires multiple flood fill iterations.  Time Complexity: O(mn(m+n)).	Not recommended for this at all.  This approach has a high time complexity of O(m^2n^2 log(m^2n^2)) due to the use of Dijkstra's algorithm, which is not necessary to solve this problem. There are faster and simpler algorithms that can solve this problem in linear time.  The approach would be to treat the matrix as a weighted graph, where each cell is a node and there is an edge between two nodes if rain water can flow from one cell to another. Then, run Dijkstra's algorithm from each ocean to find the minimum distance to each cell. If the sum of the distances from both oceans to a cell is equal to the height of the cell, mark the cell as reachable from both oceans.	The brute-force approach would be wildly suboptimal here!  We can start from every cell and do a recursive depth-first search to explore all the reachable cells that have a lower or equal height. We can keep track of whether a cell is reachable from the Pacific and Atlantic ocean by keeping track of which ocean is the starting point of the current search.  Why it's suboptimal: This approach has an exponential time complexity in the worst-case scenario, which makes it impractical for large grids.  Time Complexity: O(4^(m*n))	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Pacific%20Atlantic%20Water%20Flow.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Pacific%20Atlantic%20Water%20Flow.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Pacific%20Atlantic%20Water%20Flow.java	https://youtu.be/mIPKgbne5gw
Algorithms Pack	Breadth-First Search	Stepping Numbers	https://leetcode.com/problems/stepping-numbers/	A stepping number is an integer such that all of its adjacent digits have an absolute difference of exactly 1.    For example, 321 is a stepping number while 421 is not.  Given two integers low and high, return a sorted list of all the stepping numbers in the inclusive range [low, high].	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	BFS	Recursive	Dynamic Programming	Backtracking	A	Correct!  In this approach, we use Breadth-First Search to explore all the possible stepping numbers within the given range [low, high]. We start by creating a queue and pushing all single-digit numbers (i.e., 0-9) within the given range into the queue. Then, we take each number from the queue and check if its last digit is not zero, in which case we subtract 1 from the last digit and enqueue the new number if it falls within the given range. Similarly, if the last digit is not 9, we add 1 to the last digit and enqueue the new number if it falls within the given range. We continue this process until the queue becomes empty.  We maintain a visited array to keep track of all the stepping numbers we have already visited. Finally, we return the visited array in sorted order.	This approach suffers from the same problem as the brute-force approach, generating and checking a large number of unnecessary numbers that are not stepping numbers, resulting in poor time complexity and inefficiency. In addition, it uses recursion, which can cause a stack overflow for large ranges.	This approach is suboptimal because it still generates and checks a large number of unnecessary numbers that are not stepping numbers, resulting in poor time complexity and inefficiency.	For a problem that can be solved in O(n) using BFS, backtracking would be considered overkill here.	The brute-force here is intuitive less than ideal. Generate all the numbers in the range [low, high] and check if each number is a stepping number by comparing adjacent digits.  Why it's suboptimal: This approach generates and checks a large number of unnecessary numbers that are not stepping numbers, resulting in poor time complexity and inefficiency.  Time Complexity: O(n*l), where n is the range and l is the number of digits in the largest number in the range.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Stepping%20Numbers.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Stepping%20Numbers.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Stepping%20Numbers.java	https://youtu.be/acLsd0DSsQQ
Algorithms Pack	Breadth-First Search	Shortest Path With Alternating Colors	https://leetcode.com/problems/shortest-path-with-alternating-colors/	You are given an integer n, the number of nodes in a directed graph where the nodes are labeled from 0 to n - 1. Each edge is red or blue in this graph, and there could be self-edges and parallel edges.    You are given two arrays redEdges and blueEdges where:    redEdges[i] = [ai, bi] indicates that there is a directed red edge from node ai to node bi in the graph, and  blueEdges[j] = [uj, vj] indicates that there is a directed blue edge from node uj to node vj in the graph.  Return an array answer of length n, where each answer[x] is the length of the shortest path from node 0 to node x such that the edge colors alternate along the path, or -1 if such a path does not exist.	O(n+m)	O(n+m)	We're expecting an optimized solution of time complexity O(n+m) and memory complexity O(n+m).	BFS	Dijkstra	Floyd-Warshall	Niji's Algorithm	A	Correct!  The approach we use in our example is a variant of Breadth-First-Search (BFS) called bi-color BFS.  We start at node 0, add two nodes to a queue, with the first node having a red edge and the second having a blue edge. We perform a level-order traversal on the graph, marking each node visited with its distance from the source node (0) with the condition that the edge color alternates from the previous node. We use a two-dimensional visited array to keep track of the distance of each node for each color. If a node has already been visited for a different color than the current node, we do not consider it in this iteration.  Time Complexity: O(E + V), where E is the number of edges and V is the number of vertices. We visit each vertex and edge once.  Memory Complexity: O(V + E), where E is the number of edges and V is the number of vertices. We use an adjacency list to store the graph and two-dimensional arrays to keep track of the distances.	Not quite.  We can probably modify Dijkstra's algorithm to keep track of the color of the last edge traversed and only consider edges of the opposite color in the priority queue.  However, Dijkstra's algorithm is designed to find the shortest path in a weighted graph - in this case, the graph is not necessarily weighted. Additionally, this approach does not guarantee finding the shortest path between nodes.	Sub-optimal, as is often the case with this selection!  The time complexity of the Floyd-Warshall algorithm is O(V^3), which can be very inefficient for large graphs. Additionally, this approach may not always find the shortest path between nodes if there are cycles in the graph.  Time Complexity: O(V^3), where V is the number of vertices in the graph.	Color me disappointed! You've picked one of the few entirely fictional algorithms. 'Niji' is Japanese for rainbow.	The brute-force here: generate all possible paths from node 0 to each node in the graph, keeping track of the shortest path that alternates between red and blue edges.  Why it's suboptimal: The number of possible paths in the graph can be very large, making this approach very inefficient even for relatively small graphs. This approach has an exponential time complexity.  Time Complexity: O(2^E), where E is the number of edges in the graph.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Shortest%20Path%20with%20Alternating%20Colors.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Shortest%20Path%20with%20Alternating%20Colors.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Shortest%20Path%20with%20Alternating%20Colors.java	https://youtu.be/UPSB5SLaaoM
Algorithms Pack	Breadth-First Search	Water And Jug Problem	https://leetcode.com/problems/water-and-jug-problem/	You are given two jugs with capacities jug1Capacity and jug2Capacity liters. There is an infinite amount of water supply available. Determine whether it is possible to measure exactly targetCapacity liters using these two jugs.    If targetCapacity liters of water are measurable, you must have targetCapacity liters of water contained within one or both buckets by the end.    Operations allowed:    Fill any of the jugs with water.  Empty any of the jugs.  Pour water from one jug into another till the other jug is completely full, or the first jug itself is empty.	O(nm)	O(nm)	We're expecting an optimized solution of time complexity O(nm) and memory complexity O(nm). ***NOTE: this problem has an O(1) maths solution (Euclidean) based on coprimes/gcd. It is rather complicated, and is unlikely to be expected of you at interviews***	Backtracking + Hash Set	Dijkstra's Algorithm	BFS	Binary Search	C	Not efficient at all, even compared to the other approaches! This approach performs a backtracking search of all possible combinations of jug 1 and jug 2, and stores visited combinations in a hash set to avoid revisiting them. This approach is much less efficient than the other approaches and only works for small input sizes.	Incorrect!  This algorithm is designed for finding the shortest path in a weighted graph. Although the jugs problem can be represented as a graph, it is not a weighted graph, and therefore Dijkstra's algorithm would not be effective in solving the problem.	Correct! Controversially, you might impress with a BFS approach here!  In this approach, we use BFS to simulate the process of filling, emptying, and pouring the jugs. We keep track of the state of the jugs (represented as a pair of integers) and maintain a queue of states that we need to explore. We start with an empty state and consider all the possible states that can be reached from the current state by filling, emptying, or pouring the jugs. We add these states to the queue and continue until we find a state where one of the jugs has the target capacity or we have explored all possible states.  Time Complexity: O(nm)  The number of possible states that we need to explore is equal to the product of the capacities of the jugs, which is O(CAP1 * CAP2).  Memory Complexity: O(nm).	Simply, not a good approach here!	This is one of the rare cases where our answer actually matches the brute-force approach. We liked the movie Die Hard With A Vengeance, hence this problem remains in this deck!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Water%20and%20Jug%20Problem.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Water%20and%20Jug%20Problem.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Water%20and%20Jug%20Problem.java	https://youtu.be/em0-DccIGJc
Algorithms Pack	Breadth-First Search	Sliding Puzzle	https://leetcode.com/problems/sliding-puzzle/	On an 2 x 3 board, there are five tiles labeled from 1 to 5, and an empty square represented by 0. A move consists of choosing 0 and a 4-directionally adjacent number and swapping it.    The state of the board is solved if and only if the board is [[1,2,3],[4,5,0]].    Given the puzzle board board, return the least number of moves required so that the state of the board is solved. If it is impossible for the state of the board to be solved, return -1.	O(box * key)	O(box)	We're expecting an optimized solution of time complexity O(box * key) and memory complexity O(box).	Greedy	BFS	A* Algorithm	Dijkstra's Algorithm	B	This approach may find a path to the goal state, but it's not guaranteed to be the shortest path. In some cases, it may even get stuck in a loop or fail to find a solution at all.	Correct!  In this approach, we first convert the 2D board into a single string by concatenating each element of the board in row-major order. Then, we define a vector of vectors to hold the indices of neighboring nodes for each position in the string. We maintain a queue to keep track of the current state and a hash set to keep track of visited states. We start by adding the initial state to the queue and mark it as visited in the hash set.  We then iterate over the queue, popping off the first state and checking its neighbors by finding the index of the '0' tile in the string and swapping it with each of its neighboring tiles. If the resulting string matches the target string, we have found a solution, and we return the level of BFS (number of moves required) to reach that solution. If not, we add the resulting string to the queue and mark it as visited in the hash set.  If we have exhausted all possible moves and still have not reached the solution state, we return -1 to indicate that it is impossible to reach the solution state.  Time Complexity: The time complexity of BFS is O(b^d), where b is the branching factor, and d is the depth of the shallowest solution. In this case, the branching factor is at most 4 (4 directions to move from any given state), and the depth is at most 9!/(2! * 3!)=181,440 (number of possible states). Therefore, the time complexity is O(4^9) = O(262,144).  Memory Complexity: The memory complexity is O(b^d) because we need to store all the states that we have visited. In this case, the maximum number of states is 181,440, so the memory complexity is O(181,440).	A* requires an admissible heuristic function, which is not easy to define for this problem. Moreover, the branching factor of the search tree is quite large, so A* may not be efficient enough for this particular problem.	Simply…no.	The brute-force here involves checking all possible permutations of the board until we find the correct configuration. This would involve generating all possible permutations and checking each one of them against the goal state. Since there are 6 tiles on the board, the number of possible permutations is 6!, or 720. Thus, the time complexity of this approach would be O(720). However, this approach is suboptimal and impractical since it would take too much time for the larger board.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Sliding%20Puzzle.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Sliding%20Puzzle.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Sliding%20Puzzle.java	https://youtu.be/_bMgYIiySw8
Algorithms Pack	Breadth-First Search	Maximum Candies From Boxes	https://leetcode.com/problems/maximum-candies-you-can-get-from-boxes	You have n boxes labeled from 0 to n - 1. You are given four arrays: status, candies, keys, and containedBoxes where:    status[i] is 1 if the ith box is open and 0 if the ith box is closed,  candies[i] is the number of candies in the ith box,  keys[i] is a list of the labels of the boxes you can open after opening the ith box.  containedBoxes[i] is a list of the boxes you found inside the ith box.  You are given an integer array initialBoxes that contains the labels of the boxes you initially have. You can take all the candies in any open box and you can use the keys in it to open new boxes and you also can use the boxes you find in it.    Return the maximum number of candies you can get following the rules above.	O(n+m)	O(n)	We're expecting an optimized solution of time complexity O(n+m) and memory complexity O(n).	Greedy	Randomized Search	BFS	Rabin-Karp Algorithm	C	Doesn't find the most optimal solution.  We start with the initial boxes and collect all the candies we can from them. Then we choose the box with the most candies that we can open using the keys we have, and collect all the candies we can from that box. We repeat this process until we can't open any more boxes or collect any more candies.  Time Complexity: O(n^2), where n is the number of boxes.  Memory Complexity: O(n), where n is the number of boxes.	BFS or DFS would be better.  We start with the initial boxes and perform a randomized search on all the boxes we can access from them. At each step, we randomly choose one of the unvisited boxes that we can access, and add any new keys we find to our keyring, and use them to open any new boxes we come across. We keep track of the candies we collect along the way and return the total number of candies collected.  Time Complexity: O(n^2), where n is the number of boxes.  Memory Complexity: O(n), where n is the number of boxes.	Correct!  Here, we use BFS to traverse through the boxes and collect the maximum number of candies. Initially, we add keys from the initial boxes to the status of the boxes and collect candies from those boxes. We then check the contained boxes in those boxes, add keys from those boxes to the status and collect candies from those boxes as well. We continue to do this until there are no more unvisited boxes. The BFS algorithm ensures that we traverse through all the boxes in a breadth-first manner.  Time Complexity: O(n+m), where n is the number of boxes and m is the total number of keys and contained boxes in the boxes. We traverse through all the boxes and perform constant time operations on each box, hence the linear time complexity.  Memory Complexity: O(n), where n is the number of boxes. We use a boolean visited array to keep track of the visited boxes, hence the linear space complexity.	Not applicable in this case!	The brute-force approach is barely viable, but we would start by iterating over all possible combinations of boxes we can choose to open. For each combination, we check if we can open all the boxes in the combination, and if we can, we calculate the total number of candies we can collect from them. We return the maximum total number of candies we can collect from any combination of boxes.  Time Complexity: O(2^n * n), where n is the number of boxes.  Memory Complexity: O(1).	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Maximum%20Candies%20You%20Can%20Get%20from%20Boxes.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Maximum%20Candies%20You%20Can%20Get%20from%20Boxes.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Maximum%20Candies%20You%20Can%20Get%20from%20Boxes.java	https://youtu.be/Psfim3qitWA
Algorithms Pack	Breadth-First Search	Shortest Bridge	https://leetcode.com/problems/shortest-bridge	You are given an n x n binary matrix grid where 1 represents land and 0 represents water.    An island is a 4-directionally connected group of 1's not connected to any other 1's. There are exactly two islands in grid.    You may change 0's to 1's to connect the two islands to form one island.    Return the smallest number of 0's you must flip to connect the two islands.	O(n^2)	O(n^2)	We're expecting an optimized solution of time complexity O(n^2) and memory complexity O(n^2).	Union-Find	Browning's Approach	Flood Fill	BFS + Flood Fill	D	Not quite!  We can use the union-find algorithm to label each island with a unique number, and then try flipping each 0 to a 1 and checking if we can connect both islands. We can do this by checking if the two neighboring cells of the flipped 0 are part of different islands, and then joining the islands if they are not. We repeat this process for all 0's in the grid and return the minimum number of flips needed to connect the islands.  Why it's suboptimal: This approach has a time complexity of O(n^2 alpha(n)) where alpha(n) is the inverse Ackermann function, which is a very slow-growing function. In practice, this function can be considered constant, but the worst-case time complexity is still worse than the flood-fill approach.  Time Complexity: O(n^2 alpha(n))	No! This refers to 'A Bridge Too Far' rather than the 'Shortest Bridge'. For what it's worth, Browning's Approach didn't work out particularly well in that instance either.	Unfortunately, BFS is a must for efficiency!  This approach has a time complexity of O(n^3) since we need to iterate over all cells in the grid and for each cell, we may need to perform a BFS operation that could potentially traverse the entire grid.  Time Complexity: O(n^3).	Correct!  Our solution uses Flood Fill and Breadth-First Search (BFS), to find and connect the two islands. The Flood Fill algorithm is used to find one of the islands and change its values to a different number (2 in this case). Then, BFS is used to explore the neighboring cells of this island until it reaches the other island. During BFS, we keep track of the level or depth of the BFS, which gives us the smallest number of 0's we need to flip to connect the two islands.  Time Complexity: O(n^2), where n is the size of the grid, as we traverse the entire grid twice: once in the Flood Fill algorithm and once in the BFS algorithm.  Memory Complexity: O(n^2), as we use a queue to store the cells during BFS, which can hold up to n^2 cells in the worst case.	The brute-force here is very expensive. We can try flipping each 0 to a 1 and then checking if we can connect both islands. We can do this using DFS or BFS, checking if we can reach both islands from the flipped 0. We repeat this process for all 0's in the grid and return the minimum number of flips needed to connect the islands.  Why it's suboptimal: This approach has a time complexity of O(n^4) since we need to iterate over all cells in the grid and for each cell, we may need to perform a BFS or DFS operation that could potentially traverse the entire grid.  Time Complexity: O(n^4)	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Shortest%20Bridge.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Shortest%20Bridge.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Shortest%20Bridge.java	https://youtu.be/SihF6njhWvU
Algorithms Pack	Breadth-First Search	Tree Diameter	https://leetcode.com/problems/tree-diameter/	The diameter of a tree is the number of edges in the longest path in that tree.    There is an undirected tree of n nodes labeled from 0 to n - 1. You are given a 2D array edges where edges.length == n - 1 and edges[i] = [ai, bi] indicates that there is an undirected edge between nodes ai and bi in the tree.    Return the diameter of the tree.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Floyd-Warshall	DFS	Dynamic Programming	Oak's Algorithm	B	Not best in this case.  The Floyd-Warshall algorithm is a good algorithm for finding the shortest path between all pairs of nodes in a graph, but it is overkill for this problem. It has a time complexity of O(n^3), which is much worse than the O(n) time complexity of our solution.	Correct!  Our solution can be called a tree traversal algorithm, specifically a depth-first search (DFS). The algorithm uses the concept of height to calculate the diameter of the tree. Starting from a root node, we recursively traverse each child node and calculate its height, which is the maximum height of its children nodes plus one. The two maximum heights among all children nodes are then added up to give the maximum path length that goes through the current node. We keep updating the maximum path length until we reach the end of the tree.  Time Complexity: O(n).  Memory Complexity: O(n).	Not ideal, although a reasonable idea!  Method: dynamic programming is a technique that solves a complex problem by breaking it down into smaller subproblems and solving each subproblem only once. In the case of trees, we can define a function f(node) that returns the height of the node, and use it to calculate the diameter of the tree. We can then use memoization to store the results of each subproblem to avoid solving them multiple times.   Weakness: efficiency! The time complexity of this approach is O(n^2), and the space complexity is also O(n^2).	This doesn't exist, and you've unearthed a tree-based pun. Q) Why did Professor Oak plant a tree on his computer? A) So he could 'log' on ^_^	The Brute Force approach simply calculates the diameter of the tree by comparing the length of all possible paths. The time complexity of this approach is O(n^2), where n is the number of nodes in the tree. This is because we need to calculate the length of all possible paths, which is proportional to the number of pairs of nodes in the tree. The space complexity of this approach is O(1) since we don't need to store any additional data.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Tree%20Diameter.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Tree%20Diameter.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Tree%20Diameter.java	https://youtu.be/WzaPOq9W8IQ
Algorithms Pack	Topological Sorting	Course Schedule I	https://leetcode.com/problems/course-schedule/	There are a total of numCourses courses you have to take, labeled from 0 to numCourses - 1. You are given an array prerequisites where prerequisites[i] = [ai, bi] indicates that you must take course bi first if you want to take course ai.    For example, the pair [0, 1], indicates that to take course 0 you have to first take course 1.  Return true if you can finish all courses. Otherwise, return false.	O(n+m)	O(n+m)	We're expecting an optimized solution of time complexity O(n+m) and memory complexity O(n+m).	Sorting	Kahn's Algorithm	Backtracking	Greedy	B	Not correct here.  Simply, this approach may not work in cases where there are circular dependencies between courses. It also does not find a valid ordering of courses.	Correct!  Kahn's algorithm is a well-known algorithm for finding a topological ordering of a directed acyclic graph (DAG). In the context of this problem, we can use Kahn's algorithm to determine if it's possible to complete all the courses. The algorithm works by iteratively selecting nodes with zero incoming edges (indegree) and removing them from the graph until there are no more nodes left.  Time Complexity: O(V+E), where V is the number of nodes and E is the number of edges. This is because we need to traverse each node and edge exactly once during the algorithm.  Memory Complexity: O(V+E), where V is the number of nodes and E is the number of edges. This is because we need to store the graph and the indegree vector, each of which takes O(V+E) space. Additionally, we need to store a queue and a visited vector, which each take O(V) space.	This will work, but it is a rather complicated approach. Topological sorting would be better.	This approach may not always yield a valid ordering of the courses. For example, if there are two courses that have the same number of prerequisites, the greedy algorithm may choose the wrong course to take first	The brute-force here would involve generating all possible orderings of courses, and check if each ordering satisfies the prerequisites.  Why it's suboptimal: The number of possible orderings of courses is n!, where n is the number of courses. This approach is therefore impractical for any reasonable value of n.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Course%20Schedule.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Course%20Schedule.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Course%20Schedule.java	https://youtu.be/qdHAZnYQUSo
Algorithms Pack	Topological Sorting	Parallel Courses	https://leetcode.com/problems/parallel-courses/	You are given an integer n, which indicates that there are n courses labeled from 1 to n. You are also given an array relations where relations[i] = [prevCoursei, nextCoursei], representing a prerequisite relationship between course prevCoursei and course nextCoursei: course prevCoursei has to be taken before course nextCoursei.    In one semester, you can take any number of courses as long as you have taken all the prerequisites in the previous semester for the courses you are taking.    Return the minimum number of semesters needed to take all courses. If there is no way to take all the courses, return -1.	O(n+m)	O(n+m)	We're expecting an optimized solution of time complexity O(n+m) and memory complexity O(n+m).	DFS	Floyd-Warshall	Greedy	Kahn's Algorithm	D	Not optimum in this case!  Method: use a depth-first search (DFS) traversal to generate all possible course orderings that satisfy the prerequisite relationships and return the minimum number of semesters needed among them. This approach would also be very inefficient, as the number of possible orderings grows exponentially with the number of courses and prerequisites.  Weakness: this approach is suboptimal because it would take too much time to generate all possible course orderings and compare their semesters needed.	By no means the best approach!  Method: compute the transitive closure of the graph of prerequisite relationships using the Floyd-Warshall algorithm. The length of the longest path in the resulting graph gives the minimum number of semesters required.  Weakness: This approach has a time complexity of O(n^3), which is worse than the other approaches for large values of n.	Incorrect!  This won't work for cases where a course has uncompleted prerequisites, leading to an incomplete sequence of courses.	Correct!  Our solution uses topological sorting to determine the minimum number of semesters needed to take all courses. We first create a directed graph from the given prerequisite relationships, with the courses as nodes and the prerequisites as edges. We then perform a topological sort on the graph, which orders the nodes such that for any directed edge (u, v), u comes before v in the ordering. This allows us to determine the minimum number of semesters needed to take all courses, as each semester can consist of any set of courses that come after all their prerequisites in the ordering.  Time Complexity: O(n + m), where n is the number of courses and m is the number of prerequisite relationships, as we need to perform a linear scan of the relations vector to create the graph and then perform a topological sort on the graph.  Memory Complexity: O(n + m), as we need to store the graph and the incoming edge counts for each node.	Generate all possible combinations of courses that satisfy the prerequisite relationships and return the minimum number of semesters needed among them. This approach would be very inefficient, as the number of possible combinations grows exponentially with the number of courses and prerequisites.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Parallel%20Courses.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Parallel%20Courses.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Parallel%20Courses.java	https://youtu.be/jWoc4tELCTk
Algorithms Pack	Topological Sorting	Sequence Reconstruction	https://leetcode.com/problems/sequence-reconstruction/	You are given an integer array nums of length n where nums is a permutation of the integers in the range [1, n]. You are also given a 2D integer array sequences where sequences[i] is a subsequence of nums.    Check if nums is the shortest possible and the only supersequence. The shortest supersequence is a sequence with the shortest length and has all sequences[i] as subsequences. There could be multiple valid supersequences for the given array sequences.    For example, for sequences = [[1,2],[1,3]], there are two shortest supersequences, [1,2,3] and [1,3,2].  While for sequences = [[1,2],[1,3],[1,2,3]], the only shortest supersequence possible is [1,2,3]. [1,2,3,4] is a possible supersequence but not the shortest.  Return true if nums is the only shortest supersequence for sequences, or false otherwise.    A subsequence is a sequence that can be derived from another sequence by deleting some or no elements without changing the order of the remaining elements.	O(n+m)	O(n+m)	We're expecting an optimized solution of time complexity O(n+m) and memory complexity O(n+m).	Topological Sort	BFS on Subsequences	Hash Set + Iteration	Two Pointers	A	Correct!  Our approach uses a directed graph to represent the given sequences, where the vertices are the unique numbers from the array nums, and an edge (u, v) represents that u comes before v in some sequence. We then perform a topological sort on the graph, which will give us a linear ordering of the vertices such that every edge goes from left to right. We check whether the resulting order is the same as the original array nums.  If the topological sort returns an empty vector, this means that the graph has a cycle, and we cannot construct a valid supersequence. If the resulting ordering has a different length than nums, then the resulting ordering is not a supersequence of sequences, and thus not the shortest.  Time Complexity: O(n + m), where n is the number of unique elements in nums, and m is the total number of elements in sequences.  Memory Complexity: O(n + m), since we use a hash table to represent the graph and a queue to perform the topological sort.	Expensive!  Build a graph where each node represents a number in nums and each edge represents a subsequence relationship between two numbers. Perform a topological sort on the graph, then check if the order of nodes in the sorted list matches the order of nums. Return true if they match, false otherwise.  Time complexity: O(n^2 + m), where m is the total number of subsequence relationships in sequences  Memory complexity: O(n + m).	The topological sort is better for this problem.  Approach: Store all numbers in nums in a hash set. For each subsequence in sequences, iterate through the numbers in the subsequence and check if they are in the hash set. If all numbers are found in the hash set and appear in the correct order, return true, false otherwise.  Time complexity: O(n*m), where m is the total number of subsequence relationships in sequences  Memory complexity: O(n).	Not quite optimal in terms of time.  Approach: Initialize two pointers, i and j, to point at the start of nums and the start of the first subsequence in sequences, respectively. Iterate through sequences, for each subsequence, move pointer j to the first occurrence of the first number in the subsequence in nums. Then, for each number in the subsequence, move pointer i to the next occurrence of the number in nums. If we reach the end of nums before iterating through all subsequence numbers, return false. If we reach the end of all subsequences and pointer i is at the end of nums, return true, false otherwise.  Time complexity: O(n*m), where m is the total number of subsequence relationships in sequences  Memory complexity: O(1).	Brute-force approach: Generate all permutations of the numbers in the range [1, n], then check if each permutation is a supersequence of sequences. Return true if we find a supersequence that is the same as nums, false otherwise.  Time complexity: O(n!), where n is the length of nums  Memory complexity: O(n), but the time complexity is woeful here.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Sequence%20Reconstruction.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Sequence%20Reconstruction.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Sequence%20Reconstruction.java	https://youtu.be/Hz7OrzMTBNI
Algorithms Pack	Topological Sorting	Minimum Height Trees	https://leetcode.com/problems/minimum-height-trees/	A tree is an undirected graph in which any two vertices are connected by exactly one path. In other words, any connected graph without simple cycles is a tree.    Given a tree of n nodes labelled from 0 to n - 1, and an array of n - 1 edges where edges[i] = [ai, bi] indicates that there is an undirected edge between the two nodes ai and bi in the tree, you can choose any node of the tree as the root. When you select a node x as the root, the result tree has height h. Among all possible rooted trees, those with minimum height (i.e. min(h)) are called minimum height trees (MHTs).    Return a list of all MHTs' root labels. You can return the answer in any order.    The height of a rooted tree is the number of edges on the longest downward path between the root and a leaf.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Topological Sort	Greedy	Tree Diameter Calculation	BFS/DFS	A	Correct!  We can use the Topological Sort algorithm to find the minimum height trees (MHTs) in a given tree. The idea is to iteratively remove the leaf nodes (i.e., the nodes with only one incoming edge) from the tree until there are at most two nodes left, which are the roots of the MHTs.  We start by building an adjacency list representation of the given tree using the addEdge() function. Then, we apply the topological sort algorithm using the topSort() function, which returns the roots of the MHTs. In the topSort() function, we first compute the incoming edges for each node in the graph, and initialize a queue with the leaf nodes (i.e., nodes with only one incoming edge). Then, we iteratively remove the leaf nodes, updating the incoming edges of their neighbors, until there are at most two nodes left in the graph. Finally, we return the roots of the MHTs, which are the nodes left in the queue.  Time Complexity: O(n), where n is the number of nodes in the given tree. The topological sort algorithm used in the solution has a time complexity of O(V+E), where V is the number of vertices and E is the number of edges in the graph. In the given problem, the input is a tree, which has n nodes and n-1 edges, so the number of edges is E = n-1.  Memory Complexity: O(n), where n is the number of nodes in the given tree.	Topological Sorting is preferred.  This approach does not always yield the correct answer. For instance, there can be cases where the initial root node is far away from the center of the tree, and the nodes closest to the root node are not the ones that form the minimum height trees.  Time Complexity: O(n^2).	Not ideal!  The diameter-based approach can work for some cases, but it may not always yield the correct answer. There can be cases where the minimum height trees are not close to the center of the diameter path. Moreover, computing the diameter of the tree can be computationally expensive.  We can compute the diameter of the tree (i.e., the length of the longest path in the tree) and select the nodes that are in the middle of the diameter path as the root(s). The intuition behind this approach is that the root(s) of the minimum height trees should be close to the center of the tree.  Time Complexity is O(n^2) even if correct.	Incorrect!  This approach can work for some cases, but it may not always yield the correct answer. There can be cases where the minimum height trees are not formed by the farthest nodes from the two BFS's. Moreover, this approach can be computationally expensive, as we need to perform two BFS's in the worst case.	Brute-Force Approach: generate all possible trees by considering each node as the root and recursively constructing the subtree for each child. Then, calculate the height of each tree and return the roots of the trees with minimum height. This approach has an exponential time complexity of O(n * 2^n), as there are 2^n possible subtrees for each of the n nodes in the tree.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Minimum%20Height%20Trees.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Minimum%20Height%20Trees.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Minimum%20Height%20Trees.java	https://youtu.be/JUtZUNyiqAA
Algorithms Pack	Topological Sorting	Longest Increasing Path In A Matrix	https://leetcode.com/problems/longest-increasing-path-in-a-matrix/	Given an m x n integers matrix, return the length of the longest increasing path in matrix.    From each cell, you can either move in four directions: left, right, up, or down. You may not move diagonally or move outside the boundary (i.e., wrap-around is not allowed).	O(mn + E)	O(nm)	We're expecting an optimized solution of time complexity O(mn + E) and memory complexity O(nm).	Dynamic Programming	Topological Sort + BFS	Greedy	BFS	B	Suboptimal.  For each cell, store the length of the longest increasing path that ends at that cell. We can then compute the length of the longest increasing path that ends at each cell based on the length of the longest increasing path that ends at its neighbors. This approach is suboptimal because it doesn't take into account the fact that a longer path may be possible if we start at a cell with a lower value. The time complexity is O(mnlog(m*n)) if we use a priority queue to update the length of the longest increasing path ending at each cell.	Correct!  Our approach involves using Topological Sort, which is a way of ordering nodes in a directed graph such that all edges go from earlier ordered nodes to later ordered nodes. This method is useful in detecting cycles and finding the longest path in a DAG (Directed Acyclic Graph).  We first calculate the number of incoming edges for each node in the graph by checking its neighbouring nodes (if they are bigger, we create an edge from the original node to the bigger neighbour, otherwise, we count an incoming edge). We add all nodes with 0 incoming edges to a queue, which represents the start of our path. We then perform a BFS on the graph, incrementing a counter for each level we explore. For each level, we remove all nodes that we have explored from the graph by decreasing the incoming edges of their neighbours and adding any new nodes with 0 incoming edges to the queue. The length of the longest path is the final counter value.  Time Complexity: O(mn + E) where E is the number of edges in the graph. In this problem, the number of edges can be as much as O(mn), giving a worst-case time complexity of O((mn)^2).  Memory Complexity: O(mn), since we create an incoming edge array with m*n cells, and use a queue to hold nodes.	Not so useful here. In some cases, the path that initially appears to be the longest can lead to a dead end that makes the actual longest path shorter.	BFS would be a more naïve approach here. Topological sorting is much better here, as we can treat the matrix like a DAG.	The brute-force is too simplistic. We can iterate over every cell in the matrix, and for each cell, explore all possible paths that increase in value until we can't move any further. Track the length of the longest path seen so far and return it.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Longest%20Increasing%20Path%20in%20a%20Matrix.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Longest%20Increasing%20Path%20in%20a%20Matrix.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Longest%20Increasing%20Path%20in%20a%20Matrix.java	https://youtu.be/2CFdl8B5Y4U
Algorithms Pack	Topological Sorting	Parallel Courses III	https://leetcode.com/problems/parallel-courses-iii/	You are given an integer n, which indicates that there are n courses labeled from 1 to n. You are also given a 2D integer array relations where relations[j] = [prevCoursej, nextCoursej] denotes that course prevCoursej has to be completed before course nextCoursej (prerequisite relationship). Furthermore, you are given a 0-indexed integer array time where time[i] denotes how many months it takes to complete the (i+1)th course.    You must find the minimum number of months needed to complete all the courses following these rules:    You may start taking a course at any time if the prerequisites are met.  Any number of courses can be taken at the same time.  Return the minimum number of months needed to complete all the courses.    Note: The test cases are generated such that it is possible to complete every course (i.e., the graph is a directed acyclic graph).	O(n+m)	O(n+m)	We're expecting an optimized solution of time complexity O(n+m) and memory complexity O(n+m).	Sorting + Recursion	Topological Sorting + DP	DFS	BFS	B	Incorrect!  This approach may not work in all cases since there could be cases where a course with a longer completion time has prerequisites that are quicker to complete, thus it might not be optimal to start with the longest course.	Correct!  Firstly, the graph is represented as an adjacency list, and we use topological sort to obtain the order in which the courses can be taken. In topological sorting, we start with nodes that have no incoming edges (prerequisites), and gradually remove nodes from the graph while keeping track of the order in which they were visited.  Secondly, we use dynamic programming to calculate the minimum time required to complete each course. For each course, we add its completion time to the time required to complete its prerequisites. We then take the maximum of these values to determine the minimum time required to complete the course.  Finally, we return the maximum time required to complete any course.  Time and Memory Complexity: O(n+m).	A topological sort is a better approach.  This approach may not work in all cases since DFS does not guarantee the correct order in which the courses should be taken, and may lead to wrong solutions.	Not quite.  Although BFS can find a topological order of the graph and guarantee that all prerequisites are completed before each course, this approach does not take into account the fact that multiple courses can be taken at the same time, leading to suboptimal complexity in the solution.	Brute-Force Solution: Generate all possible course completion sequences, and for each sequence, check if all prerequisites are met before taking each course. Calculate the total time required for each sequence and return the minimum time among all valid sequences.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Parallel%20Courses%20III.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Parallel%20Courses%20III.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Parallel%20Courses%20III.java	https://youtu.be/IAQkySm81-E
Algorithms Pack	Topological Sorting	Largest Color Value In A Directed Graph	https://leetcode.com/problems/largest-color-value-in-a-directed-graph/	There is a directed graph of n colored nodes and m edges. The nodes are numbered from 0 to n - 1.    You are given a string colors where colors[i] is a lowercase English letter representing the color of the ith node in this graph (0-indexed). You are also given a 2D array edges where edges[j] = [aj, bj] indicates that there is a directed edge from node aj to node bj.    A valid path in the graph is a sequence of nodes x1 -> x2 -> x3 -> ... -> xk such that there is a directed edge from xi to xi+1 for every 1 <= i < k. The color value of the path is the number of nodes that are colored the most frequently occurring color along that path.    Return the largest color value of any valid path in the given graph, or -1 if the graph contains a cycle.	O(m+n)	O(m)	We're expecting an optimized solution of time complexity O(n+m) and memory complexity O(m).	Floyd-Warshall	Niji's Algorithm	Dijkstra's Algorithm	Kahn's Algorithm + Dynamic Programming	D	Suboptimal.  We use Floyd-Warshall algorithm to calculate the shortest path between all pairs of nodes and for each path, calculate the maximum color value.  However, this approach is suboptimal because it involves computing the shortest path between all pairs of nodes, which is not necessary for this problem. As you should know, it's O(n^3) in complexity!	Color me disappointed! You've picked one of the few entirely fictional algorithms. 'Niji' is Japanese for rainbow.	Incorrect!  Dijkstra's algorithm does not work on graphs with negative-weight cycles, and this problem explicitly asks to return -1 if the graph contains a cycle.	Correct!  Our approach to this tough problem uses topological sort to traverse the graph in a topological order. In each iteration, we calculate the color values of each node that can be reached from the current node by traversing an edge in the graph. We do this using dynamic programming. We keep track of the count of each color along the current path and update the maximum count of each color that we have seen so far at each node. The largest count of any color encountered along any valid path is the answer we return.  Time Complexity: O(V+E), where V is the number of nodes and E is the number of edges in the graph. This is because we use a modified topological sort algorithm to traverse the graph in topological order, and for each node, we update the color values of its outgoing edges. This is done in constant time per node.  Memory Complexity: O(V*26), where V is the number of nodes in the graph and 26 is the number of lowercase English letters. This is because we keep a count of the number of nodes along each path with a certain color, and we have 26 possible colors.	The brute-force here isn't particularly helpful. We generate all possible paths in the graph and compute the color value of each path, then return the maximum color value among all valid paths.  Why it's suboptimal: The number of possible paths in a graph can be very large, making this approach highly inefficient. The time complexity is exponential in the worst case.  Time Complexity: O(2^n)	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Largest%20Color%20Value%20in%20a%20Directed%20Graph.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Largest%20Color%20Value%20in%20a%20Directed%20Graph.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Largest%20Color%20Value%20in%20a%20Directed%20Graph.java	https://youtu.be/wjXSYOWr-CA
Algorithms Pack	Topological Sorting	Strange Printer II	https://leetcode.com/problems/strange-printer-ii/	There is a strange printer with the following two special requirements:    On each turn, the printer will print a solid rectangular pattern of a single color on the grid. This will cover up the existing colors in the rectangle.  Once the printer has used a color for the above operation, the same color cannot be used again.  You are given a m x n matrix targetGrid, where targetGrid[row][col] is the color in the position (row, col) of the grid.    Return true if it is possible to print the matrix targetGrid, otherwise, return false.	O(n^3)	O(n^2)	We're expecting an optimized solution of (rough) time complexity O(n^3) and memory complexity O(n^2).	Topological Sorting	Backtracking	Greedy	Linear Programming	A	Correct!  Here, we represent the given matrix of colors as a graph, where each color is a node and there is a directed edge from node A to node B if color A appears in a rectangle that completely covers color B. We can then use topological sorting to find if there is a valid ordering of colors such that each rectangle can be printed in a valid order without violating the printer's rules. If we find a valid topological ordering, then we return true, otherwise, we return false.  Time Complexity is a little tough to calculate here, but we settled on O(rows * cols + Colors^2 + V + E), which is effectively O(n^2).	No, this would have exponential time complexity.	Incorrect!  The greedy coloring approach may fail to find a valid solution in some cases where mutually dependent colors are involved. In such cases, there may not be a valid order in which the colors can be colored such that each color can be covered by a single rectangular pattern.	Not realistically viable.  The LP approach can have high computational complexity and may not always guarantee finding an optimal or even feasible solution. Moreover, the LP formulation may require a large number of variables and constraints, which can make the problem intractable for large grids.	One possible brute-force solution would be to try all possible combinations of colors that can be used to cover the entire target grid. We can start with the color that appears the least number of times in the target grid, and then try all possible rectangular patterns of that color that cover all the cells with that color. Then, we move on to the next least frequent color and repeat the process until we have tried all possible combinations of colors. This approach would have exponential time complexity and is not practical for larger input sizes.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Strange%20Printer%20II.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Strange%20Printer%20II.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Strange%20Printer%20II.java	https://youtu.be/_7yroWKxcwU
Algorithms Pack	Topological Sorting	Course Schedule II	https://leetcode.com/problems/course-schedule-ii/	There are a total of numCourses courses you have to take, labeled from 0 to numCourses - 1. You are given an array prerequisites where prerequisites[i] = [ai, bi] indicates that you must take course bi first if you want to take course ai.    For example, the pair [0, 1], indicates that to take course 0 you have to first take course 1.  Return the ordering of courses you should take to finish all courses. If there are many valid answers, return any of them. If it is impossible to finish all courses, return an empty array.	O(n+m)	O(n+m)	We're expecting an optimized solution of time complexity O(n+m) and memory complexity O(n+m).	Backtracking	Topological Sort	DFS + Memoization	Greedy	B	Ouch!  Backtracking is the brute-force approach!	Correct!  This very common problem can be modeled as a directed graph, where each course is a node, and each prerequisite relationship is an edge from the prerequisite course to the course that depends on it. We can use topological sorting to find a valid order to take the courses, which requires that we first take all the prerequisites for a course before taking the course itself.  Time Complexity: O(V+E), where V is the number of vertices (courses) and E is the number of edges (prerequisites).  Memory Complexity: O(V+E), where V is the number of vertices (courses) and E is the number of edges (prerequisites).	A 'simple' topological sort is preferred to this idea.  This approach uses memoization to avoid recomputing the prerequisites of each course, but it still generates multiple topological orders, and it is not clear which one is the correct one. In addition, it requires extra checks to detect invalid solutions.  Time Complexity: O(n^2).	Suboptimal.  The greedy approach does not always produce a valid ordering, and there can be multiple valid orderings that it might miss. For example, if there are two disjoint sets of courses that have no prerequisites within each set, the greedy algorithm may take courses in the wrong order, resulting in an invalid ordering. It also fails when there are cycles in the graph.	The brute-force approach is to generate all possible permutations of the courses and check if they satisfy the prerequisites.  Why it's suboptimal: This approach generates all possible permutations of the courses, which can be very time-consuming for large inputs. In addition, it does not take into account the prerequisites of each course, which can lead to invalid solutions.  Time Complexity: O(n!).	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Course%20Schedule%20II.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Course%20Schedule%20II.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Course%20Schedule%20II.java	https://youtu.be/B1S0Veyxxas
Algorithms Pack	Dynamic Programming (General)	Fibonacci Number	https://leetcode.com/problems/fibonacci-number/	The Fibonacci numbers, commonly denoted F(n) form a sequence, called the Fibonacci sequence, such that each number is the sum of the two preceding ones, starting from 0 and 1. That is,    F(0) = 0, F(1) = 1  F(n) = F(n - 1) + F(n - 2), for n > 1.  Given n, calculate F(n).    n will NEVER be higher than 30.	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	Simple Recursion	Simple Iterative Solution	Hash Table	Union Find	B	This approach is basically the brute-force method, and is not recommended for this problem.	Correct!  We use an iterative approach to compute the Fibonacci number at the nth position. We initialize two variables a and b to 0 and 1 respectively. Then, we iterate n-1 times and update the value of a to b and b to a+b. Finally, we return the value of b.  Why it's efficient: This approach uses constant space and takes linear time in the number n to compute the nth Fibonacci number. It is more efficient than the recursive approach which has an exponential time complexity.  Time Complexity: O(n)  Memory Complexity: O(1)	Using a wasteful data structure would be unwise for this simple problem.	Whether or not you happen engineer a solution with this approach, you are clearly over-engineering if you select this option.	Starting from F(0) and F(1), we can recursively calculate each Fibonacci number by summing the previous two.  Why it's suboptimal: The time complexity of this approach is exponential, O(2^n), making it impractical for larger values of n.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Fibonacci%20Number.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Fibonacci%20Number.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Fibonacci%20Number.java	https://youtu.be/BxvuAQ5rIpE
Algorithms Pack	Dynamic Programming (General)	Climbing Stairs	https://leetcode.com/problems/climbing-stairs/	You are climbing a staircase. It takes n steps to reach the top.    Each time you can either climb 1 or 2 steps. In how many distinct ways can you climb to the top?	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Simple Recursion	Greedy	Divide And Conquer	Dynamic Programming (Memoization)	D	Too expensive! Yes, we can recursively try all possible combinations of 1 step and 2 steps at each step, and count the number of combinations that lead to reaching n steps.  Why it's suboptimal: This approach has exponential time complexity and is very inefficient for large n. For n=44, it takes over 700 million function calls to compute the result.  Time Complexity: O(2^n)	This approach does not always lead to the optimal solution. For example, if n=4, the greedy approach would choose two steps followed by two steps, which results in 2 distinct ways to reach the top. However, the optimal solution is to take one step, followed by three steps, which results in 3 distinct ways to reach the top.	Not ideal. This approach leads to a large number of repeated computations, and therefore, has a high time complexity.	Correct! (Honestly, there is a better ITERATIVE approach too, covered in the Fibonacci Sequence problem within this set).  We use dynamic programming with memoization to solve this problem. We start by initializing a memory array of size n+1, where memory[i] will store the number of ways to reach step i. We know that we can reach step i by taking either one or two steps from step i-1 or i-2 respectively. Therefore, we can calculate the number of ways to reach step i by summing up the number of ways to reach step i-1 and i-2. We store these values in memory[i] and return memory[n] as the answer.  Why it's optimal: This approach is optimal because we are storing the intermediate results in memory, which means we do not need to calculate the same subproblems repeatedly. As a result, the time complexity is O(n), which is very efficient.  Time Complexity: O(n)  Memory Complexity: O(n)	The brute-force approach here is to generate all possible combinations of 1's and 2's to climb the n steps and count the number of valid combinations.  Why it's suboptimal: This approach has an exponential time complexity of O(2^n) since for every step, we have two options (climb 1 or 2). It quickly becomes infeasible for large values of n.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Climbing%20Stairs.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Climbing%20Stairs.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Climbing%20Stairs.java	https://youtu.be/7lvvkWjUEqE
Algorithms Pack	Dynamic Programming (General)	Longest Increasing Subsequence	https://leetcode.com/problems/longest-increasing-subsequence/	Given an integer array nums, return the length of the longest strictly increasing   subsequence.	O(n^2)	O(n)	We're expecting an optimized solution of time complexity O(n^2) and memory complexity O(n).	Dynamic Programming (Memoization)	Greedy	Two Pointers	Prefix Max Array	A	Correct!  Our approach here is to find the length of the longest increasing subsequence. We begin by iterating through the array starting at each index and recursively finding the length of the longest increasing subsequence for the remaining indices that are larger than the current index. We store the results of the recursive calls in a memoization table to avoid repeated work. The base case is when we reach the end of the array, at which point we return 0. We add 1 to the result of each recursive call because we are considering the current index as part of the subsequence. The final result is the maximum length of the increasing subsequence found across all possible starting indices.  Time Complexity: O(n^2)  Memory Complexity: O(n).	This approach does not guarantee that the resulting subsequence will be the longest possible increasing subsequence. For example, consider the array [3,4,1,2,3,4]. The greedy approach will give a subsequence of length 4, whereas the longest possible subsequence has length 5.	This approach fails to account for cases where the longest increasing subsequence is not a contiguous subsequence in the input array.	This (incorrect) approach involves creating a prefix max array of the input array, where each element of the prefix max array at index i stores the maximum value in the input array up to index i. Then, we iterate through the prefix max array and keep track of the maximum subsequence length encountered so far. However, this approach fails to account for cases where the longest increasing subsequence is not a contiguous subsequence in the input array.	Brute-force: generate all possible subsequences and check if each subsequence is strictly increasing.  Why it's suboptimal: This approach has an exponential time complexity, as there are 2^n possible subsequences for a sequence of length n.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Longest%20Increasing%20Subsequence.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Longest%20Increasing%20Subsequence.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Longest%20Increasing%20Subsequence.java	https://youtu.be/5tWQR9pTCWc
Algorithms Pack	Dynamic Programming (General)	Longest Common Subsequence	https://leetcode.com/problems/longest-common-subsequence/	Given two strings text1 and text2, return the length of their longest common subsequence. If there is no common subsequence, return 0.    A subsequence of a string is a new string generated from the original string with some characters (can be none) deleted without changing the relative order of the remaining characters.    For example, 'ace' is a subsequence of 'abcde'.  A common subsequence of two strings is a subsequence that is common to both strings.	O(nm)	O(nm)	We're expecting an optimized solution of time complexity O(nm) and memory complexity O(nm).	Greedy	Divide and Conquer	Dynamic Programming (Memoization)	Backtracking	C	This approach does not guarantee finding the longest common subsequence in all cases. In some cases, it may return a suboptimal solution.	This approach has a high time complexity, as it requires solving the problem recursively for multiple subproblems. We divide both input strings into two halves and recursively find the longest common subsequence between each pair of halves. We then combine the two subsequences to find the longest common subsequence between the full input strings.	Correct!  In this approach, we make use of memoization to avoid recalculating the same subproblems repeatedly. We define a recursive function LCS to compute the longest common subsequence of two given strings. At each recursive call, we check if we have already calculated the LCS of the current substrings, and if so, we simply return the value stored in our memoization table. If not, we check if the current characters in both strings match. If they do, we add 1 to the length of the LCS, and recursively call LCS on the remaining substrings. If the characters do not match, we try two options: either skip the current character in the first string and move to the next character, or skip the current character in the second string and move to the next character. We take the maximum of the lengths of the two resulting LCSs.  Time Complexity: O(mn), where m and n are the lengths of the two input strings. This is because we have to calculate the LCS for each substring of the two input strings.  Memory Complexity: O(mn), where m and n are the lengths of the two input strings. This is because we create a 2D array to store the results of each calculated subproblem.	This approach has an exponential time complexity, as it generates all possible subsequences of one of the strings, which can be up to 2^n where n is the length of the string. Additionally, it may miss some longer common subsequences that are not generated in the backtracking process.	The brute-force here is enumerate all possible subsequences of text1 and check if they are also a subsequence of text2. Return the length of the longest common subsequence found.  This approach is inefficient because it requires checking all possible subsequences of one string, which can take exponential time. Specifically, there are 2^n possible subsequences of a string of length n, so the time complexity of this approach is O(2^n).	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Longest%20Common%20Subsequence.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Longest%20Common%20Subsequence.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Longest%20Common%20Subsequence.java	https://youtu.be/SG_-BJdz3FQ
Algorithms Pack	Dynamic Programming (General)	Partition Equal SubSet Sum	https://leetcode.com/problems/partition-equal-subset-sum/	Given an integer array nums, return true if you can partition the array into two subsets such that the sum of the elements in both subsets is equal or false otherwise.	O(nT)	O(nT)	We're expecting an optimized solution of time complexity O(nT) and memory complexity O(nT).	Recursive Memoization	Knapsack Algorithm	Sorting + Greedy	DFS	A	Correct!  The approach is to check whether it is possible to partition the array into two subsets such that the sum of the elements in both subsets is equal. To do this, we first check if the sum of all the elements in the array is odd. If it is, we can immediately return false since we can't partition an odd sum into two equal parts. If the sum is even, we divide it by 2 and check if it is possible to find a subset of the given array that sums up to this value. We use memoization with recursion to solve this subproblem. We store the result of previously computed subproblems in a 2D array named memory. The array memory[i][j] represents the subproblem where we are checking if it is possible to find a subset with the first i elements of the array that sums up to j. If the value of memory[i][j] is already computed, we return it instead of recomputing it.  Time Complexity: O(n*t), where n is the length of the given array and t is the sum of all the elements in the array divided by 2.  Memory Complexity: O(n*t), where n is the length of the given array and t is the sum of all the elements in the array divided by 2.	Not quite! The knapsack algorithm is used to find the maximum value with limited capacity, but it doesn't guarantee that the selected items will sum up to exactly the capacity. Therefore, this approach may return a false negative result.	No. The idea is okay. We sort the input array in non-increasing order and add elements to each subset starting from the largest element until one of the subsets' sum equals or exceeds half of the total sum. We then check if the two subsets' sum is equal or not.  Why it's suboptimal: This approach doesn't always work. There may be cases where the optimal solution does not contain the largest elements in the input array. For example, consider the input array [1, 2, 3, 4, 5]. The greedy approach would put 5 in one subset and [4,3,2,1] in the other, resulting in an incorrect answer.	Not ideal. This approach suffers from the same problem as the Brute-Force approach, which is the exponential number of possible subsets. Therefore, it's not feasible for large inputs.	Our brute-force approach here depends on one crucial thing. For each element in the input array, we have two choices: either include it in one subset or exclude it from both subsets. We can generate all possible combinations of subsets and check if their sum is equal or not.  Why it's suboptimal: The number of possible subsets is 2^n where n is the size of the input array. Therefore, the time complexity is O(2^n). This is an exponential time complexity and is not feasible for large inputs.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Partition%20Equal%20Subset%20Sum.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Partition%20Equal%20Subset%20Sum.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Partition%20Equal%20Subset%20Sum.java	https://youtu.be/h4jKQiziLT8
Algorithms Pack	Dynamic Programming (General)	Maximum Height By Stacking Cuboids	https://leetcode.com/problems/maximum-height-by-stacking-cuboids/	Given n cuboids where the dimensions of the ith cuboid is cuboids[i] = [widthi, lengthi, heighti] (0-indexed). Choose a subset of cuboids and place them on each other.    You can place cuboid i on cuboid j if widthi <= widthj and lengthi <= lengthj and heighti <= heightj. You can rearrange any cuboid's dimensions by rotating it to put it on another cuboid.    Return the maximum height of the stacked cuboids.	O(n^2)	O(n^2)	We're expecting an optimized solution of time complexity O(n^2) and memory complexity O(n^2).	DFS	Greedy	Recursive Approach + Memoization	Recursive Backtracking	C	Suboptimal. Presumably, the idea is to represent each cuboid as a node in a graph? This approach is suboptimal since it may not always lead to the maximum height of the stacked cuboids, and the time complexity is exponential in the worst case.	This approach is suboptimal since it may not always lead to the maximum height of the stacked cuboids.	Correct!  We first sort each cuboid's dimensions and sort all cuboids in lexicographic order. Then we define a recursive function that, given the current and previous cuboids, computes the maximum height of a stack starting from the current cuboid. We make use of memoization to avoid repeated calculations.  Time Complexity: O(n^2), because we need to compare each pair of cuboids to see if one can be placed on top of the other. However, sorting the cuboids in lexicographic order improves the time complexity of the recursive function. The use of memoization avoids repeated calculations, which makes the time complexity faster.   Memory Complexity: O(n^2).	Not quite. This approach can still have an exponential time complexity in the worst case, as the number of possible combinations of cuboids can be very large. Additionally, the amount of memory required for memoization can also be very large.  The idea itself is sound as a tentative solution: we can try all possible combinations of cuboids recursively, and memoize the maximum height obtained for each state (current cuboid and previous cuboid). We can then use the memoized values to avoid recomputing the same states multiple times.	The brute-force here is very simple. We can generate all possible subsets of the given cuboids and check if they can be stacked on each other. For each valid subset, we can calculate the height of the stacked cuboids and return the maximum height among all subsets. This approach has exponential time complexity since the number of possible subsets is 2^n.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Maximum%20Height%20by%20Stacking%20Cuboids.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Maximum%20Height%20by%20Stacking%20Cuboids.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Maximum%20Height%20by%20Stacking%20Cuboids.java	https://youtu.be/npCU-qFF4E4
Algorithms Pack	Dynamic Programming (General)	House Robber	https://leetcode.com/problems/house-robber/	You are a professional robber planning to rob houses along a street. Each house has a certain amount of money stashed, the only constraint stopping you from robbing each of them is that adjacent houses have security systems connected and it will automatically contact the police if two adjacent houses were broken into on the same night.    Given an integer array nums representing the amount of money of each house, return the maximum amount of money you can rob tonight without alerting the police.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Greedy	Recursive Approach + Memoization	Sorting + Linear Scan	Divide And Conquer	B	Incorrect! This approach fails to consider cases where the optimal solution requires skipping some houses in order to maximize the total amount of money robbed.	Correct!  We use a recursive function to simulate the robbery at each house. At each house, we have two options: either rob the house or leave it. If we rob the house, we add the value of the house to our stolen goods, and we move on to the next house but skip the one after it (since we cannot rob adjacent houses). If we leave the house, we move on to the next house. We use memoization to avoid recomputing overlapping subproblems.  Time Complexity: O(n), where n is the number of houses.  Memory Complexity: O(n), where n is the number of houses.	This is just a terrible idea.	This approach does not consider the possibility that the optimal solution may involve robbing houses from both halves of the array. It is not guaranteed to give the optimal solution in all cases.	The brute force here would make one give up their life of crime! We can rob the first house and recursively rob the rest of the houses not adjacent to the current house, or we can skip the current house and rob the rest of the houses. We take the maximum of the two options.  Why it's suboptimal: This approach has an exponential time complexity and is not practical for large input sizes.  Time Complexity: O(2^n)	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/House%20Robber.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/House%20Robber.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/House%20Robber.java	https://youtu.be/IzpJnd48bRY
Algorithms Pack	Dynamic Programming (General)	Best Time To Buy And Sell Stock With Cooldown	https://leetcode.com/problems/best-time-to-buy-and-sell-stock-with-cooldown/	You are given an array prices where prices[i] is the price of a given stock on the ith day.    Find the maximum profit you can achieve. You may complete as many transactions as you like (i.e., buy one and sell one share of the stock multiple times) with the following restrictions:    After you sell your stock, you cannot buy stock on the next day (i.e., cooldown one day).  Note: You may not engage in multiple transactions simultaneously (i.e., you must sell the stock before you buy again).	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Stack	Divide and Conquer	Greedy	Dynamic Programming (State Machine + Memoization)	D	Not quite.  This approach makes it very tough to consider the cooldown period, and hence, it can lead to suboptimal solutions.  We can use a stack to keep track of the local minimum and maximum points, and calculate the profit by adding the difference between the local maximum and minimum points.	No. This approach will fail to consider the possibility of multiple transactions within a single half of the array. It will only consider transactions that span both halves, leading to suboptimal profits.	Suboptimal. This approach does not always yield the optimal solution, as it can miss out on potential profits if a local minimum is followed by a higher peak.	Correct!  We use dynamic programming to find the maximum profit that can be achieved at any day with one of the three states: doing nothing/buying a stock/selling a stock. We keep track of these states along with the index of the day, and use memoization to avoid recalculating the same state twice.  We start with index 0, and make decisions based on whether we buy, sell, or do nothing on this day. We then move to the next day and repeat the process. The base case is when we have reached the end of the prices array.  This approach is optimal because it examines all possible states and returns the maximum profit that can be achieved with the given prices. It does this by memoizing the intermediate states to avoid recalculating the same state twice.  Time Complexity: O(n), where n is the length of the prices array.  Memory Complexity: O(1), with constant memory used regardless of the size of the input.	The brute force here? We can enumerate all possible combinations of buy and sell transactions and find the maximum profit among them.  Why it's suboptimal: This approach has an exponential time complexity and is impractical for large inputs.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Best%20Time%20to%20Buy%20and%20Sell%20Stock%20with%20Cooldown.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Best%20Time%20to%20Buy%20and%20Sell%20Stock%20with%20Cooldown.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Best%20Time%20to%20Buy%20and%20Sell%20Stock%20with%20Cooldown.java	https://youtu.be/Y-nIPGHhTzE
Algorithms Pack	Dynamic Programming (General)	Minimum Number of Removals To Make Mountain Array	https://leetcode.com/problems/minimum-number-of-removals-to-make-mountain-array/	You may recall that an array arr is a mountain array if and only if:    arr.length >= 3  There exists some index i (0-indexed) with 0 < i < arr.length - 1 such that:  arr[0] < arr[1] < ... < arr[i - 1] < arr[i]  arr[i] > arr[i + 1] > ... > arr[arr.length - 1]  Given an integer array nums​​​, return the minimum number of elements to remove to make nums​​​ a mountain array.	O(n^2)	O(n)	We're expecting an optimized solution of time complexity O(n^2) and memory complexity O(n).	Greedy	Divide and Conquer	Dynamic Programming	Linear Scan	C	Incorrect. This approach assumes that removing the largest elements from the input array will always result in a mountain array. However, this is not always true, as removing a large element could break the mountain structure of the array. Moreover, it can miss mountain arrays that are obtained by removing smaller elements.	No. This approach assumes that the largest mountain array in the input array can be obtained by merging two smaller mountain arrays. However, this is not always true, as the largest mountain array could be entirely contained within one half of the input array. Moreover, it requires O(n^2) time to merge two mountain arrays of size n, so the overall time complexity would be O(n^3).	Correct! (This can be optimized with binary search for an O(n log(n))) approach too!)  The given solution uses dynamic programming (DP) to solve the problem. Specifically, it uses the Longest Increasing Subsequence (LIS) and Longest Decreasing Subsequence (LDS) subproblems to calculate the length of the longest possible mountain array that could be formed if the current element is chosen as the peak element.  The LIS and LDS subproblems are solved using a memoization technique where the previously computed result for an index is saved and reused when the same index is encountered later. This optimization significantly reduces the time complexity of the algorithm.  The algorithm computes the length of the longest possible mountain array for each element in the input array and chooses the maximum among them. Finally, it returns the number of elements that need to be removed to make the input array a mountain array, which is equal to the difference between the length of the input array and the length of the longest possible mountain array.  Time Complexity: O(n^2): O(n^2), where n is the size of the input array. The reason for this is that both the LIS and LDS subproblems need to be solved for each element in the array. Since each subproblem can take O(n) time, the overall time complexity is O(n^2).  Memory Complexity: O(n), where n is the size of the input array. This is because the memoization tables for LIS and LDS require O(n) memory, and the input array is also stored in memory.	Simply, not a good approach here!	The brute-force approach here is to check all possible mountain arrays. We can generate all possible sub-arrays of the input array with length 3 or greater and check if each sub-array is a mountain array, returning the smallest number of elements to remove to make any of the mountain arrays found.  Why it's suboptimal: The time complexity of this approach is O(2^n * n^2), where n is the size of the input array. This is because there are 2^n possible sub-arrays to check, and checking each sub-array takes O(n^2) time (for the LIS and LDS subproblems).	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Minimum%20Number%20of%20Removals%20to%20Make%20Mountain%20Array.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Minimum%20Number%20of%20Removals%20to%20Make%20Mountain%20Array.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Minimum%20Number%20of%20Removals%20to%20Make%20Mountain%20Array.java	https://youtu.be/OkKvC93tyvI
Algorithms Pack	Dynamic Programming (Enumeration)	Edit Distance	https://leetcode.com/problems/edit-distance/	Given two strings word1 and word2, return the minimum number of operations required to convert word1 to word2.    You have the following three operations permitted on a word:    Insert a character  Delete a character  Replace a character.	O(n^2)	O(n^2)	We're expecting an optimized solution of time complexity O(n^2) and memory complexity O(n^2).	A* Algorithm	Greedy	Backtracking	Levenshtein Distance Algorithm	D	Too complex. The quality of the solution depends heavily on the quality of the heuristic function used, and finding a good heuristic function for this problem is difficult. Additionally, implementing the algorithm can be complex and requires a lot of memory.	Incorrect! The approach does not consider the global optimum and can lead to a suboptimal solution. For example, it may choose to delete a character that should have been replaced, leading to a higher number of operations.	Backtracking would be a huge waste of time in this problem - in all senses of the term.	Correct! (There are more optimal DP approaches, but felt this important to include).  The Levenshtein distance algorithm is a dynamic programming algorithm that computes the minimum number of operations (insertion, deletion, or replacement) required to transform one string into another. In this solution, we initialize a 2D array memory to store the results of subproblems, where memory[i][j] represents the minimum number of operations required to transform the substring str1[0:i] to the substring str2[0:j]. We recursively compute memory[i][j] by comparing the i-th character of str1 with the j-th character of str2 and choosing the minimum of three possible operations: insert a character, delete a character, or replace a character.	The brute-force here is tough! We would generate all possible ways to transform word1 into word2 by performing every combination of insertions, deletions, and replacements, and return the minimum number of operations required.  Why it's suboptimal: This approach generates an enormous number of combinations to check, making it very inefficient, especially for long strings.  Time Complexity: O(3^max(len(word1), len(word2)))	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Edit%20Distance.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Edit%20Distance.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Edit%20Distance.java	https://youtu.be/mkrc6PsTSMc
Algorithms Pack	Dynamic Programming (Enumeration)	Integer Break	https://leetcode.com/problems/integer-break/	Given an integer n, break it into the sum of k positive integers, where k >= 2, and maximize the product of those integers.    Return the maximum product you can get.	O(n^2)	O(n)	We're expecting an optimized solution of time complexity O(n^2) and memory complexity O(n).	Dynamic Programming (Memoization)	Backtracking	Greedy	Simple Recursion	A	Correct!  We use dynamic programming to store the maximum product of a number as the sum of k positive integers up to n. We use memoization to store the result of previously computed sub-problems to avoid recomputing them.  Time Complexity: O(n^2), where n is the given integer.  Memory Complexity: O(n), where n is the given integer.	The number of possible ways of breaking n into k positive integers is exponential, so this approach becomes impractical even for small values of n.	This approach doesn't always yield the optimal solution. For example, if n=10 and k=3, this approach would give the solution 433=36, but the optimal solution is 22222=32.	A simple recursive approach won't work as well here!	The brute-force approach here is to try all possible ways of breaking the integer n into k positive integers and calculate the product of each way. Return the maximum product.  Why it's suboptimal: The number of possible ways of breaking n into k positive integers is exponential, so this approach becomes impractical even for small values of n.  Time Complexity: O(n^k)	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Integer%20Break.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Integer%20Break.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Integer%20Break.java	https://youtu.be/clWtHkypq4g
Algorithms Pack	Dynamic Programming (Enumeration)	Min Cost Climbing Stairs	https://leetcode.com/problems/min-cost-climbing-stairs/	You are given an integer array cost where cost[i] is the cost of ith step on a staircase. Once you pay the cost, you can either climb one or two steps.    You can either start from the step with index 0, or the step with index 1.    Return the minimum cost to reach the top of the floor.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Greedy	Dynamic Programming (Memoization)	Bubble Sort	Binary Search	B	Nope. This approach does not consider the overall cost of the path and may lead to suboptimal results. For example, consider the input array [10, 15, 20]. The greedy approach would choose 10, then 15, and then finish with a cost of 25, whereas the optimal solution is to choose 15, then 20, with a total cost of 35.	Correct!  We use memoization to solve this problem recursively. We start from the first or second step and compute the cost of climbing to the top of the floor by taking either one or two steps at a time. At each step, we choose the path with minimum cost. We use an array to store the minimum cost required to reach the top of the floor from a particular index.  Memoization reduces the time complexity of the algorithm from an exponential time complexity of brute force to a polynomial time complexity.  Time Complexity: O(n)  Memory Complexity: O(n)	This would be highly inefficient if employed as part of a solution.	Problematic! We could attempt to use binary search to find the minimum cost to reach the top of the floor. We could first sort the cost array in non-decreasing order and then perform binary search on the sorted array to find the minimum cost. However, this approach would not work because the minimum cost may not necessarily correspond to a sorted index. For example, if the cost array is [10, 15, 20, 1, 5, 10], the minimum cost to reach the top is 11, but binary search would not be able to find this solution.	Here, with the brute-force, we can try all possible combinations of climbing one or two steps at each index, and return the minimum cost to reach the top.  Why it's suboptimal: This approach has an exponential time complexity since it involves trying all possible combinations. As the size of the input array increases, the time complexity will grow very quickly and become infeasible.  Time Complexity: O(2^n)	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Min%20Cost%20Climbing%20Stairs.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Min%20Cost%20Climbing%20Stairs.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Min%20Cost%20Climbing%20Stairs.java	https://youtu.be/3sBTJ_Etn9o
Algorithms Pack	Dynamic Programming (Enumeration)	Perfect Squares	https://leetcode.com/problems/perfect-squares/	Given an integer n, return the least number of perfect square numbers that sum to n.    A perfect square is an integer that is the square of an integer; in other words, it is the product of some integer with itself. For example, 1, 4, 9, and 16 are perfect squares while 3 and 11 are not.	O(n * sqrt(n))	O(n)	We're expecting an optimized solution of time complexity O(n * sqrt(n)) and memory complexity O(n).	Greedy	BFS	Recursive Approach + Memoization	Simple Brute Force	C	Unfortunately, this approach is suboptimal because it does not always produce the correct result. There are cases where a combination of smaller perfect squares would result in a smaller number of squares than using a single large perfect square.	This approach is suboptimal because it can be very memory-intensive for large values of n, since we need to store all possible nodes in the graph. Additionally, finding the shortest path using breadth-first search can be very time-consuming for large values of n.	Correct!  Our approach is to use recursion with memoization to solve the problem. We start by checking if n is less than or equal to 0. If so, we return 0 because there are no perfect squares that sum up to a negative number or zero. Next, we check if we have already computed the answer for the given value of n. If so, we return the previously computed result. Otherwise, we iterate over all possible perfect squares (i.e., i*i) less than or equal to n and compute the minimum number of perfect squares required to sum up to n.  Our approach utilizes memoization, which stores previously computed results and avoids unnecessary recomputation. As a result, the time complexity is greatly reduced. Furthermore, since we are checking all possible perfect squares less than or equal to n, we can be sure that we are considering all possible combinations of perfect squares that sum up to n.  Time Complexity: O(n*sqrt(n))  Memory Complexity: O(n)	This is almost never the answer!	Our brute-force approach here could try all possible combinations of perfect squares that sum up to n.  Why it's suboptimal: This approach is suboptimal because it involves trying all possible combinations of perfect squares, which leads to an exponential time complexity. The time complexity for this approach would be O(2^n).	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Perfect%20Squares.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Perfect%20Squares.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Perfect%20Squares.java	https://youtu.be/h2M-U_mrGjA
Algorithms Pack	Dynamic Programming (Range Patterns)	Longest Palindromic Substring	https://leetcode.com/problems/longest-palindromic-substring/	Given a string s, return the longest palindromic substring in s.	O(n^2)	O(n^2)	We're expecting an optimized solution of time complexity O(n^2) and memory complexity O(n^2).	Union-Find	Sliding Window	Dynamic Programming	Greedy	C	Try a different approach!	It's very difficult to use this to find the longest palindromic substring.	Correct!  Dynamic Programming is by no means the most optimal approach here, but it is an improvement on the alternatives.  Time Complexity: O(n^2)  Memory Complexity: O(n^2)	The greedy algorithm is not going to be useful for this problem.	The brute-force approach here is to generate all possible substrings of the given string, and check each substring to see if it is a palindrome. Keep track of the longest palindrome found.  Why it's suboptimal: This approach has time complexity of O(n^3), where n is the length of the string. Generating all possible substrings alone has O(n^2) complexity, and checking each one for palindromic property requires additional O(n) time in the worst case. This makes the approach very inefficient for longer strings.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Longest%20Palindromic%20Substring.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Longest%20Palindromic%20Substring.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Longest%20Palindromic%20Substring.java	https://youtu.be/XG3GWGBZECQ
Algorithms Pack	Dynamic Programming (Range Patterns)	Filling Bookcase Shelves	https://leetcode.com/problems/filling-bookcase-shelves/	You are given an array books where books[i] = [thicknessi, heighti] indicates the thickness and height of the ith book. You are also given an integer shelfWidth.    We want to place these books in order onto bookcase shelves that have a total width shelfWidth.    We choose some of the books to place on this shelf such that the sum of their thickness is less than or equal to shelfWidth, then build another level of the shelf of the bookcase so that the total height of the bookcase has increased by the maximum height of the books we just put down. We repeat this process until there are no more books to place.    Note that at each step of the above process, the order of the books we place is the same order as the given sequence of books.    For example, if we have an ordered list of 5 books, we might place the first and second book onto the first shelf, the third book on the second shelf, and the fourth and fifth book on the last shelf.  Return the minimum possible height that the total bookshelf can be after placing shelves in this manner.	O(n^2)	O(n)	We're expecting an optimized solution of time complexity O(n^2) and memory complexity O(n).	Greedy	Dynamic Programming (Memoization)	Backtracking	BFS	B	This won't work well. In this approach, we always choose the book with the maximum height first, and then fill the rest of the shelf with as many books as possible until we reach the shelf width limit. This approach is suboptimal because it does not consider the order of the books and may not result in the minimum possible total height. For example, if we have a tall book followed by several short books that together exceed the shelf width, this approach would place the tall book alone on a shelf and waste space.	Correct!  This approach uses dynamic programming to solve the problem. Specifically, we use a top-down approach with memoization. We define a function arrange that takes an index as input and returns the minimum possible height of the bookshelf if we start placing books from that index.  In the function arrange, we start by checking if we have already computed the result for the current index. If so, we return the cached value. Otherwise, we initialize ret to a large value, width to 0, and mxHeight to 0. Then, we iterate over all possible splits (i.e., indices) from the current index to the end of the array. For each split, we add the thickness of the book to width and update mxHeight to be the maximum height of the books we have considered so far. If the width exceeds the shelf width, we break out of the loop since we cannot add any more books to the current shelf. Otherwise, we compute the minimum possible height of the remaining books (i.e., the books that come after the current split) recursively by calling the arrange function with split+1 as the argument. We add the maximum height of the books we just placed to the result of the recursive call, and update ret to be the minimum value we have seen so far.  At the end of the arrange function, we return ret, which represents the minimum possible height of the bookshelf if we start placing books from the current index.  The main function minHeightShelves initializes the books and shelfWidth variables, and then calls the arrange function with an index of 0. It also initializes the memory array to -1 before calling arrange.  Time Complexity: O(n^2), where n is the number of books.  Memory Complexity: O(n), where n is the number of books.	Highly suboptimal. In this approach, we start with an empty shelf and recursively try to place each book on the shelf, updating the minimum possible total height as we go. This approach is suboptimal because it tries every possible combination of books on each shelf, which has an exponential time complexity. It is similar to the Brute-Force approach but with more overhead since we would have to undo previous placements if they do not lead to a minimum possible total height.	No. This approach is not guaranteed to terminate or find the optimal solution because the graph can be infinitely large if the shelf width is too small or if the books have very small thicknesses.	Brute-Force Approach:  We could try all possible combinations of books on each shelf, and then check the total height of each shelf to see which one is the smallest. This approach is suboptimal because it has an exponential time complexity. For example, if we have n books, we could have up to 2^n possible combinations of books on each shelf.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Longest%20Palindromic%20Substring.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Longest%20Palindromic%20Substring.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Longest%20Palindromic%20Substring.java	https://youtu.be/KyVm1U2npCc
Algorithms Pack	Dynamic Programming (Range Patterns)	Partition Array for Maximum Sum	https://leetcode.com/problems/partition-array-for-maximum-sum/	Given an integer array arr, partition the array into (contiguous) subarrays of length at most k. After partitioning, each subarray has their values changed to become the maximum value of that subarray.    Return the largest sum of the given array after partitioning. Test cases are generated so that the answer fits in a 32-bit integer.	O(nk)	O(n)	We're expecting an optimized solution of time complexity O(nk) and memory complexity O(n).	Recursive Memoization	Greedy	Sliding Window	Binary Search	A	Correct!  In this approach, we use a recursive function to solve the problem. The idea is to start at the beginning of the array and partition the array into subarrays of length at most k. We use memoization to store the results of the subproblems to avoid recomputing them.  We define our recursive function partition() to take the starting index of the current subarray as a parameter. At each recursive call, we loop over the next k elements of the array and determine the maximum value of the subarray. We then calculate the sum of the subarray multiplied by its maximum value and add it to the result of partitioning the remaining elements of the array. We take the maximum of all such values to get the final answer.  Time Complexity: O(nk), where n is the length of the array and k is the maximum length of the subarrays.  Memory Complexity: O(n), where n is the length of the array, due to the use of memoization.	This approach does not always lead to the optimal solution. In some cases, it may partition elements that should be left together, resulting in a suboptimal sum.	This approach does not always lead to the optimal solution. In some cases, it may partition elements that should be left together, resulting in a suboptimal sum.	This approach is incorrect, as there can be multiple ways of partitioning the array to achieve the same maximum sum, and binary search may not be able to find all of them.	The brute-force approach would generate all possible partitions of the array and calculate the sum for each partition, and return the maximum sum.  Why it's suboptimal: This approach has an exponential time complexity and is therefore very inefficient, especially for large arrays.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Partition%20Array%20for%20Maximum%20Sum.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Partition%20Array%20for%20Maximum%20Sum.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Partition%20Array%20for%20Maximum%20Sum.java	https://youtu.be/x7E0Om0AttA
Algorithms Pack	Dynamic Programming (Range Patterns)	Minimum Insertion Steps to Make a String Palindromic	https://leetcode.com/problems/minimum-insertion-steps-to-make-a-string-palindrome/	Given a string s. In one step you can insert any character at any index of the string.    Return the minimum number of steps to make s palindrome.    A Palindrome String is one that reads the same backward as well as forward.	O(n^2)	O(n^2)	We're expecting an optimized solution of time complexity O(n^2) and memory complexity O(n^2).	Greedy	Sorting	Divide And Conquer	Dynamic Programming (Memoization)	D	This approach does not always find the optimal solution. For example, consider the string 'abcbda'. The greedy approach would insert a 'd' on the left side resulting in 'adbcbda', but the optimal solution is to insert a 'b' on the right side resulting in 'abcbdcbda'.	This approach is incorrect because it doesn't account for the characters that are missing from the string.	This approach would not work well with this problem!	Correct!  Here, we use dynamic programming to solve the problem. We create a 2D array to store the results of all possible substrings of the string. We use memoization to avoid recomputing the results of already computed substrings. We check if the first and the last character of the substring are the same. If yes, then we can ignore these two characters and find the minimum steps required for the remaining substring. If not, then we have two options: either insert a character at the start of the substring or at the end of the substring. We choose the option which results in the minimum number of steps required to make the substring a palindrome.  Time Complexity: O(n^2), where n is the length of the input string.  Memory Complexity: O(n^2), where n is the length of the input string.	The brute-force approach here would be to generate all possible ways to make s a palindrome by inserting characters at any index, and then calculate the number of steps required for each one to be a palindrome. Return the minimum number of steps needed to make s a palindrome.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Minimum%20Insertion%20Steps%20to%20Make%20a%20String%20Palindrome.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Minimum%20Insertion%20Steps%20to%20Make%20a%20String%20Palindrome.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Minimum%20Insertion%20Steps%20to%20Make%20a%20String%20Palindrome.java	https://youtu.be/_dXrE5WLo0I
Algorithms Pack	Dynamic Programming (Range Patterns)	Minimum Cost To Cut a Stick	https://leetcode.com/problems/minimum-cost-to-cut-a-stick/	Given a wooden stick of length n units. The stick is labelled from 0 to n. For example, a stick of length 6 is labelled as follows:      Given an integer array cuts where cuts[i] denotes a position you should perform a cut at.    You should perform the cuts in order, you can change the order of the cuts as you wish.    The cost of one cut is the length of the stick to be cut, the total cost is the sum of costs of all cuts. When you cut a stick, it will be split into two smaller sticks (i.e. the sum of their lengths is the length of the stick before the cut). Please refer to the first example for a better explanation.    Return the minimum total cost of the cuts.	O(n^3)	O(n^2)	We're expecting an optimized solution of time complexity O(n^3) and memory complexity O(n^2).	Dynamic Programming	Binary Search	Greedy	Divide And Conquer	A	Correct!  We use dynamic programming to solve this problem. We store the result of the previous subproblems in a 2D array named 'memory'. We define a function named 'cost' that calculates the minimum total cost of the cuts for a specific stick interval (start to end). In each step, we iterate over all possible positions to cut the stick (between start and end). We calculate the cost of the left and right segments and add the length of the stick to the total cost. We take the minimum of all possible costs and store it in the memory array. We return the value from the memory array at the end.  Time Complexity: O(n^3) where n is the size of the cuts vector.  Memory Complexity: O(n^2) where n is the size of the cuts vector.	This approach does not always result in the optimal solution as making the cuts at the midpoints of the remaining sticks does not necessarily minimize the overall cost.	This approach does not always result in the optimal solution as making the longest cut at each step does not necessarily minimize the overall cost.	This approach can result in a suboptimal solution as making the longest cut at each step does not necessarily minimize the overall cost.	The brute-force here is to try all possible combinations of cuts and calculate the cost of each combination. Return the minimum cost.  Why it's suboptimal: This approach has an exponential time complexity as the number of possible combinations grows exponentially with the number of cuts. Therefore, it is not feasible for large input sizes.  Time Complexity: O(2^n)	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Minimum%20Cost%20to%20Cut%20a%20Stick.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Minimum%20Cost%20to%20Cut%20a%20Stick.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Minimum%20Cost%20to%20Cut%20a%20Stick.java	https://youtu.be/zqZO-wCwsIc
Algorithms Pack	Dynamic Programming (Range Patterns)	Burst Balloons	https://leetcode.com/problems/burst-balloons/	You are given n balloons, indexed from 0 to n - 1. Each balloon is painted with a number on it represented by an array nums. You are asked to burst all the balloons.    If you burst the ith balloon, you will get nums[i - 1] * nums[i] * nums[i + 1] coins. If i - 1 or i + 1 goes out of bounds of the array, then treat it as if there is a balloon with a 1 painted on it.    Return the maximum coins you can collect by bursting the balloons wisely.	O(n^3)	O(n^2)	We're expecting an optimized solution of time complexity O(n^3) and memory complexity O(n^2).	Dynamic Programming	Greedy	Divide And Conquer	Hash Table + Linear Scan	A	Correct!  We can use a dynamic programming approach to solve this problem. The idea is to use the recursive formula to find the maximum coins we can get by bursting a subarray of balloons from start to end. To avoid recomputing the same subproblems, we can use memoization to store the results of already computed subproblems in a two-dimensional array. At each recursive step, we try every possible balloon to be the last one to burst in the subarray, and we compute the maximum coins we can get in this case.  Time Complexity: O(n^3) because we are computing the value of each subproblem once, and there are n^2 subproblems, and each subproblem takes O(n) time to compute.  Memory Complexity: O(n^2) because we are using a two-dimensional array to store the results of already computed subproblems, and there are n^2 subproblems.	Unfortunately, the greedy approach does not necessarily lead to the optimal solution because it only considers the current state and does not take into account the impact of its current decision on future states. In other words, the greedy approach is myopic and does not consider the long-term consequences of its actions.	Unfortunately, the divide and conquer approach is suboptimal because the optimal solution may involve bursting balloons from both subarrays simultaneously, which is not captured by this approach. In other words, the subproblems are not independent, and the optimal solution may involve a combination of solutions from the subproblems.	Quite simply, no. Dynamic Programming is the key to solving this very difficult problem.	The brute-force approach here involves generating all possible orderings of balloons and calculating the maximum coins that can be obtained by bursting them.  Why it's suboptimal: This approach has an exponential time complexity because the number of possible orderings is n factorial (n!). Therefore, it is impractical for even small inputs.  Time Complexity: O(n!)	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Burst%20Balloons.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Burst%20Balloons.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Burst%20Balloons.java	https://youtu.be/TRECp1OS4rU
Algorithms Pack	Dynamic Programming (Counting)	Decode Ways	https://leetcode.com/problems/decode-ways/	A message containing letters from A-Z can be encoded into numbers using the following mapping:    'A' -> '1'  'B' -> '2'  ...  'Z' -> '26'  To decode an encoded message, all the digits must be grouped then mapped back into letters using the reverse of the mapping above (there may be multiple ways). For example, '11106' can be mapped into:    'AAJF' with the grouping (1 1 10 6)  'KJF' with the grouping (11 10 6)  Note that the grouping (1 11 06) is invalid because '06' cannot be mapped into 'F' since '6' is different from '06'.    Given a string s containing only digits, return the number of ways to decode it.    The test cases are generated so that the answer fits in a 32-bit integer.	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	Recursive Approach + Backtracking	Recursive Approach + Memoization	Greedy + Memoization	Regex + Memoization	B	This is the brute-force approach, so is not recommended. The time complexity is very high since we're exploring all possible combinations, which is exponential in the worst-case scenario. Also, we're doing a lot of redundant work since we're exploring the same combinations multiple times.	Recursive with Memoization  Brief explanation of approach: We start at the beginning of the string and recursively count the number of ways to decode the string. We use memoization to avoid redundant calculations. At each index, we consider taking either a single digit or a two-digit number if it's valid.  Time Complexity: O(n), where n is the length of the input string. This is because each index is visited only once and we do constant time work per index.  Time: O(n)  Memory: O(1) - the cache only needs to look at DP[n-1] && DP[n-2]. The size of our cache is n, so our time and memory is ordinarily O(n)	No. It doesn't always yield the correct result since the optimal grouping for a specific digit depends on the previous grouping. Also, there are cases where a greedy approach might result in a valid grouping but not the optimal	This is not quite the best approach for this problem. We could convert the string of digits into a regular expression pattern that represents all possible valid combinations of grouping digits together and maps them to letters - and then count the number of matches for the pattern in the string.  Why it's suboptimal: Although it's a different approach, it's still exploring all possible combinations, which is unnecessary since we only need to count the valid ones. Also, it's more complex and harder to implement compared to other approaches.	Our brute-force solution generates all possible ways of decoding the string by exploring all combinations of grouping digits together and mapping them to letters. We check that each combination is valid, and return the count of valid combinations at the end.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Decode%20Ways.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Decode%20Ways.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Decode%20Ways.java	https://youtu.be/tEUYx6whbGQ
Algorithms Pack	Dynamic Programming (Counting)	Coin Change 2	https://leetcode.com/problems/coin-change-2/	You are given an integer array coins representing coins of different denominations and an integer amount representing a total amount of money.    Return the number of combinations that make up that amount. If that amount of money cannot be made up by any combination of the coins, return 0.    You may assume that you have an infinite number of each kind of coin.    The answer is guaranteed to fit into a signed 32-bit integer.	O(nm)	O(m)	We're expecting an optimized solution of time complexity O(nm) and memory complexity O(m).	Backtracking	Greedy	Dynamic Programming	Recursion	C	Incorrect here. Backtracking can be very slow and impractical for larger inputs, and it doesn't take advantage of the fact that we can use the same coin multiple times.  Time Complexity: O(2^n)	No. Greedy algorithms don't always yield the optimal solution, and in this case, it may skip over smaller coin denominations that are necessary to reach the target amount.	The approach involves initializing a 1D array with size (amount+1), where the element at index i represents the number of ways to form the amount i using the given coins. We then iterate over each coin in the input vector and for each coin, we iterate over all possible targets (amounts) starting from 0 up to the maximum amount. We update the number of ways to form the current target by adding the number of ways to form (target - coin value) to the current number of ways to form the target. Finally, we return the value at the index equal to the given amount.  Time Complexity: O(n*m), where n is the size of the coins vector and m is the amount given.  Memory Complexity: O(m), where m is the amount given.	This is identical to the brute-force solution, and is not ideal for this problem at all.	One simple brute-force solution would be to recursively check all possible combinations of coins to reach the target amount.  Why it's suboptimal: The time complexity is exponential, which makes it very slow and impractical for larger inputs.  Time Complexity: O(2^n).	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Coin%20Change%20II.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Coin%20Change%20II.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Coin%20Change%20II.java	https://youtu.be/ZlGqXKXhuYY
Algorithms Pack	Dynamic Programming (Counting)	Combination Sum IV	https://leetcode.com/problems/combination-sum-iv/	Given an array of distinct integers nums and a target integer target, return the number of possible combinations that add up to target.    The test cases are generated so that the answer can fit in a 32-bit integer.	O(nT)	O(T)	We're expecting an optimized solution of time complexity O(nT) and memory complexity O(T), where n is the size of the vector, and T the size of the target.	Greedy	Dynamic Programming	Backtracking	Two Pointers	B	Suboptimal, because it may not find all possible combinations that add up to the target value. For example, if the nums vector is [1,2,3] and the target is 4, this approach will return only one combination (3,1), but there is another combination (2,2) that adds up to 4.	Correct!  In our approach, we are using dynamic programming with a top-down approach. We are defining a function called combi that takes a target value as input and returns the number of possible combinations that add up to that target.  We are using memoization to avoid recomputing the same target values. We are using an array called memory to store the results of previously computed targets. Initially, all values in memory are set to -1.  In the combi function, we are checking if the target is less than zero, in which case we return 0 since there are no possible combinations. If the target is 0, we return 1 since there is only one possible combination, i.e., an empty combination.  If the result of combi for the given target has already been computed, we return that value from memory.  Otherwise, we iterate through all the numbers in the given nums vector, and for each number, we recursively call the combi function with the target value reduced by that number. We add up all the returned values from these recursive calls, and store the sum in memory for future use.  Finally, we return the value of combi(target).  Time Complexity: O(target * n), where n is the size of the nums vector. This is because we are computing the combi function for all values from 0 to target and for each value, we are iterating through all the elements in nums.  Memory Complexity: O(target), which is the size of the memory array we are using to store previously computed values.	Not the best here. This approach can be very time-consuming, especially if the nums vector has many elements, because it generates all possible combinations, even if they cannot add up to the target value.	Incorrect! This approach may miss some possible combinations because it only considers combinations that include the first and last elements of the sorted vector. For example, if the nums vector is [1,2,3,4] and the target is 5, this approach will miss the combination (2,3).	The brute-force approach here is to use recursion. We generate all possible combinations of the given nums vector and count the ones that add up to the target value.  Why it's suboptimal: This approach is suboptimal because it generates all possible combinations, even if they cannot add up to the target value. It can be very time-consuming, especially if the nums vector has many elements.  Time Complexity: O(2^n), where n is the size of the nums vector.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Combination%20Sum%20IV.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Combination%20Sum%20IV.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Combination%20Sum%20IV.java	https://youtu.be/WbdzV_0lsaw
Algorithms Pack	Dynamic Programming (Counting)	Number of Dice Rolls With Target Sum	https://leetcode.com/problems/number-of-dice-rolls-with-target-sum/	Given an array of distinct integers nums and a target integer target, return the number of possible combinations that add up to target.    The test cases are generated so that the answer can fit in a 32-bit integer.	O(nT)	O(nT)	We're expecting an optimized solution of time complexity O(nT) and memory complexity O(nT).	Recursive Approach + Memoization	Backtracking	Greedy	Dynamic Programming (Memoization)	D	Not quite. Although this approach can speed up the computation significantly, it still has an exponential time complexity in the worst case, and it requires a lot of memory to store the memoization table.	This approach is slow and can produce duplicate combinations, leading to incorrect results.	This approach can produce incorrect results, since the maximum value may not always be the best choice, and there may not be a combination of rolls that can add up to the target sum.	Correct!  In this approach, we use dynamic programming to calculate the number of ways to get a target sum with n dice and k faces. We use memoization to avoid recalculating values that we have already computed. We start with the base case where we have zero dice and a target of zero, which is equal to one. We then recursively calculate the number of ways to get a target sum by rolling the dice one by one and summing up the face-up numbers until we reach our target sum. We store the computed values in a 2D array to avoid recomputing them later.  Time Complexity: O(n * target), where n is the number of dice, and target is the target sum.  Memory Complexity: O(n * target), where n is the number of dice, and target is the target sum. This is because we have to store the computed values in a 2D array.	The brute-force here is to try all possible combinations of rolling n dice with k faces and count the number of combinations that add up to target.  Why it's inefficient: The number of combinations is k^n, which grows exponentially with the number of dice, making it impractical for large values of n.  Time Complexity: O(k^n)	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Number%20of%20Dice%20Rolls%20With%20Target%20Sum.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Number%20of%20Dice%20Rolls%20With%20Target%20Sum.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Number%20of%20Dice%20Rolls%20With%20Target%20Sum.java	https://youtu.be/oD0k4mw0sjI
Algorithms Pack	Dynamic Programming (On Grid)	Unique Paths II	https://leetcode.com/problems/unique-paths-ii/	You are given an m x n integer array grid. There is a robot initially located at the top-left corner (i.e., grid[0][0]). The robot tries to move to the bottom-right corner (i.e., grid[m - 1][n - 1]). The robot can only move either down or right at any point in time.    An obstacle and space are marked as 1 or 0 respectively in grid. A path that the robot takes cannot include any square that is an obstacle.    Return the number of possible unique paths that the robot can take to reach the bottom-right corner.    The testcases are generated so that the answer will be less than or equal to 2 * 10e9.	O(mn)	O(mn)	We're expecting an optimized solution of time complexity O(mn) and memory complexity O(mn).	DFS	Simple Recursion	Greedy	Recursion + Memoization	D	DFS isn't ideal here, as it can still generate an exponential number of paths, and the use of a stack can cause stack overflow for large inputs. It has a time complexity of O(2^(m+n)), but the use of memoization can improve its performance for certain inputs.	This is simply the brute-force approach, and is not ideal here.	This approach is incorrect as it does not consider all possible paths and can miss some valid paths.	Correct!  In our approach, we first check if the robot is at the bottom-right corner or not. If it is, we return 1, as the robot has reached its destination. Else, we check if the current cell is an obstacle or not. If it is, we return 0 as the robot cannot move to that cell. We then check if we have already calculated the number of ways to reach the current cell before. If yes, we return the stored result. Otherwise, we recursively calculate the number of ways to reach the cell by moving either down or right. We store this result for later use.  We start our approach by calling the uniquePathsWithObstacles function, which initializes the memoization array and calls the countWays function to calculate the number of ways to reach the bottom-right corner.  Time Complexity: O(m * n), where m and n are the dimensions of the input grid.  Memory Complexity: O(m * n), where m and n are the dimensions of the input grid.	The brute-force approach is to generate all possible paths from the top-left corner to the bottom-right corner, avoiding obstacles, and count the valid ones.  Why it's suboptimal: This approach generates an exponential number of paths, making it very inefficient, especially for large inputs. It has a time complexity of O(2^(m+n)), where m and n are the dimensions of the input grid.  Time Complexity: O(2^(m+n)).	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Unique%20Paths%20II.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Unique%20Paths%20II.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Unique%20Paths%20II.java	https://youtu.be/Re8oT7ft3X8
Algorithms Pack	Dynamic Programming (On Grid)	Minimum Falling Path Sum	https://leetcode.com/problems/minimum-falling-path-sum/	Given an n x n array of integers matrix, return the minimum sum of any falling path through matrix.    A falling path starts at any element in the first row and chooses the element in the next row that is either directly below or diagonally left/right. Specifically, the next element from position (row, col) will be (row + 1, col - 1), (row + 1, col), or (row + 1, col + 1).	O(n^2)	O(n^2)	We're expecting an optimized solution of time complexity O(n^2) and memory complexity O(n^2).	Greedy	DFS	Recursion + Memoization	Backtracking	C	Not used here. The greedy approach does not always lead to the optimal solution, and can easily get stuck in a local minimum. It can also miss potential paths with smaller sums.	Not ideal compared to recursion + memoization. DFS can easily get stuck in a local minimum, and can be very slow for large matrices. It can also explore many unnecessary paths.	Correct!  Our solution uses memoization with recursion. We recursively compute the minimum sum of falling paths through each element of the first row, and memoize the results to avoid redundant computation. We compute the minimum sum of each falling path by considering the element directly below, and the two elements diagonally below it. We continue this process until we reach the last row, where we simply return the element. Finally, we return the minimum sum of all possible falling paths through the first row.  Time Complexity: O(n^2), where n is the size of the matrix. Each element of the matrix is visited only once, and for each element, we need to compute the minimum sum of its falling paths, which involves computing the minimum of three values.  Memory Complexity: O(n^2), where n is the size of the matrix. We use a two-dimensional memoization array of size n x n to store the computed results.	Backtracking is suboptimal because it requires generating and calculating the sum of all possible paths, leading to an exponential time complexity.	The brute-force here is to try all possible falling paths starting from each element in the first row, calculate the sum of each path, and return the minimum sum.  Why it's suboptimal: The number of falling paths is exponential, and trying all of them would result in a very high time complexity. This approach is only feasible for small input sizes.  Time Complexity: O(n^3 * 2^n), where n is the size of the matrix.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Minimum%20Falling%20Path%20Sum.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Minimum%20Falling%20Path%20Sum.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Minimum%20Falling%20Path%20Sum.java	https://youtu.be/PQoeU_6qitc
Algorithms Pack	Dynamic Programming (On Grid)	Out of Boundary Paths	https://leetcode.com/problems/out-of-boundary-paths/	There is an m x n grid with a ball. The ball is initially at the position [startRow, startColumn]. You are allowed to move the ball to one of the four adjacent cells in the grid (possibly out of the grid crossing the grid boundary). You can apply at most maxMove moves to the ball.  Given the five integers m, n, maxMove, startRow, startColumn, return the number of paths to move the ball out of the grid boundary. Since the answer can be very large, return it modulo 10e9 + 7.	O(mnl)	O(mnl)	We're expecting an optimized solution of time complexity O(mnl) and memory complexity O(mnl), where l is the maximum number of moves allowed.	Dynamic Programming (Memoization)	DFS	DFS + Backtracking	Niji's Algorithm	A	Correct!  The approach used is dynamic programming with memoization. We use a 3D memory array to store the number of ways to move the ball out of the grid at the given position and with the given number of moves remaining. The count function checks if the current position is out of bounds or if the maximum number of moves has been reached. If either of these conditions is true, it returns 1 or 0 respectively. Otherwise, it checks the memory array to see if the value has already been computed for this position and remaining number of moves. If it has, it returns the stored value. Otherwise, it recursively computes the number of ways to move the ball out of the grid from each of the four adjacent cells and stores the sum in the memory array for future use.  Time Complexity: O(mnl), where l is the maximum number of moves allowed.  Memory Complexity: O(mnl), where l is the maximum number of moves allowed.	Like the brute-force approach, DFS also explores all possible paths, making it impractical for large values of maxMove. Additionally, DFS can lead to stack overflow errors for deep recursion, which can be an issue for larger grids.	Not ideal. This approach can still have exponential time complexity in the worst case, as there can be up to 4^maxMove possible paths to explore. It can also lead to stack overflow errors for large maxMove values, and the backtracking can be costly.	Color me disappointed! You've picked one of the few entirely fictional algorithms. 'Niji' is Japanese for rainbow.	The brute-force approach here is to try all possible paths starting from the given position and count the number of paths that move the ball out of the grid boundary within maxMove moves.  Why it's suboptimal: The number of possible paths grows exponentially with the number of moves, making this approach impractical for large values of maxMove.  Time Complexity: O(4^maxMove)	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Out%20of%20Boundary%20Paths.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Out%20of%20Boundary%20Paths.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Out%20of%20Boundary%20Paths.java	https://youtu.be/pLJEeFATluU
Algorithms Pack	Dynamic Programming (On Grid)	Dungeon Game	https://leetcode.com/problems/dungeon-game/	The demons had captured the princess and imprisoned her in the bottom-right corner of a dungeon. The dungeon consists of m x n rooms laid out in a 2D grid. Our valiant knight was initially positioned in the top-left room and must fight his way through dungeon to rescue the princess.    The knight has an initial health point represented by a positive integer. If at any point his health point drops to 0 or below, he dies immediately.    Some of the rooms are guarded by demons (represented by negative integers), so the knight loses health upon entering these rooms; other rooms are either empty (represented as 0) or contain magic orbs that increase the knight's health (represented by positive integers).    To reach the princess as quickly as possible, the knight decides to move only rightward or downward in each step.    Return the knight's minimum initial health so that he can rescue the princess.    Note that any room can contain threats or power-ups, even the first room the knight enters and the bottom-right room where the princess is imprisoned.	O(mn)	O(mn)	We're expecting an optimized solution of time complexity O(mn) and memory complexity O(mn).	Greedy	Dynamic Programming	DFS	BFS	B	This approach is suboptimal because it does not take into account the possibility of gaining health in future rooms.	The approach taken here is dynamic programming. We use a two-dimensional array memory to store the minimum health needed to reach the bottom-right corner from any given cell in the dungeon. We then traverse the dungeon recursively from the top-left corner to the bottom-right corner, computing the minimum health needed to move forward in each cell, and store it in the memory array. We then return the value stored in memory[0][0].  Time Complexity: O(mn), where m and n are the dimensions of the dungeon.  Memory Complexity: O(mn), where m and n are the dimensions of the dungeon.	This approach is suboptimal because it generates all possible paths, which can be exponential in the worst case.	This approach is suboptimal because it generates all possible paths, which can be exponential in the worst case.	We regret to have omitted the brute-force solution to this problem. Sorry!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Dungeon%20Game.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Dungeon%20Game.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Dungeon%20Game.java	https://youtu.be/9XpSLtQ1C5U
Algorithms Pack	Dynamic Programming (Tabulation)	Minimum Swaps To Make Sequences Increasing	https://leetcode.com/problems/minimum-swaps-to-make-sequences-increasing/	You are given two integer arrays of the same length nums1 and nums2. In one operation, you are allowed to swap nums1[i] with nums2[i].    For example, if nums1 = [1,2,3,8], and nums2 = [5,6,7,4], you can swap the element at i = 3 to obtain nums1 = [1,2,3,4] and nums2 = [5,6,7,8].  Return the minimum number of needed operations to make nums1 and nums2 strictly increasing. The test cases are generated so that the given input always makes it possible.    An array arr is strictly increasing if and only if arr[0] < arr[1] < arr[2] < ... < arr[arr.length - 1].	O(n)	O(n)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(n).	DFS	Dynamic Programming	Greedy	Dijkstra's Algorithm	B	This approach has a time complexity of O(n!), making it infeasible for arrays of even moderate size.	Correct!  Our approach uses dynamic programming to solve the problem. We create a 2D memory array to keep track of the minimum swaps required to make nums1 and nums2 strictly increasing till the ith index. Here, memory[i][0] represents the minimum swaps required if we do not swap the ith index, and memory[i][1] represents the minimum swaps required if we swap the ith index.  We initialize memory[0][0] = 0 and memory[0][1] = 1, as no swaps are required at the first index, and a swap is required to make the arrays strictly increasing.  We then iterate through the arrays, and for each index, we check if we swap or not. If we do not swap, we check if the current index can be added to the increasing sequence formed till now. If it can, we update memory[i][0] as the minimum of memory[i][0] and memory[i-1][0]. If it cannot be added, we ignore it.  If we swap, we check if the current index can be added to the increasing sequence formed till now after swapping. If it can, we update memory[i][1] as the minimum of memory[i][1] and 1+memory[i-1][0], as 1 swap is required. If it cannot be added, we ignore it.  Finally, we return the minimum of memory[A.size()-1][0] and memory[A.size()-1][1] as the answer.  Time Complexity: O(n), where n is the length of the arrays.  Memory Complexity: O(n), as we use a 2D memory array of size n.	This approach may not always find the optimal solution, as swapping elements without considering the future consequences may lead to a dead end.	This is not at all an optimal approach for this problem.	The brute-force here is to generate all possible swaps and calculate the number of swaps required to make both arrays strictly increasing. Return the minimum number of swaps.  Why it's suboptimal: The time complexity of this approach is O(n!), making it infeasible for arrays of even moderate size.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Minimum%20Swaps%20To%20Make%20Sequences%20Increasing.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Minimum%20Swaps%20To%20Make%20Sequences%20Increasing.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Minimum%20Swaps%20To%20Make%20Sequences%20Increasing.java	https://youtu.be/M5Y0Mzzn2YA
Algorithms Pack	Priority Queue	Ugly Number II	https://leetcode.com/problems/ugly-number-ii/	An ugly number is a positive integer whose prime factors are limited to 2, 3, and 5.    Given an integer n, return the nth ugly number.	O(n log(n))	O(n)	We're expecting an optimized solution of time complexity O(n log(n)) and memory complexity O(n).	Dynamic Programming	Greedy	Priority Queue	DFS	C	Ah - not quite optimal here. This approach has a high memory complexity since we would need to store all n ugly numbers in an array. Additionally, we would need to iterate through the array multiple times to generate the next ugly numbers, leading to a higher time complexity.	This approach will frequently return the wrong answer.	Correct!  We start by adding the first ugly number 1 to a priority queue. At each iteration, we take out the smallest ugly number from the priority queue, which is at the top of the heap. We add the 2nd, 3rd, and 5th multiples of the removed ugly number to the priority queue. By doing this, we ensure that we have all the ugly numbers in the priority queue in ascending order. We repeat the process until we find the nth ugly number.  Time Complexity: O(n log(n)), where n is the value of the input integer, n. This is because the maximum size of the priority queue at any point in time will be n, and every time we perform an operation on the priority queue, it takes O(log(n)) time.  Memory Complexity: O(n), where n is the value of the input integer, n.	This approach is very slow and requires generating a lot of unnecessary numbers. It can also run into issues with stack overflow for large values of n.	We can generate all positive integers and check whether each integer is an ugly number. We can stop checking once we have found the nth ugly number.  Why it's suboptimal: This approach has an extremely high time complexity since we would need to generate and check a potentially infinite number of integers until we find the nth ugly number.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Ugly%20Number%20II.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Ugly%20Number%20II.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Ugly%20Number%20II.java	https://youtu.be/9aQLXHZ2fAc
Algorithms Pack	Dynamic Programming (Tabulation)	Wiggle Subsequence	https://leetcode.com/problems/wiggle-subsequence/	A wiggle sequence is a sequence where the differences between successive numbers strictly alternate between positive and negative. The first difference (if one exists) may be either positive or negative. A sequence with one element and a sequence with two non-equal elements are trivially wiggle sequences.    For example, [1, 7, 4, 9, 2, 5] is a wiggle sequence because the differences (6, -3, 5, -7, 3) alternate between positive and negative.  In contrast, [1, 4, 7, 2, 5] and [1, 7, 4, 5, 5] are not wiggle sequences. The first is not because its first two differences are positive, and the second is not because its last difference is zero.  A subsequence is obtained by deleting some elements (possibly zero) from the original sequence, leaving the remaining elements in their original order.    Given an integer array nums, return the length of the longest wiggle subsequence of nums.	O(n)	O(1)	We're expecting an optimized solution of time complexity O(n) and memory complexity O(1).	Dynamic Programming	Sorting	Greedy	Divide and Conquer	C	While possible, and better than brute-force, it still has a time complexity of O(n^2), since we need to compute the maximum of the up and down values for each element in the input array.	This approach has a time complexity of O(n log(n)) due to the sorting step. Additionally, it does not give us the length of the longest wiggle subsequence, only a boolean answer as to whether the input array is a wiggle sequence or not.	Correct!  The approach is based on the observation that we only need to keep track of the difference between successive numbers and whether they are positive or negative. We can use two variables up and down to keep track of the length of the longest wiggle subsequence ending at the current index, where up represents the length of the longest wiggle subsequence ending at the current index with the last two elements as an up-down pair, and down represents the length of the longest wiggle subsequence ending at the current index with the last two elements as a down-up pair.  At each index, we update the values of up and down depending on whether the difference between the current element and the previous element is positive or negative. If the difference is positive, we update up to down + 1, because the last two elements are a down-up pair. If the difference is negative, we update down to up + 1 because the last two elements are an up-down pair. Finally, the length of the longest wiggle subsequence is 1 plus the maximum of up and down.  Time Complexity: O(n), as the solution scans the input array once.  Memory Complexity: O(1), since the solution uses constant extra space to keep track of up and down.	This approach may produce incorrect results for certain inputs, especially if the longest wiggle sequence crosses over from one half to the other.	The brute-force approach here is to generate all possible subsequences of the input array and check if each subsequence is a wiggle sequence or not. Keep track of the length of the longest wiggle sequence found so far.  Why it's suboptimal: The time complexity of this approach is O(2^n) where n is the length of the input array, since we generate all possible subsequences. This approach quickly becomes infeasible for even moderately sized inputs.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Wiggle%20Subsequence.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Wiggle%20Subsequence.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Wiggle%20Subsequence.java	https://youtu.be/SefC4ZO_AiI
Algorithms Pack	Backtracking	All Paths From Source To Target	https://leetcode.com/problems/all-paths-from-source-to-target/	Given a directed acyclic graph (DAG) of n nodes labeled from 0 to n - 1, find all possible paths from node 0 to node n - 1 and return them in any order.    The graph is given as follows: graph[i] is a list of all nodes you can visit from node i (i.e., there is a directed edge from node i to node graph[i][j]).	O(2^n*n)	O(n)	We're expecting an optimized solution of time complexity O(2^n*n) and memory complexity O(n).	BFS	Dijkstra's Algorithm	Greedy	Backtracking	D	This approach does not explore all possible paths and can get stuck in a dead end.	This approach does not work for graphs with negative weights, and it is not guaranteed to find the shortest path in a directed graph.	This approach does not explore all possible paths and can get stuck in a dead end. Also, it is not guaranteed to find the shortest path in a directed graph.	Correct!  In the backtracking approach, we start with a node and explore all its neighbors. If the neighbor is the target node, then we add the current path to our answer. Otherwise, we recursively repeat the same process by exploring the neighbor's neighbors until we reach the target node. We keep track of the current path in a vector and backtrack by removing the last node from the vector and exploring other neighbors.  Time Complexity: O(2^n*n), where n is the number of nodes in the graph. The worst-case scenario is when all nodes are connected to each other, and we end up exploring all possible paths, leading to a time complexity of O(2^n). Also, for each path, we need to copy the path to our answer, which takes O(n) time.  Memory Complexity: O(n), where n is the number of nodes in the graph. We store the current path in a vector of size n and the answer vector, which can contain at most 2^n paths.	The brute-force approach here (which you need to be able to code) is, starting from node 0, to explore all possible paths by recursively traversing all its neighbors until we reach node n-1.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/All%20Paths%20From%20Source%20to%20Target.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/All%20Paths%20From%20Source%20to%20Target.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/All%20Paths%20From%20Source%20to%20Target.java	https://youtu.be/IC4rmvbcyYE
Algorithms Pack	Backtracking	N-Queens	https://leetcode.com/problems/n-queens/description/	The n-queens puzzle is the problem of placing n queens on an n x n chessboard such that no two queens attack each other.    Given an integer n, return all distinct solutions to the n-queens puzzle. You may return the answer in any order.    Each solution contains a distinct board configuration of the n-queens' placement, where 'Q' and '.' both indicate a queen and an empty space, respectively.	O(n!)	O(n^2)	We're expecting an optimized solution of time complexity O(n!) and memory complexity O(n^2).	Backtracking	Greedy	Simulated Annealing	Greedy + Deque	A	Correct!  Backtracking is an algorithmic approach that works by incrementally building a solution to a problem while searching among all possible candidates. In the n-queens problem, we place one queen at a time in a column and move to the next row, avoiding any placement that would allow another queen to attack the current one. When a solution is found, it is added to the list of solutions.  Time Complexity: O(n!)  The algorithm generates all possible configurations of queens on the board, which means the time complexity is O(n!) since there are n possibilities for the first row, n-2 possibilities for the second row (as one column is already occupied), n-4 possibilities for the third row (as two columns are already occupied), and so on. The number of possibilities reduces rapidly, making the algorithm's time complexity feasible for small values of n.  Memory Complexity: O(n^2)  The algorithm uses O(n^2) memory to store the board, O(n) memory for the column array, and O(2n-1) memory for the two diagonal arrays. The total memory usage is, therefore, O(n^2).	Incorrect. This approach could get stuck in a local minimum, and fail to find a solution even if one exists. It's not a guaranteed solution, and could potentially miss valid solutions.	Not viable here. This approach may get stuck in a local optimum and fail to find a global solution. It may also fail to find a solution for some values of n. The time complexity is not well-defined.	Even adding a deque to the greedy algorithm isn't going to help this approach work.	There's no viable brute-force for this problem!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/N-Queens.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/N-Queens.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/N-Queens.java	https://youtu.be/561NIls4dLM
Algorithms Pack	Backtracking	Sudoku Solver	https://leetcode.com/problems/sudoku-solver/	Write a program to solve a Sudoku puzzle by filling the empty cells.    A sudoku solution must satisfy all of the following rules:    Each of the digits 1-9 must occur exactly once in each row.  Each of the digits 1-9 must occur exactly once in each column.  Each of the digits 1-9 must occur exactly once in each of the 9 3x3 sub-boxes of the grid.  The '.' character indicates empty cells.	O(9^(mn))	O(1)	We're expecting an optimized solution of time complexity O(9^(mn)) and memory complexity O(1).	Backtracking	Greedy	Sieve of Eratosthenes	Merge Sort	A	Correct!  The backtracking algorithm is a brute-force approach to solve the Sudoku puzzle. It tries all possible values for each empty cell of the board until it finds a valid solution. At each empty cell, we try all the digits from 1 to 9 and check if the digit is valid for that cell according to the Sudoku rules. If a valid digit is found, we mark it as used in the row, column, and box and move on to the next empty cell. If there is no valid digit for the current cell, we backtrack to the previous cell and try a different digit.  Time Complexity: The backtracking algorithm tries all possible values for each empty cell of the board until it finds a valid solution. Therefore, the time complexity of this algorithm is exponential, O(9^(m*n)), where m and n are the dimensions of the board.  Memory Complexity: The algorithm uses three 2D arrays of size 9x9 to keep track of the used digits in each row, column, and box, and two vectors to store the coordinates of the empty cells. Therefore, the memory complexity of this algorithm is O(1) since the size of the input board is fixed.	Greedy algorithms make locally optimal choices at each step without considering the global optimal solution. This can lead to incorrect solutions if the initial choice was incorrect or if there are multiple solutions to the puzzle.	Not viable for this problem.	Nope, merge sort won't help here.	There's no viable brute-force for this problem!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Sudoku%20Solver.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Sudoku%20Solver.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Sudoku%20Solver.java	https://youtu.be/CmeuavhCxkQ
Algorithms Pack	Backtracking	Permutations	https://leetcode.com/problems/permutations/	Given an array nums of distinct integers, return all the possible permutations. You can return the answer in any order.	O(n*n!)	O(n*n!)	We're expecting an optimized solution of time complexity O(n*n!) and memory complexity O(n*n!).	Sort + Swap	Backtracking	Divide And Conquer	Bubble Sort	B	This approach involves sorting the array in ascending order and then generating permutations by swapping adjacent elements. However, this approach fails to generate all possible permutations for the input array and can lead to incorrect answers.	Correct!  In this approach, we generate all possible permutations of the given distinct integers array by recursively swapping each element with all other remaining elements one by one until all possible permutations are generated. This is a type of backtracking where we start with the first element, swap it with each element in the array and recursively permute the remaining elements. We backtrack by swapping back the elements, so we can generate all permutations. When the current index becomes equal to the length of the array, it means we have generated one possible permutation of the array.  Time Complexity: O(n*n!), where n is the length of the array. The number of permutations of n elements is n!, and for each permutation, we have to perform n swaps.  Memory Complexity: O(n*n!), since we need to store all possible permutations.	This approach does not work because merging the permutations of both parts is not as simple as just concatenating them. It is not possible to get all permutations by just merging the permutations of both parts.	This won't help at all with generating the permutations by itself.	There's no viable brute-force for this problem!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Permutations.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Permutations.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Permutations.java	https://youtu.be/7aHr8j-mWgA
Algorithms Pack	Backtracking	Permutations II	https://leetcode.com/problems/permutations-ii/	Given a collection of numbers, nums, that might contain duplicates, return all possible unique permutations in any order.	O(n! * n)	O(n! * n)	We're expecting an optimized solution of time complexity O(n! * n) and memory complexity O(n! * n).	Sorting	Subset Generation	Backtracking + Set	Random Sampling	C	This approach relies on the input array being sorted, which adds an extra O(n log(n)) time complexity to the solution. It also skips generating permutations that are equivalent to the previous one, but this does not account for cases where there are repeated elements that are not adjacent in the sorted array. Therefore, it does not guarantee unique permutations for all possible inputs.	Even less efficient than backtracking. Brief explanation of approach: Generate all subsets of the input array, and then generate permutations for each subset. This ensures that there are no repeated elements in the permutation, but it requires generating a large number of subsets, which can be highly inefficient. Why it's suboptimal: This approach generates a large number of subsets, which can be highly inefficient, especially for larger inputs. It also generates permutations for each subset, which adds an extra time complexity to the solution.	Correct!  We use a backtracking approach to generate all possible permutations of the given numbers while keeping track of unique permutations using a filter (a set in this case).  Time Complexity: O(n! * n) - generating n! permutations and checking uniqueness takes O(n) time for each permutation.  Memory Complexity: O(n! * n) - storing all permutations in a set requires O(n! * n) space.	Not viable at all. This approach is highly inefficient as it involves generating a large number of random permutations until all unique permutations have been found. It also may not generate all unique permutations for certain inputs, resulting in incorrect output.	There's no viable brute-force for this problem!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Permutations%20II.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Permutations%20II.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Permutations%20II.java	https://youtu.be/4EqvRxfcqus
Algorithms Pack	Divide And Conquer	Global And Local Inversions	https://leetcode.com/problems/global-and-local-inversions/	You are given an integer array nums of length n which represents a permutation of all the integers in the range [0, n - 1].    The number of global inversions is the number of the different pairs (i, j) where:    0 <= i < j < n  nums[i] > nums[j]  The number of local inversions is the number of indices i where:    0 <= i < n - 1  nums[i] > nums[i + 1]  Return true if the number of global inversions is equal to the number of local inversions.	O(n log(n))	O(n)	We're expecting an optimized solution of time complexity O(n log(n)) and memory complexity O(n).	Insertion Sort	Two Pointers	Merge Sort	Divide And Conquer without Sorting	C	Too slow compared to Merge Sort. We cmplement the Insertion Sort algorithm and count the number of local inversions while sorting the array, then count global inversions using the brute-force approach. Why it's suboptimal: This approach has a time complexity of O(n^2) for counting global inversions and O(n^2) for sorting the array, resulting in a total time complexity of O(n^2 + n^2).	Incorrect! This approach only counts local inversions and does not account for global inversions, so it may return a false positive result.	Correct!  In this approach, we use merge sort to count the number of global inversions in the given array, as merge sort inherently counts inversions during the merging process. We also count the number of local inversions using a simple loop. Finally, we compare the counts of local and global inversions to determine if they are equal.  We use a mergeSortedSubarrays function to merge two sub-arrays in the merge sort process. This function returns the number of inversions that occur between the two sub-arrays being merged. If the current element of the right sub-array is smaller than the current element of the left sub-array, we count an inversion and add the number of remaining elements in the left sub-array to the inversion count.  Time Complexity: O(n log(n)), where n is the length of the input array. This is due to the use of merge sort.  Memory Complexity: O(n), where n is the length of the input array. This is because we create a temporary array of the same length as the input array to store the sorted sub-arrays during the merge sort process.	Incorrect! This approach only counts global inversions between the two halves and does not account for local inversions or global inversions within each half, so it may return a false negative result. We want Divide&Conquer with Merge Sort!	The brute-force approach here is to generate all pairs of indices (i, j) and check if they satisfy the condition nums[i] > nums[j] for global inversions, or nums[i] > nums[i+1] for local inversions.  Why it's suboptimal: This approach has a time complexity of O(n^2) since we need to generate all possible pairs of indices.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Global%20and%20Local%20Inversions.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Global%20and%20Local%20Inversions.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Global%20and%20Local%20Inversions.java	https://youtu.be/_YC-yHb8UT4
Algorithms Pack	Divide And Conquer	Pow(x,n)	https://leetcode.com/problems/powx-n/	Implement pow(x, n), which calculates x raised to the power n (i.e., xn).	O(log(n))	O(log(n))	We're expecting an optimized solution of time complexity O(log(n)) and memory complexity O(log(n)).	Divide & Conquer	Dynamic Programming	Recursion	Hash Table	A	Correct!  Our approach to solve the given problem is recursive binary exponentiation. It is a widely used technique to calculate the power of a number, where instead of multiplying the base number with itself 'n' times, we divide the power by two repeatedly until it reaches the base case (power equals 0) while multiplying the base with itself at every step.  To implement the algorithm, we define a recursive function, fastPow, that takes the base number and power as input and returns the result of base to the power of n. It uses a divide and conquer strategy to calculate the power more efficiently.  In the recursive function, if n is 0, we return 1, as anything raised to the power of 0 is 1. Otherwise, we call the function recursively with n/2 and multiply the result by itself to get result*result. If n is odd, we also multiply the result by x.  In the main function, myPow, we first convert n to a long long variable N to avoid overflow issues. If n is negative, we update x to be its reciprocal, and then calculate the power using the fastPow function.  Time Complexity: O(log(n)) since we divide the power by 2 at each recursive call, reducing the number of recursive calls from n to log(n).  Memory Complexity: O(log(n)) as we need to store the intermediate result at each level of the recursive call stack.	This would be over-engineering the approach.	This brute-force approach becomes highly inefficient, as it will always require n-1 multiplications.	This is completely unnecessary for this problem.	In the brute-force approach, we multiply the base number with itself 'n' times to calculate the power.  Why it's suboptimal: This approach is highly inefficient as it requires n-1 multiplications to calculate the power.  Time Complexity: O(n)	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Pow(x%2C%20n).cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Pow(x%2C%20n).py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Pow(x%2C%20n).java	https://youtu.be/58sPeKHtoJU
Algorithms Pack	Shortest Path Algorithms	Network Delay Time	https://leetcode.com/problems/network-delay-time/	You are given a network of n nodes, labeled from 1 to n. You are also given times, a list of travel times as directed edges times[i] = (ui, vi, wi), where ui is the source node, vi is the target node, and wi is the time it takes for a signal to travel from source to target.    We will send a signal from a given node k. Return the minimum time it takes for all the n nodes to receive the signal. If it is impossible for all the n nodes to receive the signal, return -1.	O(n+e log(n))	O(e log(n))	We're expecting an optimized solution of time complexity O(n+e log(n)) and memory complexity O(e log(n)).	Floyd-Warshall	Bellman-Ford Algorithm	Prim's Algorithm	Dijkstra's Algorithm	D	This works, but the time complexity of this approach is O(n^3), which is inefficient for larger networks. It also requires more memory than Dijkstra's Algorithm.	This will work, but is slightly less efficient than Dijkstra's Algorithm for this problem.	Prim is best used for MST (Minimum Spanning Tree) problems. Dijkstra is the best choice here.	Correct!  We construct an adjacency list from the given times vector and perform Dijkstra's algorithm starting from the source node k to find the minimum time it takes for all nodes to receive the signal.  This approach is optimal because Dijkstra's algorithm is a well-known algorithm for finding the shortest path in a weighted graph. It is guaranteed to find the shortest path to all nodes from the source node in O((E+V)logV) time complexity, where E is the number of edges and V is the number of vertices.	Check all possible paths between nodes and find the minimum time it takes for all nodes to receive the signal.  Why it's suboptimal: This approach checks all possible paths between nodes, which is very time-consuming and inefficient. It has a time complexity of O(n^3) using the Floyd-Warshall algorithm.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Network%20Delay%20Time.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Network%20Delay%20Time.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Network%20Delay%20Time.java	https://youtu.be/ZNn9sh4NpAs
Algorithms Pack	Shortest Path Algorithms	Number of Ways To Arrive At Destination	https://leetcode.com/problems/number-of-ways-to-arrive-at-destination/	You are in a city that consists of n intersections numbered from 0 to n - 1 with bi-directional roads between some intersections. The inputs are generated such that you can reach any intersection from any other intersection and that there is at most one road between any two intersections.    You are given an integer n and a 2D integer array roads where roads[i] = [ui, vi, timei] means that there is a road between intersections ui and vi that takes timei minutes to travel. You want to know in how many ways you can travel from intersection 0 to intersection n - 1 in the shortest amount of time.    Return the number of ways you can arrive at your destination in the shortest amount of time. Since the answer may be large, return it modulo 1e9 + 7.	O(m log(n))	O(n+m)	We're expecting an optimized solution of time complexity O(m log(n)) and memory complexity O(n+m).	Dijkstra's Algorithm	Greedy	DFS	Kruskal's Algorithm	A	Correct!  Time Complexity: O(m log(n)), where m is the number of edges and n is the number of nodes. This is because we need to process each edge once and for each edge, we may add or remove a node from the priority queue, which takes log(n) time.  Memory Complexity: O(n+m), where n is the number of nodes and m is the number of edges. We use an adjacency list to store the graph, which takes O(n+m) space, a vector of distances, and a vector of path counts, each of size n. We also use a priority queue, which takes up to O(n) space.	This approach may get stuck in a suboptimal solution and miss the globally optimal solution, especially if there are multiple local optima.	This approach may require exploring a large number of combinations, leading to an exponential time complexity.	Kruskal's Algorithm is best used for MST (Minimum Spanning Tree) problems. Dijkstra is the best choice here.	For this problem, brute-force search is especially slow and impractical when dealing with large problem sizes, as it checks every possible solution without any optimization.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Number%20of%20Ways%20to%20Arrive%20at%20Destination.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Number%20of%20Ways%20to%20Arrive%20at%20Destination.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Number%20of%20Ways%20to%20Arrive%20at%20Destination.java	https://youtu.be/fAVF9vDTHjY
Algorithms Pack	BFS	Minimum Cost To Make At Least One Valid Path in a Grid	https://leetcode.com/problems/minimum-cost-to-make-at-least-one-valid-path-in-a-grid/	Given an m x n grid. Each cell of the grid has a sign pointing to the next cell you should visit if you are currently in this cell. The sign of grid[i][j] can be:    1 which means go to the cell to the right. (i.e go from grid[i][j] to grid[i][j + 1])  2 which means go to the cell to the left. (i.e go from grid[i][j] to grid[i][j - 1])  3 which means go to the lower cell. (i.e go from grid[i][j] to grid[i + 1][j])  4 which means go to the upper cell. (i.e go from grid[i][j] to grid[i - 1][j])  Notice that there could be some signs on the cells of the grid that point outside the grid.    You will initially start at the upper left cell (0, 0). A valid path in the grid is a path that starts from the upper left cell (0, 0) and ends at the bottom-right cell (m - 1, n - 1) following the signs on the grid. The valid path does not have to be the shortest.    You can modify the sign on a cell with cost = 1. You can modify the sign on a cell one time only.    Return the minimum cost to make the grid have at least one valid path.	O(nm)	O(nm)	We're expecting an optimized solution of time complexity O(nm) and memory complexity O(nm).	Greedy + Deque	BFS+Deque	Dijkstra's Algorithm + Deque	Brute Force + Deque	B	No. This approach does not take into account that changing a sign might lead to a cheaper or more optimal path, and may lead to getting stuck in an infinite loop.	Correct!  We use BFS to explore the grid from the start cell, and we keep track of the distance from the start cell to each visited cell. We use a deque to store the cells that we need to visit next, and we prioritize cells with zero edge weight (i.e. cells that we can visit without incurring any cost).  We start by setting the distance of the start cell to zero, and we add it to the deque. Then, for each cell in the deque, we explore its neighbors and update their distances if necessary. If the edge weight between the current cell and its neighbor is zero, we add the neighbor to the front of the deque (since it has zero cost), otherwise we add it to the back of the deque.  Finally, we return the distance of the bottom-right cell from the start cell. If the bottom-right cell is not reachable from the start cell, its distance will be equal to infinity (i.e. the initial value we set for all cells), so we return a value that indicates that the grid has no valid path.	Not optimal at all! This approach fails to take into account that we can change the direction of the signs in the grid, and may lead to finding a non-optimal path.	Good luck with that! A BFS + Deque will work here.	Our brute-force would be a recursive search! Basically, we try every possible path starting from (0, 0) to (m-1, n-1). For each cell, recursively check all possible directions until either a valid path is found or all paths are exhausted.  Why it's suboptimal: This approach has exponential time complexity as it explores all possible paths, making it very expensive for even moderately large grids.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Minimum%20Cost%20to%20Make%20at%20Least%20One%20Valid%20Path%20in%20a%20Grid.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Minimum%20Cost%20to%20Make%20at%20Least%20One%20Valid%20Path%20in%20a%20Grid.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Minimum%20Cost%20to%20Make%20at%20Least%20One%20Valid%20Path%20in%20a%20Grid.java	https://youtu.be/InealzgYMjg
Algorithms Pack	Shortest Path Algorithms	Number of Restricted Paths From First To Last Node	https://leetcode.com/problems/number-of-restricted-paths-from-first-to-last-node/	There is an undirected weighted connected graph. You are given a positive integer n which denotes that the graph has n nodes labeled from 1 to n, and an array edges where each edges[i] = [ui, vi, weighti] denotes that there is an edge between nodes ui and vi with weight equal to weighti.    A path from node start to node end is a sequence of nodes [z0, z1, z2, ..., zk] such that z0 = start and zk = end and there is an edge between zi and zi+1 where 0 <= i <= k-1.    The distance of a path is the sum of the weights on the edges of the path. Let distanceToLastNode(x) denote the shortest distance of a path between node n and node x. A restricted path is a path that also satisfies that distanceToLastNode(zi) > distanceToLastNode(zi+1) where 0 <= i <= k-1.    Return the number of restricted paths from node 1 to node n. Since that number may be too large, return it modulo 10e9 + 7.	O(m log(n) + n^2)	O(n + m)	We're expecting an optimized solution of time complexity O(m log(n) + n^2) and memory complexity 'Y'.	Exhaustive Search + Brute Force	Greedy	Dynamic Programming + Dijkstra's Algorithm	Randomized Search	C	Not the right approach. This approach has a high time complexity and is only practical for small problem sizes. For larger problems, the search space becomes too large to explore in a reasonable amount of time.	Not the correct method to use here. The approach is suboptimal because it may not always lead to the optimal solution. Greedy algorithms may be fooled by a locally optimal choice that does not lead to the globally optimal solution.	Correct!  Our approach involves using Dijkstra's Algorithm to find the shortest distance from node n to every other node in the graph. This is stored in the vector<long long>dist. We then apply Dynamic Programming to count the number of restricted paths from node 1 to node n.  To count the number of restricted paths, we use a recursive function countPaths that counts the number of restricted paths from the current node to the destination node (n). We store the number of paths in memory to avoid recomputing.  The time complexity of the approach is dominated by Dijkstra's algorithm, which has a time complexity of O(m log(n)), where m is the number of edges and n is the number of nodes. The space complexity of the approach is O(n + m), where n is the number of nodes and m is the number of edges.  Time Complexity: O(m log(n) + n^2), where m is the number of edges and n is the number of nodes.  Memory Complexity: O(n + m)	This is just a terrible idea for this problem.	There's very little worth to the brute-force in this case. Try all possible solutions and pick the correct one.  Why it's suboptimal: This approach is very inefficient and is not practical for large problem instances.  Time Complexity: O(n!), where n is the number of possible solutions.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Number%20of%20Restricted%20Paths%20From%20First%20to%20Last%20Node.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Number%20of%20Restricted%20Paths%20From%20First%20to%20Last%20Node.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Number%20of%20Restricted%20Paths%20From%20First%20to%20Last%20Node.java	https://youtu.be/2A2qD0cpsjY
Algorithms Pack	Minimum Spanning Tree	Minimum Cost To Connect All Points	https://leetcode.com/problems/min-cost-to-connect-all-points/	You are given an array points representing integer coordinates of some points on a 2D-plane, where points[i] = [xi, yi].    The cost of connecting two points [xi, yi] and [xj, yj] is the manhattan distance between them: |xi - xj| + |yi - yj|, where |val| denotes the absolute value of val.    Return the minimum cost to make all points connected. All points are connected if there is exactly one simple path between any two points.	O(n^2 log(n))	O(n^2)	We're expecting an optimized solution of time complexity O(n^2 log(n)) and memory complexity O(n^2).	Kruskal's Algorithm + Union Find	Floyd-Warshall	Dijkstra's Algorithm	DFS	A	Correct! Actually, there's an optimized version of Prim's Algorithm that is superior to this, but it would be very harsh is that were demanded in an interview!  We use Kruskal's Algorithm to find the minimum spanning tree (MST) of the complete graph formed by the given points, where the weight of each edge is the Manhattan distance between the two points it connects. We create an edge list with all possible edges and their weights and sort it in decreasing order of weight. We then use Union-Find data structure to iteratively add edges to the MST while avoiding cycles until all nodes are connected.  Time Complexity: O(n^2 log(n)), where n is the number of points. We first create an edge list of size n^2 and sort it, which takes O(n^2 log(n)) time. Kruskal's Algorithm takes O(n log(n)) time, and we run it on the edge list, which contains n^2 edges.  Memory Complexity: O(n^2), as we store all possible edges in the edge list.	No. Firstly, the Floyd-Warshall algorithm has a time complexity of O(n^3), which is worse than the O(n^2 log(n)) time complexity of Kruskal's algorithm on this problem.	No. Dijkstra's algorithm is designed for finding the shortest path between two points in a graph, and is not suitable for finding minimum spanning trees, which violates the problem constraints.	Incorrect!  DFS can only find a spanning tree of the graph, which may not be the minimum spanning tree. Moreover, it may not work on disconnected graphs, which violates the problem constraints.	The brute-force here is arguably more difficult than the correct answer. Calculate the Manhattan distance between all pairs of points, and then choose an MST algorithm to find the minimum spanning tree of the graph.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Min%20Cost%20to%20Connect%20All%20Points.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Min%20Cost%20to%20Connect%20All%20Points.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Min%20Cost%20to%20Connect%20All%20Points.java	https://youtu.be/0lzuqGIB3XY
Algorithms Pack	Minimum Spanning Tree	Similar String Groups	https://leetcode.com/problems/similar-string-groups/	Two strings X and Y are similar if we can swap two letters (in different positions) of X, so that it equals Y. Also two strings X and Y are similar if they are equal.    For example, 'tars' and 'rats' are similar (swapping at positions 0 and 2), and 'rats' and 'arts' are similar, but 'star' is not similar to 'tars', 'rats', or 'arts'.    Together, these form two connected groups by similarity: {'tars', 'rats', 'arts'} and {'star'}. Notice that 'tars' and 'arts' are in the same group even though they are not similar. Formally, each group is such that a word is in the group if and only if it is similar to at least one other word in the group.    We are given a list strs of strings where every string in strs is an anagram of every other string in strs. How many groups are there?	O(n^2 * k)	O(n)	We're expecting an optimized solution of time complexity O(n^2 * k) and memory complexity O(n).	Hash Table	Union Find	DFS	Brute Force + Deque	B	This is suboptimal due to the time and memory complexity. The approach is: for each word, generate a set of all similar words using the isSimilar function. Add each word and its set of similar words to a hash table. For each word in the hash table, union it with all the words in its set of similar words.	Correct!  Our approach utilizes the Union-Find data structure. We first initialize an instance of the Union-Find data structure with n sets, where n is the number of strings in the given list. Each set represents a group of similar strings. We iterate through each pair of strings in the list and check if they are similar using the isSimilar() function. If two strings are similar, we perform a union operation on the corresponding sets in the Union-Find data structure using union_set() function. Finally, we return the number of sets, which represents the number of groups of similar strings.  Time Complexity: O(n^2 * k), where n is the number of strings in the given list and k is the length of each string. This is because we compare each pair of strings, which takes O(k) time, and we perform up to O(n^2) union operations, each taking O(α(n)) time, where α(n) is the inverse Ackermann function, which grows very slowly.  Memory Complexity: O(n), since we initialize an instance of the Union-Find data structure with n sets, and each set requires a constant amount of memory.	Not quite! This solution has a time complexity of O(n^2 * l^2) in the worst case, because we need to check each pair of words to see if they are similar, which takes O(n^2 * l^2) time. Then we build the graph	With or without a deque, a Union-Find would be a better choice for solving this problem.	Sorry, we're not offering a brute-force suggestion for this problem.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Similar%20String%20Groups.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Similar%20String%20Groups.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Similar%20String%20Groups.java	https://youtu.be/mpSDMbCJJIY
Algorithms Pack	Minimum Spanning Tree	Number of Operations To Make Network Connected	https://leetcode.com/problems/number-of-operations-to-make-network-connected/	There are n computers numbered from 0 to n - 1 connected by ethernet cables connections forming a network where connections[i] = [ai, bi] represents a connection between computers ai and bi. Any computer can reach any other computer directly or indirectly through the network.    You are given an initial computer network connections. You can extract certain cables between two directly connected computers, and place them between any pair of disconnected computers to make them directly connected.    Return the minimum number of times you need to do this in order to make all the computers connected. If it is not possible, return -1.	O(n + m log(n))	O(n)	We're expecting an optimized solution of time complexity O(n + m log(n)) and memory complexity O(n).	Floyd-Warshall	BFS	Recursive Backtracking	Union Find	D	This will be suboptimal, and incorrect even if the edges were guaranteed to exist.	BFS does not guarantee that all nodes are connected. It only counts the number of connected components.	Even if this were engineered to work, this would be wildly inferior to a Union-Find approach.	Correct!  Our approach is based on the Union-Find algorithm. First, we check if the number of connections is less than n-1, which is the minimum number of edges required to connect all nodes. If there are fewer edges than required, we know there are disconnected components in the graph. Then we create a Union-Find data structure uf with n nodes, and iterate through each connection in connections and call the union_set method of uf to join the nodes represented by the current connection. The union_set method first finds the sets to which the nodes belong using the find_set method of uf and then merges the smaller set into the larger set using the link method of uf. If the sets were merged, it decrements the forests counter of uf by 1. Finally, we return the number of connected components in the graph, which is equal to the number of forests in uf minus 1.  Time Complexity: O(n + m log(n)), where n is the number of nodes and m is the number of edges. The union_set operation takes O(log(n)) time, and we call it once for each edge in the input.  Memory Complexity: O(n), where n is the number of nodes. We use two vectors, rank and parent, each with n elements, to store the Union-Find data structure.	The brute-force approach here is to generate all possible sets of edges that connect all nodes, and count the minimum number of operations required to form a connected graph from the given edges.  Why it's suboptimal: This approach generates all possible subsets of edges, which results in a time complexity of O(2^m), where m is the number of edges in the graph. This quickly becomes infeasible even for moderately sized graphs.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Number%20of%20Operations%20to%20Make%20Network%20Connected.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Number%20of%20Operations%20to%20Make%20Network%20Connected.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Number%20of%20Operations%20to%20Make%20Network%20Connected.java	https://youtu.be/OQPmhj9qjJY
Algorithms Pack	Minimum Spanning Tree	Find Critical And Pseudo-Critical Edges In Minimum Spanning Tree	https://leetcode.com/problems/find-critical-and-pseudo-critical-edges-in-minimum-spanning-tree/	Given a weighted undirected connected graph with n vertices numbered from 0 to n - 1, and an array edges where edges[i] = [ai, bi, weighti] represents a bidirectional and weighted edge between nodes ai and bi. A minimum spanning tree (MST) is a subset of the graph's edges that connects all vertices without cycles and with the minimum possible total edge weight.    Find all the critical and pseudo-critical edges in the given graph's minimum spanning tree (MST). An MST edge whose deletion from the graph would cause the MST weight to increase is called a critical edge. On the other hand, a pseudo-critical edge is that which can appear in some MSTs but not all.    Note that you can return the indices of the edges in any order.	O(m log(n))	O(n+m)	We're expecting an optimized solution of time complexity O(m log(n)) and memory complexity O(n+m).	Kruskal's Algorithm + Union Find	DFS	Dijkstra's Algorithm	Rabin-Karp Algorithm	A	Correct!  Our solution uses Kruskal's algorithm, a greedy algorithm for finding minimum spanning trees, along with a union-find data structure for efficient set manipulation. The algorithm first sorts the given edges in non-descending order and adds edges one by one to the set of edges if they do not form a cycle in the current set. We maintain the MST weight so far as we add edges. We then iterate over all edges and remove each one in turn to check whether it is critical, pseudo-critical or neither. For this, we compute the MST weight of the remaining edges. If removing the edge causes an increase in the MST weight, then it is a critical edge. If the edge is required to be included in every MST of the graph, it is a pseudo-critical edge. Otherwise, the edge is neither critical nor pseudo-critical.  Time Complexity: O(m logm), where m is the number of edges in the graph. Sorting the edges takes O(m logm) time, and performing union-find on edges requires O(m log(n)) time in the worst case, where log is the iterated logarithm function.  Memory Complexity: O(m+n), where n is the number of nodes in the graph. The Union-Find data structure uses O(n) memory, and the edge list uses O(m) memory.	This approach is suboptimal because it requires computing a new MST for each modified graph, which may take a lot of time, especially for large graphs. In addition, identifying all cycles in the graph requires a DFS, which has a time complexity of O(m+n).  Time Complexity: O(m^3 log m)	No! Given that we're looking for a minimum spanning tree here, you should be thinking about Prim or Kruskal's algorithms.	No! Given that we're looking for a minimum spanning tree here, you should be thinking about Prim or Kruskal's algorithms. This is a string searching algorithm!	To brute-force this extremely difficult problem, we'd need to generate all possible subsets of edges, and find the minimum spanning tree for each subset. Among all the minimum spanning trees, identify critical and pseudo-critical edges.  Why it's suboptimal: The number of possible subsets is 2^m where m is the number of edges in the graph. Therefore, the time complexity of this algorithm is O(2^m * m log m), which is prohibitively expensive for graphs with even moderate numbers of edges.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Find%20Critical%20and%20Pseudo-Critical%20Edges%20in%20Minimum%20Spanning%20Tree.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Find%20Critical%20and%20Pseudo-Critical%20Edges%20in%20Minimum%20Spanning%20Tree.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Find%20Critical%20and%20Pseudo-Critical%20Edges%20in%20Minimum%20Spanning%20Tree.java	https://youtu.be/t-cpaIXowWI
Algorithms Pack	Minimum Spanning Tree	Minimum Malware Spread	https://leetcode.com/problems/minimize-malware-spread/	You are given a network of n nodes represented as an n x n adjacency matrix graph, where the ith node is directly connected to the jth node if graph[i][j] == 1.    Some nodes initial are initially infected by malware. Whenever two nodes are directly connected, and at least one of those two nodes is infected by malware, both nodes will be infected by malware. This spread of malware will continue until no more nodes can be infected in this manner.    Suppose M(initial) is the final number of nodes infected with malware in the entire network after the spread of malware stops. We will remove exactly one node from initial.    Return the node that, if removed, would minimize M(initial). If multiple nodes could be removed to minimize M(initial), return such a node with the smallest index.    Note that if a node was removed from the initial list of infected nodes, it might still be infected later due to the malware spread.	O(n^2)	O(n)	We're expecting an optimized solution of time complexity O(n^2) and memory complexity O(n).	Union Find + Connected Components	Greedy	Divide And Conquer	Shell Sort	A	Correct! (DFS is also viable here).  Our approach uses Union Find data structure to represent the graph and find the connected components. We build the Union Find data structure with the given adjacency matrix, and use it to find the connected components in the graph.  We use a 'rootMalwareCount' vector to store the count of the number of malware nodes in each connected component. We then iterate over the initial list of malware nodes and find the connected component that contains the current node. If this connected component contains exactly one malware node, we check if the size of this connected component is greater than the current maxInfected value. If it is, we update maxInfected and nodeRemove.  Time Complexity: O(n^2 + nlogn) = O(n^2) (where n is the number of nodes in the graph) since we first use two nested loops to build the UnionFind structure, which takes O(n^2) time. Sorting the initial list of malware nodes takes O(nlogn) time, and iterating over the list takes O(n) time. In addition, finding the connected component of each node and checking its size takes O(n) time.  Memory Complexity: O(n) since we use a UnionFind data structure of size n, and two additional vectors of size n to store the count of malware nodes and the size of each connected component.	Very unlikely to succeed. This would make the locally optimal choice at each step in the hope of finding a global optimum.  Why it's suboptimal: Greedy algorithms do not always lead to the best solution, as the globally optimal choice may not always be the locally optimal one. It can also get stuck in local optima and fail to find the global optimum.	Not for this problem. Divide and conquer algorithms may not always produce the optimal solution, as the optimal solution may require combining the solutions of the sub-problems in a specific way.	Time to sort out a revision plan…incorrect!	This is the worst problem for any brute-force approach! Try all possible solutions until the correct one is found.  Why it's inefficient: The time complexity is typically exponential, which is very slow and impractical for large problem instances.  Time Complexity: O(2^n), where n is the size of the problem instance.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Minimize%20Malware%20Spread.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Minimize%20Malware%20Spread.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Minimize%20Malware%20Spread.java	https://youtu.be/01ulSaglaE4