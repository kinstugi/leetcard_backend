Pack Name	Topic	Problem Name	Problem Link	Question Statement	Time Complexity	Memory Complexity	Question	A	B	C	D	Correct	Message Response For A	Message Response For B	Message Response For C	Message Response For D	Brute Force Approach	C++ Answer	Python Answer	Java Answer	English Solution Video
Algorithms Pack	Sorting	Sort An Array	https://leetcode.com/problems/sort-an-array/	Given an array of integers *nums*, sort the array in ascending order and return it.<br><br>You must solve the problem **without using any built-in functions** in *O(n log(n))* time complexity and with the smallest space complexity possible.	O(n log(n))	O(1)	We're expecting an optimized solution of:<br>Time Complexity: **O(n log(n))**<br>Memory Complexity: **O(1)**	Selection Sort	Counting Sort	Insertion Sort	Shell Sort	B	**Suboptimal!**<br><br>**Count sort** is better here.<br><br>**Selection sort** has a time complexity of *O(n^2)*, which makes it inefficient for large arrays.	**Correct!**<br><br>**Counting sort** works by counting the number of occurrences of each value in the input array and then using this information to reconstruct the sorted array.<br><br>We first add a large number to all elements in the input array to ensure that all values are non-negative. We then find the max element in the array to determine the size of the counting array.<br><br>We create a counting array of size (maxValue+1) and use it to count the number of occurrences of each element in the input array, and update it by iterating over the counting array and reconstructing the sorted array.<br><br>Time Complexity: **O(n)** or **O(n+maxValue)**, where n is the size of the input array and maxValue is the maximum value in the input array.<br>Memory Complexity: **O(1)**, as we need to create a counting array of size (maxValue+1).	**Incorrect!**<br><br>**Count sort** would be better here.  **Insertion sort** has a time complexity of O(n^2) in the worst case, and it requires shifting many elements to insert an element in its correct position, which makes it less efficient than other sorting algorithms.	It's not as efficient as the **count sort**.<br><br>Although shell sort has a better average case time complexity than other O(n^2) algorithms, its worst-case time complexity is still O(n^2), which makes it less efficient than other sorting algorithms with better worst-case time complexity.	The bubble sort is a viable brute-force solution. Obviously, this is almost **never** the optimal answer!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Sort%20an%20Array.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Sort%20an%20Array.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Sort%20an%20Array.java	https://youtu.be/rEBj4QtGbZg
Algorithms Pack	Sorting	Minimum Absolute Difference	https://leetcode.com/problems/minimum-absolute-difference/	Given an array of **distinct** integers *arr*, find all pairs of elements with the minimum absolute difference of any two elements.<br><br>Return a list of pairs in ascending order(with respect to pairs), each pair *[a, b]* follows:<br>1) *a, b* are from *arr*<br>2) *a < b*<br>3) *b - a* equals to the minimum absolute difference of any two elements in *arr*	O(n log(n))	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n log(n))**<br>Memory Complexity: **O(n)**	Greedy	Bubble Sort	Divide And Conquer	Sorting + Linear Scanning	D	**Not ideal!**<br><br>We'd start with the first element in the array and compare it to *all other elements* to find the minimum difference. Then, move to the next element and repeat the process.<br><br>However, this approach has a time complexity of O(n^2), and doesn't guarantee that we will find all pairs with the minimum absolute difference.	**No.**<br><br>This approach is inefficient, with a time complexity of O(n^2) in the worst case.<br><br>Additionally, sorting the array is **unnecessary** for this problem, and may result in incorrect answers if there are duplicate elements in the array.	**Suboptimal**<br><br>D&C may not necessarily give the correct answer, because the minimum absolute difference could be between two elements in different subarrays.<br><br>For example, consider the input *[1, 3, 5, 9, 12]*. If we divide this array into subarrays *[1, 3]* and *[5, 9, 12]*, we will miss the minimum absolute difference between 3 and 5.	**Correct!**<br><br>We can sort the array in ascending order, and then compare adjacent elements to find the minimum absolute difference. We can then iterate through the sorted array again and return all pairs that have this difference.<br><br>Time Complexity: O(n log(n))<br>Memory Complexity: O(n)	We can iterate through all possible pairs of elements in the array with nested loops and calculate their absolute difference.  We can then find the minimum absolute difference and return all pairs that have this difference.<br><br>However, this approach has a time complexity of O(n^2), which is not efficient for large arrays.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Minimum%20Absolute%20Difference.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Minimum%20Absolute%20Difference.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Minimum%20Absolute%20Difference.java	https://youtu.be/EAmEH3ZpRMw
Algorithms Pack	Sorting	Largest Perimeter Triangle	https://leetcode.com/problems/largest-perimeter-triangle/	Given an integer array *nums*, return the **largest perimeter of a triangle with a non-zero area, formed from three of these lengths**. If it is impossible to form any triangle of a non-zero area, return *0*.	O(n log(n))	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n log(n))**<br>Memory Complexity: **O(n)**	Sorting + Greedy	Hash Table	Binary Search	Dynamic Programming	A	**Correct!**<br><br>We can sort the input array in decreasing order and then traverse the sorted array from left to right, considering each element as the longest side of a potential triangle. For a triangle to be formed, the **sum of the other two sides must be greater than the longest side**.<br><br>If we find three consecutive elements that satisfy this condition, we can return their sum as the largest perimeter. If no such triplet is found, we return *0*.<br><br>Time Complexity: **O(n log(n))**<br>Memory Complexity: **O(n)**	**Weak**<br><br>The idea would be to create a hash table to store the counts of each integer in the input array. Iterate through all possible pairs of integers and check if there exists a third integer such that they form a valid triangle - returning the maximum perimeter found.<br><br>This approach has a time complexity of O(n^2), which is better than the brute-force approach, but worse than the optimal solution. It also requires extra space for the hash table.	Binary search **doesn't work as well** for this problem.<br><br>This approach has a time complexity of *O(n^2 log(n))*, which is worse than the optimal solution. It also requires sorting and binary search operations.	**Not ideal.**<br><br>This is far too **expensive** an approach, both in terms of time and memory complexity!<br><br>To briefly explain a DP approach: we create a 2D array *dp[i][j]* where *dp[i][j]* represents the largest valid triangle that can be formed using the values from index *i* to index *j* in the input array.<br><br>Initialize the diagonal values to *0*, and then fill in the rest of the array by checking each possible triangle that can be formed from values in the subarray. Return the largest value in the array.	Generate all possible triplets of integers from the input array and check if they form a valid triangle, while keeping track of the maximum perimeter found so far.<br><br>Why it's suboptimal:<br>This approach has a time complexity of O(n^3), which is very inefficient for large input sizes.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Largest%20Perimeter%20Triangle.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Largest%20Perimeter%20Triangle.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Largest%20Perimeter%20Triangle.java	https://youtu.be/qklQggMJANk
Algorithms Pack	Sorting	Array Partition	https://leetcode.com/problems/array-partition/	Given an integer array *nums* of *2n* integers, group these integers into *n* pairs *(a1, b1), (a2, b2), ..., (an, bn)* such that the sum of *min(ai, bi)* for all *i* is **maximized**. Return the **maximized sum**.	O(n log(n))	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n log(n))**<br>Memory Complexity: **O(n)**	Prefix Sum	Trie	Sorting	Divide And Conquer	C	**Not** really recommended here!	Trie?<br><br>**Try again!**	**Correct!**<br><br>Firstly, sort the given array 'nums' in non-decreasing order. We can then group the numbers in pairs *(a, b)* by iterating over the sorted array with a step of 2, and adding up the minimum of each pair to a running sum.<br><br>Since our array is sorted, the smaller number of each pair will always be the first element of the pair (i.e., *ai = nums[i]* and *bi = nums[i+1]*).<br><br>Our approach maximizes the sum of the smaller numbers since we pair them with larger numbers in the array. We can see that any other pairing strategy would result in a smaller sum of minimums since we would be pairing larger numbers with each other, resulting in larger minimums.<br><br>Time Complexity: O(n log(n))<br>Memory Complexity: O(n)	**Incorrect**<br><br>Sorting is a more intuitive approach, since D&C may fail to consider the possibility of larger minimums that can be obtained by pairing elements from different halves.	The brute-force here is truly terrible.<br><br>We'd generate all possible pairings of the given 2n integers, compute the sum of minimums for each pairing, and return the maximum sum.<br><br>Why it's suboptimal:<br>generating all possible pairings of the given integers takes O((2n)!) time. This is prohibitively slow for large values of n, making this approach impractical.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Array%20Partition.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Array%20Partition.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Array%20Partition.java	https://youtu.be/B7zV0naH16s
Algorithms Pack	Sorting	Wiggle Sort	https://leetcode.com/problems/wiggle-sort/	Given an integer array *nums*, reorder it such that *nums[0] <= nums[1] >= nums[2] <= nums[3]...*.<br><br>You may assume the input array always has a valid answer.	O(n log(n))	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n log(n))**<br>Memory Complexity: **O(n)**	Careful Sort + Swap Operations	Bubble Sort	Selection Sort	Insertion Sort	A	**Correct!**<br><br>Method:<br>We sort the input array in ascending order, and swap all adjacent elements (starting from index *1*!) with their right neighbor. This will produce the wiggled order expected.<br><br>If we start with a sorted array in ascending order, the smallest elements will be placed in the even positions (0, 2, 4,...) and the largest elements will be placed in the odd positions (1, 3, 5,...). When we swap the adjacent elements, we are essentially swapping the smallest element in an even position with its neighboring larger element in an odd position, which guarantees the wiggle pattern.<br><br>Time Complexity: **O(n log(n))**, which is dominated by the sorting step<br><br>Memory Complexity: **O(n)**	**Inefficient** ompared to sort and swap!<br><br>Bubble sort can be used to create the wiggled pattern. This would take **O(n^2)** time complexity in the worst case, and the space complexity would be O(1). However, it takes more time to sort the input array than the optimal solution in most cases, except when the input array is already sorted in ascending order.<br><br>Additionally, the problem constraints do not require a fully sorted array, but rather a wiggled array.	**Inefficient** compared to sorting and swapping!<br><br>Selection sort can be used to create the wiggled pattern. This would take O(n^2) time complexity in the worst case, and the space complexity would be O(1).<br><br>However, it takes more time to sort the input array than the optimal solution in most cases, except when the input array is already sorted in ascending order.<br><br>Additionally, the problem constraints do not require a fully sorted array, but rather a wiggled array. This sort it more general-purpose and sorts the array completely, so it is not as efficient for the specific problem.	**Inefficient** compared to sorting and swapping!<br><br>Insertion sort can be used to create the wiggled pattern. This would take O(n^2) time complexity in the worst case, and the space complexity would be O(1).<br><br> However, it takes more time to sort the input array than the optimal solution in most cases, except when the input array is already sorted in ascending order.<br><br>Additionally, the problem constraints do not require a fully sorted array, but rather a wiggled array. This sort it more general-purpose and sorts the array completely, so it is not as efficient for the specific problem.	A naive approach is to generate all possible permutations of the input array and check each permutation to see if it satisfies the wiggle condition.<br><br>Time Complexity: O(n!) to generate all permutations, with O(n) time complexity to check each permutation.<br><br>Space Complexity: O(n!) to store all permutations.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Wiggle%20Sort.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Wiggle%20Sort.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Wiggle%20Sort.java	https://youtu.be/yejhNBoG2fg
Algorithms Pack	Sorting	Eliminate Maximum Number of Monsters	https://leetcode.com/problems/eliminate-maximum-number-of-monsters/	You're defending your city from a group of *n* monsters. You're given a **0-indexed** integer array *dist* of size *n*, where *dist[i]* is the **initial distance** in km of the *ith* monster from the city.<br><br>The monsters walk toward the city at a **constant** speed. The speed of each monster is given to you in an integer array *speed* of size *n*, where *speed[i]* is the speed of the *ith* monster in km per minute.<br><br>You have a weapon that, fully charged, eliminates a **single** monster. The weapon is fully charged at the very start, but takes 1 minute to charge.<br><br>You lose when any monster reaches your city. If a monster reaches the city at the exact moment the weapon is fully charged, it counts as a **loss**, and it all ends.<br><br>Return the **maximum** *number of monsters that you can eliminate before losing*, or *n* **if you can eliminate all the monsters before they reach the city**.	O(n log(n))	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n log(n))**<br>Memory Complexity: **O(n)**	Binary Search	Count Sort	Greedy + Sorting	Brute-Force	C	Unfortunately, this **BS** approach won't help us with these monsters.	**Not quite!**<br><br>However, if you mean **sort** after calculating the arrival time, and then **counting** the monsters, this is a good answer! However, **Greedy + Sorting** is a better choice here.	**Correct!**<br><br>Our approach calculates the arrival time for each monster based on their distance and speed, and sorts times in ascending order. We then iterate through the sorted array and keep count of how many monsters we can eliminate before any one of them reaches the city.<br><br>We stop counting when either we have eliminated all the monsters or when the number of monsters we have encountered exceeds their earliest arrival time. The idea is to prioritize eliminating the monsters with the earliest arrival time.<br><br>Time Complexity: **O(n log(n))**, due to the sorting operation, where *n* is the number of monsters.<br>Space Complexity: **O(n)**, because of the extra vector used to store the arrival times. A *priority queue* is a good alternative answer for this problem.	**Not quite!**<br><br>This works, but there is at least one better approach.	Check every possible elimination combination with loops.<br><br>Try eliminating each monster one by one, keeping track of the remaining monsters' positions and arrival times. Stop when any monster reaches the city.<br><br>Time Complexity: O(n^2), so there are superior approaches!<br><br>Space Complexity: O(n)	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Eliminate%20Maximum%20Number%20of%20Monsters.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Eliminate%20Maximum%20Number%20of%20Monsters.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Eliminate%20Maximum%20Number%20of%20Monsters.java	https://youtu.be/GYZ0OC98KPo
Algorithms Pack	Sorting	Maximize Sum Of Array After K Negations	https://leetcode.com/problems/maximize-sum-of-array-after-k-negations/	Given an integer array *nums* and an integer *k*, modify the array in the following way:<br>Choose an index *i* and replace *nums[i]* with *-nums[i]*.<br><br>You should apply this process exactly *k* times. You may choose the same index *i* multiple times.<br><br>Return the largest *possible sum of the array after modifying it in this way*.	O(n log(n))	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n log(n))**<br>Memory Complexity: **O(n)**	Divide & Conquer	Sorting + Greedy	Priority Queue	Dynamic Programming	B	This approach would likely **fail** when *k* is large enough to completely negate the smaller part, leaving the remaining part with only positive numbers.	**Correct!**<br><br>First sort the given array to easily apply the negation operation to the smallest values in the array. Then, we iterate through the array and negate the values with a negative sign *k* times.<br><br>In each iteration, we decrement *k* and negate the current number if it's negative. While iterating, we also keep track of the **minimum value** that has been negated so far.<br><br>After the iterations, if *k* is an odd number, we subtract **twice the minimum value** from the sum to ensure that the remaining *k* negations are applied to a positive number in the array, thus maximizing the final sum.<br><br>Time Complexity: **O(n log(n))** due to the sorting operation<br>Memory Complexity: O(n)	**Expensive** in terms of space complexity. We can use a *priority queue* to keep track of the smallest values in the array, negate them *k* times and add them to the sum.<br><br>This approach is suboptimal because it does not consider the fact that negating a larger positive number can contribute more to the sum than negating a smaller negative number.	**Not ideal.**<br><br>We can use DP to keep track of the maximum sum that can be obtained by applying a certain number of negations to a certain index in the array. We can then use this information to compute the maximum sum by applying *k* negations to the appropriate indices.<br><br>This approach is suboptimal because it has a very high time complexity.	We can try all possible combinations of *k* negations and calculate the sum of each combination. Finally, we can return the maximum sum obtained among all the combinations.<br><br>This approach is suboptimal because the time complexity is O(2^nk) and thus impractical for even moderate values of n and k.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Maximize%20Sum%20Of%20Array%20After%20K%20Negations.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Maximize%20Sum%20Of%20Array%20After%20K%20Negations.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Maximize%20Sum%20Of%20Array%20After%20K%20Negations.java	https://youtu.be/_5gN7lQJ3UE
Algorithms Pack	Sorting	Shortest Unsorted Continuous Subarray	https://leetcode.com/problems/shortest-unsorted-continuous-subarray/	Given an integer array *nums*, you need to find one **continuous subarray** such that if you only sort this subarray in non-decreasing order, then the whole array will be sorted in non-decreasing order.<br><br>Return the **shortest such subarray and output its length**.	O(n)	O(1)	We're expecting an optimized solution of:<br>Time Complexity: **O(n)**<br>Memory Complexity: **O(1)**	Sorting + Greedy	Hash Table	Stack	Linear Scanning	D	**Not the most efficient approach**.<br><br>It's entirely possible to use linear scanning instead for this problem.	**Suboptimal** due to memory complexity.<br><br>The approach has a time complexity of **O(n)**, but requires **extra memory to store the hash table**.<br><br>The approach itself is relatively easy to understand: use a hash table to keep track of the indices of each element in the original array, and then find the leftmost and rightmost indices that need to be sorted by comparing the original array with the sorted array.	**Inferior to linear scanning** due to the extra space required.<br><br>A stack-based approach would have a time complexity of O(n), but requires extra memory to store the stack - plus, the implementation can be tricky!	**Correct!**<br><br>We scan the array in each direction to find the leftmost and rightmost indices that need to be sorted.<br><br>During the first pass, we keep track of the max value seen so far. If the current element is less than the max, it means that the current element needs to be sorted, so we update the right index.<br><br>During the second pass, we keep track of the min value seen so far. If the current element is greater than the min, then the current element needs to be sorted, so we update the left index.<br><br>Finally, we return the length of the subarray by computing the difference between the right and left indices plus one. If right is still *-1*, it means that the array is already sorted, so we return *0*.<br><br>Time Complexity: **O(n)** since we only need to scan the array twice.<br>Memory Complexity: **O(1)** since we only use a constant amount of extra memory to store the left and right indices, and the max and min values seen so far.	Check every possible subarray to see if it's sorted, and then return the length of the shortest such subarray.<br><br>However, this approach has a time complexity of O(n^3), which is very inefficient and would not be able to handle large input sizes.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Shortest%20Unsorted%20Continuous%20Subarray.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Shortest%20Unsorted%20Continuous%20Subarray.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Shortest%20Unsorted%20Continuous%20Subarray.java	https://youtu.be/DjMdSYUJLB4
Algorithms Pack	Sorting	Most Profit Assigning Work	https://leetcode.com/problems/most-profit-assigning-work/	You have *n* jobs and *m* workers. You are given three arrays: *difficulty*, *profit*, and *worker* where:<br>1) *difficulty[i]* and *profit[i]* are the difficulty and the profit of the *ith* job<br>2) *worker[j]* is the ability of *jth* worker (i.e., the *jth* worker can only complete a job with difficulty at most *worker[j]*).<br><br>Every worker can be assigned **at most one job**, but one job can be **completed multiple times**.<br>For example, if three workers attempt the same job that pays *$1*, then the total profit will be *$3*. If a worker cannot complete any job, their profit is *$0*.<br><br>Return the maximum profit we can achieve after assigning the workers to the jobs.	O((n+m) log(n))	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O((n+m) log(n))**<br>Memory Complexity: **O(n)**	Dynamic Programming	Binary Search	Sorting + Greedy	Union Find	C	**Not ideal!**<br><br>This approach is suboptimal because the time complexity is high (O(nm^2)).<br><br>It requires us to fill in a 2D array, which can be computationally expensive for large inputs.	**Incorrect!**<br><br>Binary search won't work for this problem - at least, not as optimally as *sorting+greedy*.	**Correct!**<br><br>We first create a vector of *jobs* pairing their difficulty and profit. Sort *jobs* by difficulty **and** the *workers* ability in **ascending order**. <br><br>Then, we iterate through *workers*, and for each worker, we assign the job(s) that they can complete with the highest profit. We keep track of the maximum profit seen so far, adding it to the total profit - and repeat this process for all workers.<br><br>At each iteration, we advance the job pointer only if the current job difficulty is smaller or equal to the worker's ability, ensuring that we assign jobs with the smallest difficulty and maximum profit to workers that can complete them.<br><br>Time Complexity: **O(n log(n))**, due to the sorting step. The loop over the workers takes O(m), where m is the length of the worker array, so the overall time complexity is O((n+m) log(n)).<br>Memory Complexity: **O(n)**	Honestly, even if this manages to somehow work, there is clear **over-engineering** at play here.  **Sorting+Greedy** is the best approach!	We can try every possible combination of job assignments and calculate the total profit, returning the maximum profit.<br><br>The time complexity for this approach is very high (O(m^n)), where n is the number of jobs and m is the number of workers - and not feasible for large inputs.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Most%20Profit%20Assigning%20Work.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Most%20Profit%20Assigning%20Work.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Most%20Profit%20Assigning%20Work.java	https://youtu.be/Jf__9kVxhBY
Algorithms Pack	Sorting	Reduction Operations To Make The Array Elements Equal	https://leetcode.com/problems/reduction-operations-to-make-the-array-elements-equal/	Given an integer array *nums*, your goal is to make all elements in *nums* equal. To complete one operation, follow these steps:<br>1) Find the **largest** value in *nums*. Let its index be *i* (**0-indexed**) and its value be *largest*. If there are multiple elements with the largest value, pick the smallest *i*.<br>2) Find the **next largest** value in *nums* **strictly smaller** than *largest*. Let its value be *nextLargest*.<br>3) Reduce *nums[i]* to *nextLargest*.<br><br>Return the **number of operations to make all elements in** *nums* **equal**.	O(n log(n))	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n log(n))**<br>Memory Complexity: **O(n)**	Backtracking	Recursion	Sorting + Counting	Binary Search	C	*Backtracking* would be **very inefficient**, especially if the array has many duplicate values or if it is already close to being equal.<br><br>Moreover, it is not guaranteed to find the optimal solution.	Wildly **suboptimal**.<br><br>In this approach, we can recursively try all possible pairs of elements in the array and reduce them to the smaller value. We repeat this process until all elements in the array become equal.<br><br>However, this is **even slower than the naive brute-force approach** since it involves recursion. The time complexity of this approach is O(n^n), so far from a feasible solution for any input size.	**Correct!**<br><br>First, we sort the given array in descending order. Then, we traverse the array from the second element to the end, and for each element, we compare it with the previous element.<br><br>If not equal, we add the index of the current element to the running count of operations. We add the index of the current element because when we reduce the current element to the value of the previous element, we are actually increasing the number of elements that have the value of the previous element. Thus, we need to keep track of how many elements we have processed so far.<br><br>Time Complexity: **O(n log(n))**, due to the sorting operation.<br>Memory Complexity: **O(n)**	**No!**<br><br>This would look and sound like a wrong guess in a real interview too!	SImply check all values sequentially.<br><br>Start with the first element of the array and reduce all other elements to its value. Then we move to the next element and repeat the process until all elements in the array are equal.<br><br>However, this approach takes O(n^2) time because for each element, we have to compare it with every other element in the array.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Reduction%20Operations%20to%20Make%20the%20Array%20Elements%20Equal.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Reduction%20Operations%20to%20Make%20the%20Array%20Elements%20Equal.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Reduction%20Operations%20to%20Make%20the%20Array%20Elements%20Equal.java	https://youtu.be/IDNxRt1TGYI
Algorithms Pack	Binary Search	Binary Search	https://leetcode.com/problems/binary-search/	Given an array of integers *nums* which is sorted in ascending order, and an integer *target*, write a function to search *target* in *nums*. If *target* exists, then return its index. Otherwise, return *-1*.<br><br>You must write an algorithm with **O(log(n))** runtime complexity.	O(log(n))	O(1)	We're expecting an optimized solution of:<br>Time Complexity: **O(log(n))**<br>Memory Complexity: **O(1)**	Linear Search	Kruskal's Algorithm	Binary Search	Count Sort	C	**The clue is in the name**! Use a Binary Search!	**No**, binary search!	**We really hope you got this one correct!**<br><br>Of course, **binary search** is going to be the correct answer for a problem named *binary search*!	**No!** Binary Search!	We can simply traverse the array and check if the current element is equal to the target.<br><br>If it is, we return the current index. Otherwise, we move to the next element. We continue this process until we reach the end of the array.<br><br>However, this approach has a linear time complexity of O(n), which won't satisfy the requirements of the question.  Can you spot the right answer to **binary search**?  You'll need to use all of your braincells for this puzzle!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Binary%20Search.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Binary%20Search.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Binary%20Search.java	https://youtu.be/iUUiAwlPb1M
Algorithms Pack	Binary Search	Find First And Last Position Of Element In Sorted Array	https://leetcode.com/problems/find-first-and-last-position-of-element-in-sorted-array/	Given an array of integers *nums* sorted in non-decreasing order, find the starting and ending position of a given *target* value.<br><br>If *target* is not found in the array, return *[-1, -1]*.<br><br>You must write an algorithm with **O(log(n))** runtime complexity.	O(log(n))	O(1)	We're expecting an optimized solution of:<br>Time Complexity: **O(log(n))**<br>Memory Complexity: **O(1)**	Two Pointers	Binary Search	Hash Table	Linear Search	B	**No**<br><br>This approach has a time complexity of O(n), which does not meet the requirement of O(log(n)) in the problem statement.<br><br>For the curious, we can achieve this by initializing two pointers at the start of the array, and move them towards the middle until the target element is found. Once found, move one pointer left and the other right to find the first and last occurrences of the element.	**Correct!**<br><br>Our C++ approach here uses the STL function *equal_range* which performs binary search to find the first and final occurrence of an specific target in a sorted range of elements.  This function returns a pair of iterators, which can be converted to indices.<br><br>We apply *equal_range* on the given vector of integers *nums* and the *target* value. If the first iterator of the returned pair is pointing to the end of the vector, we return [-1, -1]. Otherwise, we extract the indices of the iterators from *nums*.<br><br>Finally, we check if the value at the first index is equal to the target value. If not, we return *[-1, -1]*. Otherwise, we return the indices of the first and the last occurrence of the target value in the nums vector.<br><br>Time Complexity: **O(log(n))**<br>Memory Complexity: O(1), using constant extra memory to store the indices and the result vector.	This approach requires creating a **hash table** of the entire array, which has a space complexity of O(n).<br><br>Additionally, finding the first and last occurrences of the element has a time complexity of O(n), making the overall approach suboptimal with a time complexity of O(n).<br><br>How does it work?<br>We create a hash table of the array with element values as keys and indices as values. Find the index of the target element in the hash table, and then expand the search to find the first and last occurrences of the element.	**No!**<br><br>The brute-force approach has a time complexity of **O(n)**, which does not meet the requirement of O(log(n)) in the problem statement.	Simply iterate through the array and find the first and last occurrence of the target element.<br><br>However, this approach has a time complexity of O(n), which does not meet the requirement of O(log(n)) in the problem statement.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Find%20First%20and%20Last%20Position%20of%20Element%20in%20Sorted%20Array.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Find%20First%20and%20Last%20Position%20of%20Element%20in%20Sorted%20Array.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Find%20First%20and%20Last%20Position%20of%20Element%20in%20Sorted%20Array.java	https://youtu.be/vzHQBpMyU3o
Algorithms Pack	Binary Search	Find Right Interval	https://leetcode.com/problems/find-right-interval/	You are given an array of *intervals*, where *intervals[i] = [starti, endi]* and each *starti* is unique.<br><br>The right interval for an interval *i* is an interval *j* such that *startj >= endi* and *startj* is **minimized**. Note that *i* may equal *j*.<br><br>Return an array of **right interval** indices for each interval *i*. If no **right interval** exists for interval *i*, then put *-1* at index *i*.	O(n log(n))	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n log(n))**<br>Memory Complexity: **O(n)**	Binary Search + Hash Table	Single Hash Table	Sorting + Linear Scan	Nested Loops	A	**Correct!** (A heap approach is also possible, with similar complexity!)<br><br>Our approach stores the start index of each interval, where the key is the start point, and the value is the index of the interval in the original array. Then we iterate through each interval in the array and use a binary search to find the index of the smallest start point that is greater than or equal to the end point of the current interval.<br><br>If such an interval is found, its index is stored in the resulting array at the corresponding index of the current interval. If not, the resulting array is set to -1 at the index of the current interval.<br><br>Time Complexity: O(n log(n)), since the binary search also takes O(log(n)) time, and we perform it n times<br>Memory Complexity: **O(n)**	No, **not quite!**<br><br>Although this approach can be faster than the nested loop approach in some cases, it still has a worst-case time complexity of O(n^2) if all intervals have the same start point.	**Suboptimal!**<br><br>A binary search and hash table is better.<br><br>This still has a time complexity of O(n^2) in the worst case because the linear search may have to examine all the intervals in the sorted array.	This is very rarely optimal - and **is not optimal** here!<br><br>This is basically the brute-force approach.	Iterate through each interval and, for each interval, iterate through all subsequent intervals to find the right interval (if it exists).<br><br>This approach has a time complexity of O(n^2), which is very inefficient for large values of n. For example, if n = 10^4, this approach would require 100 million iterations.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Find%20Right%20Interval.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Find%20Right%20Interval.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Find%20Right%20Interval.java	https://youtu.be/erDK63JK14A
Algorithms Pack	Binary Search	Valid Triangle Number	https://leetcode.com/problems/valid-triangle-number/	Given an integer array *nums*, **return the number of triplets chosen from the array that can make triangles if we take them as side lengths of a triangle**.	O(n^2)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n^2)**<br>Memory Complexity: **O(n)**	Binary Search	Dynamic Programming	Hash Table	Sorting + Two Pointers	D	**Not ideal here.**<br><br>This approach would have a time complexity of O(n^2 log(n)), which is better than brute force but still not optimal. It requires sorting the array and binary search for every combination of two sides, which can take a lot of time for larger inputs.	**Suboptimal**<br><br>Such an approach would have a time complexity of O(n^2), but it requires additional memory to store the dynamic programming table, which can be problematic for large input sizes.	**Suboptimal** due to the high space complexity!<br><br>The idea is to use a hash table to count the frequency of each number in the array, and then for every possible pair of sides, check if the sum of those sides is less than or equal to any other number in the array.<br><br>Although this approach has a time complexity of O(n^2), it requires additional memory to store the hash table, which can be problematic for large input sizes.	**Correct!**<br><br>Here, we use the two-pointer approach to find the number of triplets that can make triangles. We sort the array and iterate through each element, setting two pointers, one pointing to the next element and the other pointing to the end of the array. We then move the pointers inwards, comparing the sum of the elements at the two pointers with the current element.<br><br>If the sum is greater than the current element, we increment the count by the difference between the right pointer and left pointer, move the right pointer inward, and repeat the process. If the sum is not greater than the current element, we move the left pointer inward and repeat the process. We return the final count.<br><br>Time Complexity: **O(n^2)**, due to our nested loop.<br>Memory Complexity: **O(1)**, using a constant amount of extra memory for the pointers and the count.	Simpy, iterate through all possible triplets of numbers in the array and check if they can form a triangle.<br><br>However, this approach has a time complexity of O(n^3), which makes it inefficient for larger input sizes.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Valid%20Triangle%20Number.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Valid%20Triangle%20Number.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Valid%20Triangle%20Number.java	https://youtu.be/z0MrIl0HM7Q
Algorithms Pack	Binary Search	Arranging Coins	https://leetcode.com/problems/arranging-coins/	You have *n* coins and you want to build a staircase with these coins. The staircase consists of *k* rows where the *ith* row has exactly *i* coins. The last row of the staircase **may be** incomplete.<br><br>Given the integer *n*, **return the number of complete rows of the staircase you will build**.	O(log(n))	O(1)	We're expecting an optimized solution of:<br>Time Complexity: **O(log(n))**<br>Memory Complexity: **O(1)**	Binary Search	Linear Search	Hash Table	Topological Sort	A	**Correct, although there **is** a formula-based approach too (not mentioned here).<br><br>Our approach uses a binary search to find the number of rows that can be fully constructed using the given coins. We first define a helper function *possible* that takes in the number of coins and rows, returning whether it's **possible** to construct *row* number of rows with *coins* number of coins.<br><br>We start with the entire range of possible rows i.e., *0* to *coins*, and then perform binary search. For each iteration, we calculate the mid row and check if it's possible to construct mid rows using the given number of coins. If it is possible, we update the answer and move to the right half of the range. Otherwise, we move to the left half of the range.<br><br>Time Complexity **O(log(n))**<br>Memory Complexity: **O(1)**, since we're only using constant extra space	**Suboptimal.**<br><br>This brute-force approach has a time complexity of **O(n)**, which is not optimal for large values of n.	**Suboptimal.**<br><br>This would be a very expensive approach in terms of space complexity.	**No chance!**	We iterate over all possible rows and return the maximum number of rows such that the total number of coins in the rows is less than or equal to n.<br><br>However, this approach has a time complexity of O(n), which is not optimal for large values of n.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Arranging%20Coins.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Arranging%20Coins.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Arranging%20Coins.java	https://youtu.be/xiF8dy67tRM
Algorithms Pack	Binary Search	Find The Smallest Divisor Given A Threshold	https://leetcode.com/problems/find-the-smallest-divisor-given-a-threshold/	Given an array of integers *nums* and an integer *threshold*, we will choose a positive integer *divisor*, divide all the array by it, and sum the division's result. Find the **smallest** *divisor* such that the result mentioned above is less than or equal to *threshold*.<br><br>Each result of the division is rounded to the nearest integer greater than or equal to that element.  For example:<br>*7/3 = 3*<br>*10/2 = 5)*<br><br>The test cases are generated so that there will be an answer.	O(n log(max(nums)))	O(1)	We're expecting an optimized solution of:<br>Time Complexity: **O(n log(max(nums)))**<br>Memory Complexity: **O(1)**	Binary Search	Priority Queue	Sorting	Hash Table + Linear Scan	A	**Correct!**<br><br>We use binary search to find the smallest divisor which satisfies the condition mentioned in the problem statement. We first find the maximum element in the array, which is an upper bound on the possible divisors.<br><br>We then use binary search to find the smallest divisor. For a given divisor, we find the sum of the ceiling of each element divided by it. If this sum is less than or equal to the threshold, we update our answer and move to a smaller divisor. Otherwise, we move to a larger divisor.<br><br>Time Complexity: **O(n log(max(nums)))**, where n is the size of the nums vector, and max(nums) is the maximum element in the nums vector.<br>Memory Complexity: **O(1)**, as we use constant extra space in the form of a few variables to store our intermediate results.	**Not quite!**<br><br>This method uses a priority queue to keep track of the elements whose ceiling when divided by the divisor is the largest, and keep popping elements until the sum of the ceiling of each element divided by the divisor is less than or equal to the threshold.<br><br>Weakness: Less efficient than a binary search!<br><br>Time Complexity: O(n log(n) + n log(max(nums)))<br>Memory Complexity: O(n)	**Less efficient** than other solutions.<br><br>Method: Sort the array in descending order and try all possible divisors from 1 to the maximum element in the array.<br><br> Weakness: less efficient than binary search!<br><br>Time Complexity: **O(n log(n) + n log(max(nums)))**<br>Memory Complexity: O(1)	This is **not at all** an optimal approach for this problem.	Iterate over all possible divisors from *1* to *max(nums)* and compute the sum for each divisor. Return the smallest divisor which satisfies the condition mentioned in the problem statement.<br><br>Time Complexity: O(max(nums) * n) where n is the size of the nums vector.  Can you improve on this, without sacrificing the O(1) memory complexity?	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Find%20the%20Smallest%20Divisor%20Given%20a%20Threshold.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Find%20the%20Smallest%20Divisor%20Given%20a%20Threshold.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Find%20the%20Smallest%20Divisor%20Given%20a%20Threshold.java	https://youtu.be/oCaiNNOcVLs
Algorithms Pack	Binary Search	Minimum Number Of Days To Make M Bouquets	https://leetcode.com/problems/minimum-number-of-days-to-make-m-bouquets/	You are given an integer array **bloomDay**, an integer *m* and an integer *k*.<br><br>You want to make *m* bouquets. To make a bouquet, you need to use *k* **adjacent flowers** from the garden.<br><br>The garden consists of *n* flowers, the *ith* flower will bloom in the *bloomDay[i]* and then can be used in **exactly one** bouquet.<br><br>Return the **minimum number of days** you need to wait to be able to make *m* bouquets from the garden. If it is impossible to make *m* bouquets return *-1*.	O(n log(n))	O(1)	We're expecting an optimized solution of:<br>Time Complexity: **O(n log(n))**<br>Memory Complexity: **O(1)**	Sorting + Linear Search	Binary Search	Sliding Window	DFS	B	**Incorrect!**<br><br>Binary search is possible, and would be much better.<br><br>Sorting +Linear Search doesn't consider the possibility that there may be no *k* adjacent flowers available to make a bouquet. It also does not guarantee the minimum number of days required to make m bouquets.<br><br>The time complexity of this is O(n^2) due to the nested inner loop to check for previous bouquets.	Correct!<br><br>We can use binary search. Set lower bound to 1 and upper bound to 10^9, representing max days.<br><br>In each iteration, calculate the mid of the range, and check if making m bouquets in k adjacent flowers in *mid* days is possible.<br><br>If so, set the upper bound to mid-1 and save *mid* as the answer. Else, set lower bound to mid+1.<br><br>To check if m bouquets in k adjacent flowers in mid days is possible, iterate bloomDay, count adjacent flowers bloomed in mid days. Track the bouquets made and adjacent flowers used. Return false if bouquets < m, else return true.<br><br>Time Complexity: **O(n X log(max(bloomDay)))**, where n is bloomDay size.  Our loop runs log(max(bloomDay)) times, and each iteration checks m bouquets in O(n).<br><br>Memory Complexity: **O(1)**	**Suboptimal!**<br><br>This approach doesn't guarantee the minimum number of days required to make *m* bouquets, as the minimum *bloomDay* for each bouquet may not be optimal.<br><br>It also does not consider the possibility that there may not be k adjacent flowers available to make a bouquet.<br><br>The time complexity here is O(nmk) due to the nested loops for the sliding window and checking for previous bouquets.	Not the best approach!<br><br>While possible, a DFS approach would be **time-consuming**, especially when *k* and *m* are large.	We can enumerate all possible combinations of bouquets using k adjacent flowers from the garden and check if each combination is valid to make a bouquet.<br><br>Start by choosing the first k flowers as the first bouquet, the next k flowers as the second bouquet, and so on, until we have m bouquets. Then, we can check if each bouquet is valid by comparing the bloomDay of the flowers used in the bouquet.<br><br>The brute-force solution we are offering here would be particularly wasteful, with an exponential time complexity of O(n^k), since we are enumerating all possible combinations of k adjacent flowers.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Minimum%20Number%20of%20Days%20to%20Make%20m%20Bouquets.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Minimum%20Number%20of%20Days%20to%20Make%20m%20Bouquets.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Minimum%20Number%20of%20Days%20to%20Make%20m%20Bouquets.java	https://youtu.be/dHyF855ezUA
Algorithms Pack	Binary Search	Heaters	https://leetcode.com/problems/heaters/	**Winter is coming!** During the contest, your first job is to design a standard heater with a fixed warm radius to warm all the houses.<br><br>Every house can be warmed, as long as the house is within the heater's warm radius range.<br><br>Given the positions of *houses* and *heaters* on a horizontal line, return the **minimum radius standard of heaters so that those heaters could cover all houses**.<br><br>Notice that all the *heaters* follow your radius standard, and the warm radius will the same.	O(n log(n) + m log m)	O(1)	We're expecting an optimized solution of:<br>Time Complexity: **O(n log(n) + m log(m))**<br>Memory Complexity: **O(1)**	Greedy	Hash Table	Sorting + Binary Search	Dynamic Programming	C	**Not quite!**<br><br>This approachmay not consider heaters that are not the closest but still cover more houses. For example, if two heaters can each cover half the houses, but are farther away than the closest heater, this approach will miss them.<br><br>The approach for this answer?<br>1) Sort the heater positions in increasing order.<br>2) For each house, find the closest heater that is to the right of the house.<br>3) If there is no such heater, find the closest heater that is to the left of the house.<br>4) We then calculate the distance between the house and the heater, and take the maximum distance as the minimum radius required to cover all houses.	**Suboptimal!**<br><br>This would be far too expensive in terms of space complexity.	**Correct!**<br><br>This approach involves binary search to find the minimum radius required to cover all the houses using the heaters like so.<br>1) Sort the houses and heaters in ascending order.<br>2) Initialize the range of radius as 0 to 1e9 (a very large value).<br>3) Check if it is possible to cover all the houses with the given radius using the *possible()* function.<br>4) If so, then we update the end of the range to *mid-1* and update the radius to *mid*. Else, we update the start of the range to *mid+1*.<br>5) We repeat the process above until start becomes greater than end.<br>6) Finally, we return the minimum radius that can cover all the houses.<br><br>Time Complexity: **O(n log(n))**, where n is the maximum of the size of houses and heaters.<br>Memory Complexity: **O(1)**, since we are using constant extra space.	**Nope!**<br><br>This approach does not work for the given problem because the optimal placement of heaters is not dependent on the previous placements.<br><br>Each heater's placement is independent of the previous heaters' placement.	Check ALL possible heater radii, from 0 to the maximum distance between the leftmost and rightmost houses. We can then find the minimum radius that covers all the houses.<br><br>Why it's suboptimal: This approach has a time complexity of O(n^2), where n is the number of houses. As n can be as large as 10^4, this approach can take a long time to run and is not feasible for large inputs.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Heaters.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Heaters.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Heaters.java	https://youtu.be/XodnzXl_--A
Algorithms Pack	Binary Search	Missing Element in Sorted Array	https://leetcode.com/problems/missing-element-in-sorted-array/	We're given an integer array *nums* which is sorted in **ascending order**, with all of its elements being **unique**.<br><br>We're also given an integer *k*.<br><br>Return the *kth* missing number starting from the leftmost number of the array.	O(log(n))	O(1)	We're expecting an optimized solution of:<br>Time Complexity: **O(log(n))**<br>Memory Complexity: **O(1)**	Binary Search + Linear Search	Hash Table	Two Binary Searches	Binary Search	D	**Convoluted!**<br><br>Method: use binary search to find the index where the *kth* missing number should be, then perform a linear search from that index to find the kth missing number.<br><br>Weakness: while binary search can help us find the index where the kth missing number should be, performing a linear search from that index to find the kth missing number can be inefficient, especially if k is large.<br><br>Time Complexity: O(log(n)+ k), so try another approach!	**Incorrect!**<br><br>Method: create a hash table to store all the numbers in the array, then iterate through the array, checking for missing numbers in the hash table until we find the *kth* missing number.<br><br>Weakness: the time and space complexity are worse than binary search, both being O(n).	This involves **unnecessary complexity**!<br><br>Method: use binary search to find the index where the kth missing number should be, then use binary search again to find the kth missing number starting from that index.<br><br>Weakness: performing Binary Search twice is a **BS approach** for this problem. It gives a time complexity of *O(log(n) * log k)*, which is significantly worse than the optimal approach.	**Correct!**<br><br>*missing* calculates the number of missing elements from the leftmost element of the array up to our given index. *kth* returns the kth missing element starting from the given index.<br><br>We check if the number of missing elements from the last element of the array is less than *k*. If so, we can find the *kth* missing element starting from the last element. Otherwise, we perform **binary search** on the array to find the index where the *kth* missing element would be located.<br><br>Update search indices based on whether the number of missing elements up to the *mid* index is less than or greater than *k* - and keep track of the latest index where *number of missing elements <= k*.<br><br>Finally, we can use *kth* to return the kth missing element starting from the latest index where the number of missing elements is less than or equal to k.<br><br>Time Complexity: **O(log(n))**<br>Space Complexity: **O(1)**	**Linear search** can be used.  Just iterate through the array, counting the missing numbers until we find the kth missing number.<br><br>Time Complexity: O(n)<br>Space Complexity: O(1), since we don't need to store any extra data<br><br>However, there is a better approach!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Missing%20Element%20in%20Sorted%20Array.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Missing%20Element%20in%20Sorted%20Array.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Missing%20Element%20in%20Sorted%20Array.java	https://youtu.be/Ucvaef4ySnc
Algorithms Pack	Binary Search	Kth Smallest Number in Multiplication Table	https://leetcode.com/problems/kth-smallest-number-in-multiplication-table/	Nearly everyone has used the **Multiplication Table**. The multiplication table of size *m x n* is an integer matrix *mat* where *mat[i][j] == i X j* (**1-indexed**).<br><br>Given three integers *m*, *n*, and *k*, return the *kth* smallest element in the *m x n* multiplication table.	O(m log(nm))	O(1)	We're expecting an optimized solution of:<br>Time Complexity: **O(m log(mn))**<br>Memory Complexity: **O(1)**	Min-Heap	Binary Search	Sorting	Optimized Count Sort	B	**Suboptimal.**<br><br>The idea is to use a heap to store the elements in the multiplication table in increasing order. We would then pop k elements from the heap and return the kth smallest element.<br><br>This would have a time complexity of O(mnlog(m*n)) which is the same as the brute force approach!	**Correct!**<br><br>Our approach uses **binary search** to find the kth smallest element in the m x n multiplication table. We first set the start to *1* and the end to *m x n*, then we compute the *mid* value, and use a helper function *lessEqCount* to count the number of elements in the table less than or equal to *mid*. If the count is less than *k*, then we update the start to *mid + 1*. Otherwise, we update the *end* to *mid - 1* and update the answer to *mid*. We repeat this process until the start and end meet.<br><br>Time Complexity: O(m log (mn))<br>Memory Complexity: O(1)	This apporach would yield a **poor time complexity**, especially when compared to the binary search!	Not a good option: **binary search** is the best approach by far!	Simply iterate over all elements in the table and sort them, returning the *kth* smallest element. This would have a time complexity of O(mnlog(m*n)) which is too slow for large values of *m* and *n*.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Kth%20Smallest%20Number%20in%20Multiplication%20Table.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Kth%20Smallest%20Number%20in%20Multiplication%20Table.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Kth%20Smallest%20Number%20in%20Multiplication%20Table.java	https://youtu.be/pwv3di65RJY
Algorithms Pack	Binary Search	Sqrt(x)	https://leetcode.com/problems/sqrtx/	Given a non-negative integer *x*, return the **square root** of *x* **rounded down to the nearest integer**. The returned integer should be **non-negative** as well.<br><br>You **must not use** any built-in exponent function or operator:<br>e.g. do not use *pow(x, 0.5)* in C++.	O(log(n))	O(1)	We're expecting an optimized solution of:<br>Time Complexity: **O(log(n))**<br>Memory Complexity: **O(1)**	Linear Search	Binary Search	Inbuilt Exponent Function	Binary Search + Linear Search	B	**Not quite the best solution here!**<br><br>We can start with a guess of 0 and increment it until we find the smallest square greater than x, and then return the previous integer value as the square root of x.<br><br>This approach has a time complexity of O(sqrt(x)), which is slower than binary search, especially for large values of x.<br><br>It also requires checking each integer value up to the square root of x, which can be computationally expensive for large values of *x*.	**Correct!** A very nice solution, only surpassed by **Newton's Method**, which is also interesting to look up!<br><br>Our approach uses binary search, starting with a lower bound of 0 and an upper bound of x. We then repeatedly calculate the midpoint between these bounds, and check whether the square of the midpoint is greater than or less than x.<br><br>If the square of the midpoint is less than x, we update the lower bound to be the midpoint; otherwise, we update the upper bound to be the midpoint. We continue this process until the difference between the upper and lower bounds is less than a small epsilon value, at which point we return the lower bound plus the epsilon value as the square root of x rounded down to the nearest integer.<br><br>This approach is also known as the **Babylonian method**<br><br>Time Complexity: O(log(x)), since we use binary search to converge on the correct answer.<br>Space Complexity: O(1)	**Oops!** Read the question carefully!<br><br>It says **no built-in exponent functions!**	Combining these two approaches here is going to be way **less efficient** than simply using one or the other.	A linear search starts with a guess of 0, incrementing it until we find the largest square less than or equal to *x*.<br><br>This approach has a time complexity of **O(sqrt(x))**, since we need to try all possible integers up to the square root of *x*.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Sqrt(x).cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Sqrt(x).py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Sqrt(x).java	https://youtu.be/NHnl8iqG4O8
Algorithms Pack	Depth-First Search	Kill Process	https://leetcode.com/problems/kill-process/	You have *n* processes forming a rooted tree structure. You are given two integer arrays *pid* and *ppid*, where *pid[i]* is the ID of the *ith* process and *ppid[i]* is the ID of the *ith* process's parent process.<br><br>Each process has only **one parent process** but may have multiple children processes. Only one process has *ppid[i] = 0*, which means this process has **no parent process** (the root of the tree).<br><br>When a process is **killed**, all of its children processes will also be killed.<br><br>Given an integer *kill* representing the ID of a process you want to kill, *return a list of the IDs of the processes that will be killed. You may return the answer* **in any order**.	O(n)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n)**<br>Memory Complexity: **O(n)**	Topological Sorting	Linear Search	Inorder Traversal	DFS	D	**Incorrect!**<br><br>We don't have a guaranteed DAG here, so topological sort is **not a realistic option**.	**Too slow**.<br><br>The idea is okay.  We iterate over the list of processes and check if each process is a descendant of the process to be killed. If so, we add it to our result.<br><br>Why it's suboptimal:<br>This approach is inefficient for large trees since it checks all processes, even if they are not descendants of the process to be killed.<br>Time Complexity: O(n^2)	**Suboptimal.**<br><br>Inorder traversal normally assumes a *binary tree*, and is usually a more complicated process in a general tree.	**Correct!**<br><br>DFS can traverse the process tree **starting from the process to be killed**.<br>1) Build a graph using a hash table where each process ID maps to a list of its children's IDs.<br>2) Starting from the process to be killed, we perform a DFS and add all visited nodes to a set.<br>3) The set will contain all the processes that need to be killed.<br><br>Time Complexity: O(n), where n is the total number of processes. The DFS function traverses the tree, visiting each node at most **once**, so the set construction from visited nodes takes O(n) time in the worst case.<br>Memory Complexity: O(n)	Traverse the process tree for every process to identify which processes need to be killed.<br><br>This approach is suboptimal because it requires traversing the entire tree for each process. Hence, the time complexity is O(n^2), where n is the total number of processes.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Kill%20Process.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Kill%20Process.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Kill%20Process.java	https://youtu.be/bG4VPzYB1Uo
Algorithms Pack	Depth-First Search	Employee Importance	https://leetcode.com/problems/employee-importance/	You have a data structure of employee information, including the employee's unique ID, importance value, and direct subordinates' IDs.<br><br>You are given an array of employees *employees* where:<br>1) *employees[i].id* is the ID of the *ith* employee.<br>2) *employees[i].importance* is the importance value of the *ith* employee.<br>3) *employees[i].subordinates* is a list of the IDs of the direct subordinates of the *ith* employee.<br><br>Given an integer *id* that represents an employee's ID, return the **total importance value of this employee and all their direct and indirect subordinates**.	O(n)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n)**<br>Memory Complexity: **O(n)**	DFS + Hash Table	Recursive Traversal	BFS Without Hash Table	Union Find	A	**Correct!**<br><br>We use DFS to traverse the hierarchy of employees starting from the given **employee ID**. We use an hash table to map each employee's *ID* to the corresponding *employee* object for quick access.<br><br>We start by fetching the *employee* object for the given *ID* and add its *importance* value to the result. We then recursively call the *dfs* function on each of its subordinates and add their importance values to the result.<br><br>We repeat this process for each subordinate, continuing to traverse the hierarchy of employees until we have visited every direct and indirect subordinate. We return the final result, which will be the total importance value of the given employee and all their direct and indirect subordinates.<br><br>Time Complexity: **O(n)**<br>Memory Complexity: **O(n)**	**Suboptimal.**<br><br>This is identical to the brute-force approach.	Acutally, it's **better to include a hash table**.<br><br>Starting from the employee with the given ID, we can use BFS to traverse the hierarchy and add up the importance values of all visited employees. This approach is suboptimal because it can result in duplicate work, since the same employee can be visited multiple times from different paths in the hierarchy.<br><br>The time complexity of BFS without a hash table is **O(n^2)** in the worst case, where n is the number of employees.	Union Find is **overkill** for this problem. This solution requires some additional setup time to build the graph and perform the Union Find operations, which could be avoided with a simpler solution, such as using **DFS with a hash table**.	For each employee, recursively traverse its direct and indirect subordinates, adding up their importance values.<br><br>Time Complexity: O(n^2) in the worst case, where n is the number of employees.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Employee%20Importance.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Employee%20Importance.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Employee%20Importance.java	https://youtu.be/V1FvJHDfdaw
Algorithms Pack	Depth-First Search	Number Of Connected Components in an Undirected Graph	https://leetcode.com/problems/number-of-connected-components-in-an-undirected-graph/	You have a graph of *n* nodes. You are given an integer *n* and an array *edges* where *edges[i] = [ai, bi]* indicates that there is an edge between *ai* and *bi* in the graph.<br><br>Return the **number of connected components** in the graph.	O(n+m)	O(n+m)	We're expecting an optimized solution of:<br>Time Complexity: **O(n+m)**<br>Memory Complexity: **O(n+m)**	Hash Set of Hash Sets	Kruskal's Algorithm	DFS	Matrix Multiplication	C	**No!**<br><br>In this case, it's best to **avoid making a complete hash of things**.<br><br>Use DFS instead!	**Inefficient!**<br><br>We'd use Kruskal's algorithm to find the minimum spanning tree of the graph, and then count the number of connected components in the tree.<br><br>This approach has a time complexity of **O(m log m)**, where *m* is the number of edges in the graph. However, it is not guaranteed to work correctly if the graph is not connected, and is less efficient than the other approaches in practice.	**Correct!**<br><br>To do this, we first create an **undirected graph** using the edges given in the input. Then, we traverse the graph using **DFS** to count the number of connected components in the graph.<br><br>We start from an unvisited node and traverse all of its neighbors, **marking them as visited** as we go. We continue until all nodes in the connected component have been visited, then we increment our count of connected components and move on to the next unvisited node.<br><br>Once we have visited all nodes in the graph, our count of connected components will be the final answer.<br><br>Time Complexity: **O(n + m)**<br>Memory Complexity: **O(n + m)**, because we store the graph as an *adjacency list* with *one vector for each node* and a *vector of pairs for each edge*.	**Highly inefficient!**<br><br>The idea is to build an *adjacency matrix* from the given edges and perform matrix multiplication with the matrix raised to the power of the number of nodes in the graph. We can count the number of non-zero entries in the resulting matrix, but with horrendous complexity.<br><br>Time Complexity: O(n^3 log(n))<br>Memory Complexity: O(n^2)	We can start by generating all possible subsets of the nodes in the graph, and for each subset, check if all the nodes are connected.<br><br>This can be done by performing DFS or BFS on the subset and checking if all the nodes are visited.<br><br>Time Complexity: O(2^n * n^2)	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Number%20of%20Connected%20Components%20in%20an%20Undirected%20Graph.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Number%20of%20Connected%20Components%20in%20an%20Undirected%20Graph.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Number%20of%20Connected%20Components%20in%20an%20Undirected%20Graph.java	https://youtu.be/RZYpd3ZlfAM
Algorithms Pack	Depth-First Search	Flood Fill	https://leetcode.com/problems/flood-fill/	An image is represented by an *m x n* integer grid image where *image[i][j]* represents the pixel value of the image.<br><br>You are also given three integers *sr*, *sc*, and *color*. You should perform a **flood fill** on the image starting from the pixel **image[sr][sc]**.<br><br>To perform a **flood fill**, consider the starting pixel, plus any pixels connected **4-directionally** to the starting pixel of the same color as the starting pixel, plus any pixels connected **4-directionally** to those pixels (also with the same color), and so on. Replace the color of all of the aforementioned pixels with *color*.<br><br>Return the **modified image after performing the flood fill**.	O(mn)	O(mn)	We're expecting an optimized solution of:<br>Time Complexity: **O(mn)**<br>Memory Complexity: **O(mn)**	Dynamic Programming	DFS	Topological Sort	Rabin-Karp Algorithm	B	Not needed here.	**Correct!**<br><br>While the memory complexity is far from optimal, this problem is included to give a basic demonstration of the **flood fill** algorithm itself. In this approach, we start from the given starting point *(sr, sc)* and perform a DFS traversal. During traversal, we mark all the pixels which are connected to, and have the same color as, the starting pixel. We'll replace the color of all such pixels with the new color.<br><br>In our solution, we use the helper function *isValid* to check if the given pixel is valid or not. We also use a vector *visited* to mark these visited pixels.<br><br>Time Complexity: **O(mn)**<br>Memory Complexity: **O(mn)**	No, not used at all!	**No!**<br><br>Let us know if we need to resize our buttons!	The actual brute-force solution is very similar to the correct answer, so we'll challenge you to try and learn this very well-known algorithmic method!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Flood%20Fill.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Flood%20Fill.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Flood%20Fill.java	https://youtu.be/s6_NsNHCEOw
Algorithms Pack	Depth-First Search	Count Sub Islands	https://leetcode.com/problems/count-sub-islands/	You are given two *m x n* binary matrices *grid1* and *grid2* containing only *0*'s (representing water) and *1*'s (representing land). An **island** is a group of *1*'s connected **4-directionally** (horizontal or vertical). Any cells outside of the grid are considered water cells.<br><br>An island in *grid2* is considered a **sub-island** if there is an island in *grid1* that contains **all** the cells that make up this island in *grid2*.<br><br>Return the number of islands in *grid2* that are considered **sub-islands**.	O(mn)	O(mn)	We're expecting an optimized solution of:<br>Time Complexity: **O(mn)**<br>Memory Complexity: **O(mn)**	Flood Fill	Union Find	DFS	Brute-Force + Memoization	C	*Suboptimal* because it still checks every cell of *each island* in grid2 - we could simply use DFS to visit all cells of an island in linear time.<br><br>Approach: Perform a flood-fill algorithm for each island in grid2 to find all the cells that belong to the island, then check if all the cells also belong to the corresponding island in grid1.<br><br>Time complexity: O(mn X (m+n)) where m and n are the number of rows and columns in the matrix respectively.<br>Memory complexity: O(mn)	**No!**<br><br>This approach would create disjoint sets for each island in grid2, then merge cells in the same island. For each set of cells in grid2,we'd check if it is a subset of any set in grid1.<br><br>Time complexity: O(mn(mn)) where m and n are the number of rows and columns in the matrix respectively, and  is the inverse Ackermann function.<br>Memory complexity: O(mn)	**Correct!**<br><br>We traverse through each cell of grid2 that contains a 1 and perform a DFS to check if it is a sub-island of grid1. To do this, we simply check if the current cell is a valid cell and if it contains a 1.<br><br>If so, we check if the corresponding cell in grid1 also contains a 1.   If not, the current island is not a sub-island, and we return. If it does, we mark the current cell as visited in grid2 and recursively check its neighboring cells.<br><br>We continue until we have visited all the cells of the current island. If at any point during the traversal, we encounter a cell that is not a part of the corresponding island in grid1, we mark the island as false and return.<br><br>Time Complexity: **O(mn)**<br>Memory Complexity:** O(mn)**<br>The **DFS recursion stack** uses O(mn) space in the worst case, where all cells are part of a single island.	**Highly inefficient!**<br><br>The approach here:<br>1) For each cell in grid2, memoize whether the cell belongs to an island, and whether that island is a sub-island of grid1.<br>2) To memoize, use a hash table to store visited islands of grid2, and check if an island in grid2 is a subset of any visited island in grid1.<br><br>Time Complexity: O(m^2n^2)<br>Memory Complexity: O(mn)	Simply check every island in grid2 to see if it is a sub-island of grid1 by comparing each cell in the island to the corresponding cell in the island of grid1.<br><br>Time complexity: O(m^2n^2) where m and n are the number of rows and columns in the matrix respectively. This is because we need to check every cell of every island in grid2 and compare it to every cell of the corresponding island in grid1.<br><br>Find another approach!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Count%20Sub%20Islands.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Count%20Sub%20Islands.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Count%20Sub%20Islands.java	https://youtu.be/WDsH02DSRUQ
Algorithms Pack	Depth-First Search	Coloring A Border	https://leetcode.com/problems/coloring-a-border/	You are given an *m x n* integer matrix *grid*, and three integers *row*, *col*, and *color*. Each value in the grid represents the color of the grid square at that location.<br><br>Two squares are called **adjacent** if they're next to each other in any of the 4 directions.  Two squares belong to the same **connected component** if they have the same color and are next to each other in any of the 4 directions.<br><br>The **border of a connected component** is all the squares in the connected component that are either 4-directionally adjacent to a square not in the component, or on the boundary of the grid (the first or last row or column).<br><br>You should color the **border** of the **connected component** that contains the square *grid[row][col]* with *color*.<br><br>Return **the final grid**.	O(mn)	O(mn)	We're expecting an optimized solution of:<br>Time Complexity: **O(mn)**<br>Memory Complexity: **O(mn)**	Union-Find	Flood Fill	DFS	Edge-Detection	C	**Suboptimal!**<br><br>The method uses a union-find data structure to find the connected component containing the specified cell. During the union-find, you need to keep track of which cells are on the border of the component. Finally, color all the border cells with the specified color.<br><br>The weakness here is the time complexity, which is **O(mn X (mn)), where  is the inverse Ackermann function.<br>The space complexity, at **O(mn)**, offers little improvement.	This adds a little unnecessary complexity. A **general DFS is better**.	**Correct!**<br><br>Our solution uses a DFS algorithm to identify the connected component that contains the given square at *(row, col)* and mark it as *visited*. Then, we iterate over the entire grid and color in the border of the visited component with the given color.<br><br>To do this, we check if a cell is adjacent to an unvisited cell or is on the border of the grid. If so, we color it in.<br><br>Time Complexity: **O(mn)**<br>Memory Complexity: **(mn)**	**Controversially, no!**<br><br>With your chosen approach, the idea would be to iterate through the border of the grid, and for each cell on the border, check if it is part of a connected component with the same color as the specified cell. If it is, color that cell with the specified color.<br><br>Time Complexity: **O(m + n)** where m and n are the dimensions of the grid.<br>Space Complexity: O(1)<br><br>The main weakness here?  The edge-detection approach is efficient in terms of both time and space complexity, but it only works if the specified cell is on the border of the grid. If the cell is in the interior of the grid, this approach will not work.	Iterate through every cell in the grid, and for each cell, check if it is on the border of a connected component. If it is, color that cell with the specified color.<br><br>Time complexity: O(mn X max(m, n)) where m and n are the dimensions of the grid.<br><br>Try to find a more efficient approach!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Coloring%20A%20Border.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Coloring%20A%20Border.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Coloring%20A%20Border.java	https://youtu.be/u_ss6h5ycf4
Algorithms Pack	Depth-First Search	Number of Closed Islands	https://leetcode.com/problems/number-of-closed-islands/	Given a 2D *grid* consists of *0s* (land) and *1s* (water). An *island* is a maximal 4-directionally connected group of *0s* and a closed island is an island **totally** (all left, top, right, bottom) surrounded by *1s*.<br><br>Return the number of **closed islands**.	O(mn)	O(mn)	We're expecting an optimized solution of:<br>Time Complexity: **O(mn)**<br>Memory Complexity: **O(mn)**	Union-Find	DFS	Floyd-Warshall	Rabin-Karp	B	**Plausible**, but less memory-efficient than other solutions!<br><br>This method would involve initializing a union-find data structure where each cell is a separate set. For each 0 cell, we'd check its neighbors and union their sets if they're also 0s. Finally, we can count the number of sets that are closed islands.<br><br>Weakness: this solution has a time complexity of O(mn), but it requires **extra space** to store the union-find data structure.	**Correct!** (BFS also works well!)<br><br>1) In our solution, the *DFS* function traverses through the 2D grid, checking if each cell is a connected component or not.<br>2) If the cell is a connected component, the DFS assigns a unique ID to the connected component.<br>3) *DFS* also checks if the **connected component** is **touching the boundary** using the *onBoundary* function.  If so, it sets a boolean flag to true.<br>4) The main function increments the *count* variable if the connected component is not touching the boundary.<br><br>Time Complexity: **O(mn)**<br>Memory Complexity: **O(mn)**	**Incorrect!**<br><br>This is **not even close**, as Floyd-Warshall is a **shortest path algorithm!**	**Incorrect!** This is used for **string-based problems!**	Iterate through each cell in the grid and for each 0 cell, perform a DFS to find all connected 0 cells, then check if it's a closed island. Count the number of closed islands found.<br><br>The initial brute force solution has a time complexity of O(mn(m+n)) since it performs a DFS for each 0 cell, leading to a worst-case scenario where we visit every cell twice (once for the DFS and once for the check). This is not optimal for larger grids.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Number%20of%20Closed%20Islands.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Number%20of%20Closed%20Islands.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Number%20of%20Closed%20Islands.java	https://youtu.be/D8GYZiLZKPU
Algorithms Pack	Depth-First Search	Detect Cycles in 2D Grid	https://leetcode.com/problems/detect-cycles-in-2d-grid/	Given a 2D array of characters *grid* of size *m x n*, you need to find if there exists any cycle consisting of the **same value** in *grid*.<br><br>A cycle is a path of **length 4 or more** in the grid that starts and ends at the same cell. From a given cell, you can move to one of the cells adjacent to it - in one of the four directions (up, down, left, or right), if it has the **same value** of the current cell.<br><br>Also, you cannot move to the cell that you visited in your last move. For example, the cycle *(1, 1) -> (1, 2) -> (1, 1)* is invalid because from *(1, 2)* we visited *(1, 1)* which was the last visited cell.<br><br>Return *true* if any cycle of the same value exists in *grid*, otherwise, return *false*.	O(nm)	O(nm)	We're expecting an optimized solution of:<br>Time Complexity: **O(mn)**<br>Memory Complexity: **O(mn)**	Floyd's Cycle-Finding Algorithm	Floyd-Warshall	Dynamic Programming	DFS	D	**Incorrect!**<br><br>In this approach, we use **Floyd's cycle-finding algorithm** to find a cycle of length 4 or more in the grid.<br><br>This is not guaranteeed to work for all cases.<br><br>Additionally, it has a time complexity of O(m^2*n), which is less efficient than the DFS or union-find approach.	**Almost never optimal**, unfortunately!<br><br>Floyd-Warshall algorithm has a time complexity of O(n^3), which is not feasible for large inputs.<br><br>In any case, this approach is **overkill** for finding cycles of length 4 or more, as it computes the shortest path between every pair of cells in the grid.	Using DP+Memoization would be a very **time-consuming** exercise here.<br><br>**DFS** is a better approach.	**Correct!**<br><br>We start by iterating through each cell of the grid and performing DFS if that cell hasn't been visited yet. During DFS, we mark visited cells and check if the current cell matches the starting cell of the cycle (if it does, then **we've found a cycle**).<br><br>We also avoid revisiting the parent cell that was last visited. If we find a cycle, we terminate the search immediately and return *true*.  If we traverse the entire grid and do not find any cycles, we return *false*.<br><br>Time Complexity: **O(mn)**<br>Memory Complexity: O(mn)	We'd consider the brute-force approach to be very similar to the answer here.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Detect%20Cycles%20in%202D%20Grid.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Detect%20Cycles%20in%202D%20Grid.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Detect%20Cycles%20in%202D%20Grid.java	https://youtu.be/YpbGXs5fl9s
Algorithms Pack	Depth-First Search	Restore the Array From Adjacent Pairs	https://leetcode.com/problems/restore-the-array-from-adjacent-pairs/	There is an integer array *nums* that consists of *n* unique elements, but you have forgotten it. However, you do remember every pair of adjacent elements in *nums*.<br><br>You are given a 2D integer array *adjacentPairs* of size *n-1* where each *adjacentPairs[i] = [ui, vi]* indicates that the elements *ui* and *vi* are adjacent in *nums*.<br><br>It is guaranteed that every adjacent pair of elements *nums[i]* and *nums[i+1]* will exist in *adjacentPairs*, either as *[nums[i], nums[i+1]]* or *[nums[i+1], nums[i]]*. The pairs can appear **in any order**.<br><br>Return the original array *nums*. If there are multiple solutions, return **any of them**.	O(n)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n)**<br>Memory Complexity: **O(n)**	Greedy	Sorting	Topological Sort	DFS	D	**Not the best approach.**<br><br>This may not work if there are multiple possible solutions, as it may get stuck in a cycle or a dead end. For example, if there are two adjacent pairs *(a, b)* and *(b, c)*, but also another pair *(c, d)*, then starting with *a* or *d* may lead to a dead end.<br><br>Time Complexity: O(n^2)	**Not the best approach!**<br><br>We'd construct the graph from the given adjacent pairs and sort the nodes in increasing order, giving us the original array.<br><br>Why it's suboptimal:<br>While this approach yields the correct answer, it is inefficient since it requires sorting the entire array of nodes. This can be time-consuming and memory-intensive, especially for larger graphs.<br><br>Time Complexity: **O(n log(n))**	**No**<br><br>Topological sort requires a directed acyclic graph (DAG), and it's not guaranteed that the graph created from adjacentPairs will be a DAG.	**Correct!**<br><br>We can build an **undirected graph** from the given adjacent pairs. Since the graph is connected, any connected component of the graph can represent a valid ordering of the original array.<br><br>We use DFS to traverse all the connected nodes to form a **connected component**. The sequence of nodes visited in the DFS forms a valid ordering of the original array.<br><br>Time Complexity: **O(n)**<br>Memory Complexity: O(n), which takes into account the hash table and the call stack for the DFS function!	We can generate all possible permutations of the unique elements in the array and check which permutation satisfies the adjacent pairs.<br><br>Why it's suboptimal:<br>This approach has an exponential time complexity of O(n!), which is not practical for large values of n.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Restore%20the%20Array%20From%20Adjacent%20Pairs.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Restore%20the%20Array%20From%20Adjacent%20Pairs.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Restore%20the%20Array%20From%20Adjacent%20Pairs.java	https://youtu.be/ab-iNgozKxw
Algorithms Pack	Depth-First Search	Smallest String With Swaps	https://leetcode.com/problems/smallest-string-with-swaps/	You are given a string *s*, and an array of pairs of indices in the string *pairs* where *pairs[i] = [a, b]* indicates 2 indices(**0-indexed**) of the string.<br><br>You can swap the characters at any pair of indices in the given *pairs* **any number of times**.<br><br>Return the lexicographically smallest string that *s* can be changed to after using the swaps.	O(n log(n))	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n log(n))**<br>Memory Complexity: **O(n)**	Greedy Swapping	Sort + Swap	DFS	Permutation Sorting	C	**Incorrect!**<br><br>This approach may lead to a wrong result because swapping letters based on the first pair that can be swapped does not necessarily lead to the lexicographically smallest string.<br><br>There may be other pairs that can be swapped to obtain a smaller string too.	**Not ideal.**<br><br>The method involves simply sorting all pairs and applying the swaps one by one.<br><br>This approach fails in some cases because sorting pairs does not guarantee that all pairs form a connected component. It's possible that some pairs are not connected, and applying swaps on these pairs may lead to a wrong result.	**Correct**, although **Union Find** would also be a sound approach!<br><br>Our approach uses **connected components** (CC) to group all the characters that are connected by swaps. We create an undirected graph where the nodes are the indices of the string, and the edges are the pairs of indices given.<br><br>We then traverse the graph using DFS and find all the CCs. For each CC, we sort the corresponding characters in lexicographical order and replace the corresponding characters in the original string *s*. Finally, we return the modified string.<br><br>Time Complexity: **O(n log(n))**, where *n* is the length of the input string *s*.<br>Memory Complexity: **O(n)**, for the adjacency list where *n* is the length of the input string *s*.	**No**: this would be a very complex brute-force approach.	Generate all possible permutations of the string using the given pairs and return the lexicographically smallest one.<br><br>However, the time complexity for this is factorial, or **O(n!)** which is not feasible for large inputs.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Smallest%20String%20With%20Swaps.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Smallest%20String%20With%20Swaps.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Smallest%20String%20With%20Swaps.java	https://youtu.be/wWDO8JRpeWE
Algorithms Pack	Depth-First Search	Is Graph Bipartite	https://leetcode.com/problems/is-graph-bipartite/	There is an **undirected** graph with *n* nodes, where each node is numbered between *0* and *n-1*. You are given a 2D array *graph*, where *graph[u]* is an array of nodes that node *u* is adjacent to. More formally, for each *v* in *graph[u]*, there is an undirected edge between node *u* and node *v*. The graph has the following properties:<br>1) There are no self-edges (*graph[u]* does not contain *u*).<br>2) There are no parallel edges (*graph[u]* does not contain duplicate values).<br>3) If *v* is in *graph[u]*, then *u* is in *graph[v]* (the graph is undirected).<br>The graph may not be connected, meaning there may be two nodes *u* and *v* such that there is no path between them.<br><br>A graph is **bipartite** if the nodes can be partitioned into two independent sets *A* and *B* such that every edge in the graph connects a node in set *A* and a node in set *B*.<br><br>Return *true* **if and only if it is bipartite**.	O(n+m)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(m+n)**<br>Memory Complexity: **O(n)**	Union-Find	Coloring With Flood Fill Algorithm	Kahn's Algorithm	DFS	D	**Incorrect!**<br><br>Method:<br>1) Use the union-find algorithm to group nodes into two sets.<br>2) For each edge, we check if the nodes are already in the same set. If they are, the graph is not bipartite. If they are not, we add them to different sets.<br>3) If we run out of edges to check and the graph has not been determined to be non-bipartite, we assume it is bipartite.<br><br>Weakness: DFS has better complexity. This has time complexity of O(n log(n))	Not the best approach!	This is used in **topological sorting**, and isn't really a viable approach here.	**Correct!**<br><br>This involves a DFS traversal of the given graph, marking nodes as either *1* or *2* depending on the color of their adjacent nodes, while checking for conflicts.<br><br>We initialize the vector *colors*, with all nodes colored as *0*, meaning they haven't been visited yet. Then, we iterate through each node. For each unvisited node, we start a DFS traversal, marking the node as either *1* or *2* based on the color of the parent node.<br><br>We then pass the opposite color to the child nodes, i.e., if the parent node is colored as *1*, we assign *2* to its child nodes and vice-versa. If a conflict is detected while coloring the nodes, we mark the *colorConflict* variable as *true* and immediately return *false*. Otherwise, if all nodes are colored without any conflicts, we return *true*.<br><br>Time Complexity: **O(n + m)**, where n is the number of nodes in the graph, and m is the number of edges.<br>Space Complexity: **O(n)**, which is the space used by the *colors* vector.	Partition the nodes into two sets, *A* and *B*, and check if all edges in the graph connect nodes from different sets.<br><br>Time complexity: a **very inefficient** O(n X 2^n), where n is the number of nodes in the graph.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Is%20Graph%20Bipartite.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Is%20Graph%20Bipartite.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Is%20Graph%20Bipartite.java	https://youtu.be/n93ifQRBD5o
Algorithms Pack	Depth-First Search	Reorder Routes To Make All Paths Lead To The City Zero	https://leetcode.com/problems/reorder-routes-to-make-all-paths-lead-to-the-city-zero/	There are *n* cities numbered from *0* to *n-1*, and *n-1* roads such that there is only one way to travel between two different cities (this network form a tree). Last year, The ministry of transport decided to orient the roads in one direction because they are too narrow.<br><br>Roads are represented by *connections* where *connections[i] = [ai, bi]* represents a road from city *ai* to city *bi*.<br><br>This year, there will be a big event in the capital (city *0*), and many people want to travel to this city.<br><br>Your task consists of reorienting some roads such that each city can visit the city *0*. Return the **minimum** number of edges changed.<br><br>It's guaranteed that each city can reach city *0* after reordering.	O(n)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n)**<br>Memory Complexity: **O(n)**	DFS	Kruskal's Algorithm	Greedy	Topological Sort	A	**Correct!**<br><br>In this approach, we first create an undirected graph/adjacency list by adding edges in both directions. Then we perform a DFS starting from node 0, keeping track of whether we need to reorder an edge in order to visit the node 0.<br><br>Starting with node *0*, we mark it as visited. For each of its neighbors, we mark the edge as *built* (since we can travel from 0 to that neighbor directly), and then recursively explore that neighbor's neighbors. We add the number of edges that need to be reordered to a counter variable, *count*.<br><br>After traversing the entire tree, we've counted all the edges that need to be reordered in order to visit the node 0 from all the nodes - and can return our result!<br><br>Time Complexity: **O(n)**, where n is the number of nodes in the graph.<br>Memory Complexity: **O(n)**, where n is the number of nodes in the graph.	**Not quite!**<br><br>Kruskal's Algorithm is not guaranteed to produce the minimum spanning tree of the original tree, which means that the solution may not be optimal. Additionally, the algorithm may require a large number of changes to be made to the orientation of the edges, leading to a higher reorder count.<br><br>The approach involves building a new tree rooted at the capital city by greedily selecting edges in ascending order of their weights. We can keep track of the orientation of each edge and the number of changes required to make every city reachable from the capital city.<br><br>**DFS** is more efficient.	**Greed is** *not* **good**, in this instance.<br><br>This approach does not consider the global optimal solution, and may get stuck in a local optimal solution. There can be cases where choosing a different edge at one node could lead to fewer edge reversals overall.	**Incorrect!**<br><br>This approach assumes that the graph is a Directed Acyclic Graph (DAG), and does not work for general trees. Additionally, the algorithm requires computing the indegree of each node, which takes O(n^2) time for a tree.	We can generate all possible combinations of the orientations of the edges, and count the number of changes required to make every city reachable from the capital city.<br><br>However, this approach has an extremely high time complexity, as the number of possible edge orientations grows exponentially with the number of edges. It is not feasible for large inputs.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Reorder%20Routes%20to%20Make%20All%20Paths%20Lead%20to%20the%20City%20Zero.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Reorder%20Routes%20to%20Make%20All%20Paths%20Lead%20to%20the%20City%20Zero.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Reorder%20Routes%20to%20Make%20All%20Paths%20Lead%20to%20the%20City%20Zero.java	https://youtu.be/Ls5g9Youotc
Algorithms Pack	Depth-First Search	Path With Minimum Effort	https://leetcode.com/problems/path-with-minimum-effort/	You are a hiker preparing for an upcoming hike. You are given *heights*, a 2D array of size *rows x columns*, where *heights[row][col]* represents the height of cell *(row, col)*. You are situated in the top-left cell, *(0, 0)*, and you hope to travel to the bottom-right cell, *(rows-1, columns-1)* (i.e., **0-indexed**). You can move **up**, **down**, **left**, or **right**, and you wish to find a route that requires the minimum **effort**.<br><br>A route's **effort** is the **maximum absolute difference** in heights between two consecutive cells of the route.<br><br>Return the *minimum* **effort** *required to travel from the top-left cell to the bottom-right cell*.	O(mn log K)	O(mn)	We're expecting an optimized solution of:<br>Time Complexity: **O(mn log(k))**<br>Memory Complexity: **O(mn)**	Greedy	DFS + Binary Search	Dijkstra's Algorithm	Floyd-Warshall	B	**Not the best approach**.<br><br>This doesn't guarantee that we will reach the bottom-right cell with the minimum effort. We might get stuck in a local minimum and miss a better path.<br><br>Also, it's possible to construct a counter-example where this algorithm fails.	**Correct!**<br><br>Set our lower and upper bounds at *0* and *1e9* (the maximum height difference), we can repeatedly perform a binary search on them until we find the smallest value for which a path from the top-left to bottom-right cell.<br><br>In the *possible* function, we use DFS to search for that path from *(0,0)* to *(rows-1,columns-1)*. We mark *visited* cells using a 2D boolean array to ensure we do not revisit cells.<br><br>If we find a path that reaches the destination cell, we return *true*. Otherwise, we return *false*.<br><br>Time Complexity: **O(mn log (K))**, with K being the range of the heights. We perform a binary search on K, which takes O(log (K)) time. For each search, we perform DFS, which takes O(mn) time in the worst case.<br>Memory Complexity: **O(mn)**	**Incorrect!**<br><br>This approach doesn't guarantee that we will find the minimum effort required to travel from the top-left cell to the bottom-right cell. We might miss a better path.<br><br>Also, the algorithm is not suitable for this problem because the edge weights are not non-negative.	**No!**<br><br>With a minimum time complexity of O(n^3), it's clear there are more optimal approaches than Floyd-Warshall to solve this problem.	We can generate all possible paths from the top-left cell to the bottom-right cell and compute their maximum absolute difference - eventually, we can return the minimum of all such maximum absolute differences.<br><br>This approach is very inefficient since it generates all possible paths, which could be exponential in the worst case!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Path%20With%20Minimum%20Effort.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Path%20With%20Minimum%20Effort.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Path%20With%20Minimum%20Effort.java	https://youtu.be/f9CRNdrA0Tc
Algorithms Pack	Breadth-First Search	Graph Valid Tree	https://leetcode.com/problems/graph-valid-tree/	You have a graph of *n* nodes labeled from *0* to *n-1*. You are given an integer *n* and a list of *edges* where *edges[i] = [ai, bi]* indicates that there is an undirected edge between nodes *ai* and *bi* in the graph.<br><br>Return *true* **if the edges of the given graph make up a valid tree**, and *false* otherwise.	O(n+m)	O(n+m)	We're expecting an optimized solution of:<br>Time Complexity: **O(m+n)**<br>Memory Complexity: **O(m+n)**	Floyd-Warshall	Topological Sort	Dynamic Programming	BFS	D	**Suboptimal.** The Floyd-Warshall algorithm is typically used to find the **shortest path** between all pairs of vertices in a weighted graph, but may be **overkill** for detecting cycles in an undirected graph since it has a time complexity of O(V^3) where V is the number of vertices.	**No**<br><br>**Topological sort** is primarily used for directed acyclic graphs (DAGs) and may not work for undirected graphs.	An unlucky guess?<br><br>There are many more viable approaches than DP.	**Correct!**  DFS also works!<br><br>The idea is to do a BFS traversal checking for the presence of a cycle. During the traversal, we maintain two vectors: *visited* and *parent*. *visited* stores the level at which a node is visited, and *parent* stores the parent of the node. If a node is visited more than once, then there **is a cycle in the graph**.<br><br>We initialize *visited* with a very large value **OO** to indicate that a node is not yet visited.  If we revisited a visited node, we check whether the parent is the same as our current node's parent. If so, then we skip that edge, as it is the one connecting the current node to its parent. Otherwise, we have found a cycle in the graph.<br><br>If there is no cycle in the graph, we must ensure the number of edges is *n-1*, where *n* is the number of nodes in the graph. If not, then the graph is disconnected and cannot be a tree.<br><br>Time Complexity: **O(V+E)**, where V is the number of nodes and E is the number of edges in the graph.<br>Memory Complexity: O(V+E), where V is the number of nodes and E is the number of edges in the graph.	Generate all possible permutations of edges and check if the resulting graph is a valid tree.<br><br>However, the number of possible permutations of edges grows factorially with the number of edges, making this approach highly inefficient for large graphs.<br><br>Time Complexity: O(n!)	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Graph%20Valid%20Tree.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Graph%20Valid%20Tree.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Graph%20Valid%20Tree.java	https://youtu.be/ffvp3Mw6LG8
Algorithms Pack	Breadth-First Search	Shortest Path To Get Food	https://leetcode.com/problems/shortest-path-to-get-food/	You are starving and you want to eat food as quickly as possible. You want to find the shortest path to arrive at any food cell.<br><br>You are given an *m x n* character matrix, *grid*, of these different types of cells:<br>'*' is your location. There is **exactly one** '*' cell.<br>*#* is a food cell. There may be **multiple** food cells.<br>*O* is free space, and you can travel through these cells.<br>*X* is an obstacle, and you cannot travel through these cells.<br><br>You can travel to any adjacent cell **north**, **east**, **south**, or **west** of your current location if there is not an obstacle.<br><br>Return the **length** of the shortest path for you to reach **any** food cell. If there is no path for you to reach food, return *-1*.	O(nm)	O(nm)	We're expecting an optimized solution of:<br>Time Complexity: **O(mn)**<br>Memory Complexity: **O(mn)**	BFS	Dijkstra's Algorithm	Recursive Backtracking	Greedy	A	**Correct!**<br><br>Our solution uses BFS to find the shortest path from the starting cell to any food cell in the given grid.<br><br>We first find the starting cell, and then use BFS to explore all possible cells reachable from it.  They must be neither blocked by an obstacle *X*, nor already *visited*.<br><br>We use a queue to store the cells to visit next, and visit each cell in the queue to check if it is a food cell *#*. If so, we return the level at which the food cell was found. Otherwise, we add all reachable, unvisited cells marked as free space *O* to the queue, and mark them as visited.<br><br>If all cells have been *visited* and no food cells have been found, we return *-1* to indicate that there is no path to any food cell.<br><br>Time Complexity: **O(mn)**<br>Memory Complexity: **O(mn)**	**Suboptimal.**<br><br>While it would work, this approach involves calculating the shortest path to all cells in the grid, which is not necessary for this problem.<br><br>The time complexity would be much higher than the **BFS** approach.	**No**<br><br>This is the brute-force approach, and isn't as good as **BFS** for this problem.	**Incorrect**<br><br>The greedy algorithm may not always find the shortest path since it does not consider the overall path length, but only the next step. It may also get stuck in local minima and miss shorter paths to other food cells.<br><br>It could be done by choosing the nearest food cell at each step and following the shortest path to that food cell - repeating until we reach a food cell. However, the time complexity is still suboptimal.	Generate all possible paths from the starting cell to each food cell and select the shortest path. This can be done using various methods.<br><br>However, the time complexity of this approach is very high, as it involves exploring all possible paths in the grid, which can take exponential time.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Shortest%20Path%20to%20Get%20Food.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Shortest%20Path%20to%20Get%20Food.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Shortest%20Path%20to%20Get%20Food.java	https://youtu.be/ajddnybnPug
Algorithms Pack	Breadth-First Search	Jump Game III	https://leetcode.com/problems/jump-game-iii/	Given an array of non-negative integers *arr*, you are initially positioned at *start* index of the array. When you are at index *i*, you can jump to *i + arr[i]* or *i - arr[i]*, check if you can reach to **any** index with value 0.<br><br>Notice that you can not jump outside of the array at any time.	O(n)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n)**<br>Memory Complexity: **O(n)**	Dynamic Programming	Greedy	BFS	Binary Search	C	*Not so efficient.*<br><br>DP can be inefficient if the jumps have large values or if the array is very large, because it requires computing and storing many intermediate results. In some cases, it may also not be possible to find a solution with this method.	**Incorrect!**<br><br>A greedy algorithm is not guaranteed to find an optimal solution, because it may get stuck in a local optimum or a loop that prevents reaching an index with value 0.	**Correct!**<br><br>Our approach starts at the starting vertex and explores all the neighboring vertices before moving to the next level. In this approach, we use a queue to maintain the order of nodes to be visited. We start from the start index and push it into the queue. We then process the nodes in the queue **level-by-level** until we reach an index with value *0* or we have traversed all nodes.<br><br>Time Complexity: **O(n)**, where n is the length of the input array. In the worst case, we will visit every node in the array.<br>Memory Complexity: **O(n)**	**Incorrect!**<br><br>Binary search assumes that the array is sorted, which is not the case here.	We can try all possible jumps from the current index, and repeat this process for each new index we reach until we find an index with value 0. This approach can be implemented recursively or using a loop.<br><br>However, this is very inefficient because it explores all possible paths, even those that are clearly not optimal. This can lead to an exponential time complexity in the worst case, and it's not practical for large arrays.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Jump%20Game%20III.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Jump%20Game%20III.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Jump%20Game%20III.java	https://youtu.be/uGnlCuUzSJU
Algorithms Pack	Breadth-First Search	Minimum Operations To Convert Number	https://leetcode.com/problems/minimum-operations-to-convert-number/	You are given a **0-indexed** integer array *nums* containing **distinct** numbers, an integer *start*, and an integer *goal*. There is an integer *x* that is initially set to *start*, and you want to perform operations on *x* such that it is converted to *goal*. You can perform the following operation repeatedly on the number *x*:<br>If *0 <= x <= 1000*, then for any index *i* in the array (*0 <= i < nums.length*), you can set *x* to any of the following:<br>1) *x + nums[i]*<br>2) *x - nums[i]*<br>3) *x ^ nums[i]* (bitwise-XOR)<br><br>Note that you can use each *nums[i]* any number of times in any order. Operations that set *x* to be out of the range *0 <= x <= 1000* are valid, but no more operations can be done afterward.<br><br>Return the **minimum number of operations** needed to convert *x = start* into *goal*, and *-1* if it is not possible.	O(n)	O(1)	We're expecting an optimized solution of:<br>Time Complexity: **O(n)**<br>Memory Complexity: **O(1)**	BFS	Dijkstra's Algorithm	Greedy	Dynamic Programming	A	**Correct!**<br><br>Initialize a queue with the starting value, marking it as visited, then perform BFS on this queue's new front element, adding any neighbors to our queue.<br><br>For each element, we iterate through the given array, checking if we can reach the goal by adding, subtracting, or XORing it with the current element. If so, we return the current level plus one, which represents the **minimum number of operations** needed to reach the goal.<br><br>Otherwise, we mark the current element as *visited*, adding its neighbors (obtained by performing any of the operations) to the queue, continuing this process until we either find the goal **or** the queue becomes empty. If we can't find the goal, we return *-1*.<br><br>Time Complexity: **O(n)**, or more precisely; O(n X (1000^2)), where n is the length of the given array.<br>Memory Complexity: **O(1)**	**Not optimal** at all, since the edges in any graph we generate are not weighted uniformly.<br><br>Some edges may require more or fewer operations than others, depending on the value of the number being operated on.<br><br>In short, Dijkstra's algorithm will not guarantee that we find the minimum number of operations required.	**Not the best approach here.**<br><br>This is suboptimal because it does not consider all possible combinations of operations that can be performed on x. It may be the case that using smaller elements first leads to a shorter sequence of operations.	**Not the best approach** due to the space complexity!<br><br>The idea here?<br>Create a 2D array DP where *DP[i][j]* represents the minimum number of operations needed to convert *i* to *j*. We can fill the array using a recursive formula that considers all possible operations on *i*.<br><br>The idea is okay, but can be improved upon significantly.	We can iterate over all possible sequences of operations that can be performed on x until we reach the goal.<br><br>However, the number of possible sequences of operations is very large, and we would have to check all of them to guarantee that we find the minimum number of operations required.<br><br>This approach is very inefficient and not practical for larger inputs.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Minimum%20Operations%20to%20Convert%20Number.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Minimum%20Operations%20to%20Convert%20Number.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Minimum%20Operations%20to%20Convert%20Number.java	https://youtu.be/2rwXiFl-Ac0
Algorithms Pack	Breadth-First Search	Open The Lock	https://leetcode.com/problems/open-the-lock/	You have a lock in front of you with 4 circular wheels. Each wheel has 10 slots: *'0', '1', '2', '3', '4', '5', '6', '7', '8', '9'*. The wheels can rotate freely and wrap around: for example we can turn *'9'* to be *'0'*, or *'0'* to be *'9'*. Each move consists of turning one wheel one slot.<br><br>The lock initially starts at *'0000'*, a string representing the state of the 4 wheels.<br><br>You are given a list of dead ends called *deadends*, meaning if the lock displays any of these codes, the wheels of the lock will stop turning and you will be unable to open it.<br><br>Given a *target* representing the value of the wheels that will unlock the lock, return the minimum total number of turns required to open the lock, or *-1* if it is impossible.	O(n^2 A^n + d)	O(a^n+d)	We're expecting an optimized solution of:<br>Time Complexity: **O(n^2 A^n + d)**,  where *A* is the number of digits in our alphabet, *n* is the number of digits in the lock, and *d* is the size of deadends. We might visit every lock combination, plus we need to instantiate our set *dead*.<br>Memory Complexity: **O(A^n + d)**	Greedy	BFS	Simulated Annealing	Bucket Sort	B	**Not optimal.**<br><br>Simply, it doesn't consider the fact that turning one wheel may affect the other wheels. For example, turning the first wheel to *9* and the second wheel to *1* may not be the best approach, since it might be better to turn the second wheel to *9* instead.<br><br>This approach may result in the incorrect minimum number of turns required to reach the target combination.	**Correct!**<br><br>We use BFS to explore all possible combinations of the lock.  Starting with *0000*, we explore all its neighbors, which are obtained by changing one digit at a time.<br><br>For each neighbor, we check if it has been visited before or if it is a deadend. If it hasn't been visited and it is not a deadend, we add it to the queue and mark it as visited. We repeat this process until we reach the target combination or until we have explored all possible combinations.<br><br>Why it's a good approach: in this problem, the target may not be directly reachable from the initial combination, so we need to explore all possible combinations to find the shortest path. BFS allows us to do this efficiently by exploring all neighbors of the current combination before moving on to the next level.<br><br>Time Complexity: **O(n^2a^n + d)**, where n is the number of wheels (4 in this case), a is the number of digits per wheel (10 in this case), and d is the number of deadends.<br>Memory Complexity: **O(a^n + d)**, since we need to store all possible combinations of the lock, which is a^n. We also need to store the set of deadends, which takes O(d) space.	**No.**<br><br>Simulated annealing may not always find the optimal solution since it relies on randomness and may get stuck in a local minimum.<br><br>Time Complexity: **O(kn)**, where *k* is the number of iterations and *n* is the number of digits in the combination.	If you chose this option without it being a mouse-slip or finger-slip, do the following:<br>1)Finding an actual, physical bucket<br>Sit down on it facing the corner of the room<br>Don a dunce cap<br><br>On the other hand, if you know how to make this work efficiently, please let us know!	We can use the computer equivalent of turning the lock by hand<br><br>Try every possible combination of rotations for the wheels of the lock until we find the target code or exhaust all possible combinations.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Open%20the%20Lock.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Open%20the%20Lock.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Open%20the%20Lock.java	https://youtu.be/A5B46qkZzd4
Algorithms Pack	Breadth-First Search	Walls And Gates	https://leetcode.com/problems/walls-and-gates/	You are given an *m x n* grid *rooms* initialized with these three possible values:<br>*-1* is a wall or an obstacle.<br>*0* is a gate.<br>*INF* representing infinity means an empty room. We use the value *2^31-1 = 2147483647* to represent *INF* as you may assume that the distance to a gate is less than *2147483647*.<br><br>Fill each empty room with the distance to its *nearest gate*. If it is impossible to reach a gate, it should be filled with *INF*.	O(nm)	O(nm)	We're expecting an optimized solution of:<br>Time Complexity: **O(mn)**<br>Memory Complexity: **O(mn)**	Dijkstra's Algorithm	Floyd-Warshall	BFS	A* Search	C	**No.**<br><br>Dijkstra's algorithm is not suitable for this problem because it is designed for finding the shortest path between two specific nodes, not for computing the distance of all nodes from a set of source nodes.	**No**<br><br>This algorithm has a time complexity of O(m^3), which is far from optimal for large grids.	**Correct!**<br><br>We use BFS to traverse the grid from all the gate cells (cells with value |*0*) in the grid. We pushing these gate cells into a queue, marking them as visited. For each level, we check all cells that are reachable from cells at the current level (those adjacent to the cells in the current level) and add them to the queue, marking them as visited. We also fill in the distance to the nearest gate for each of the cells visited.<br><br>Here, BFS guarantees that we will visit all the cells in the grid in order of their distance from the starting cells. So, we can be sure that by the time we visit a cell, we have found the shortest distance to that cell from one of the starting cells.<br><br>Time Complexity: **O(mn)**, since we traverse each cell in the grid only once<br>Memory Complexity: **O(mn)**	**Suboptimal** compared to a BFS approach. While this approach is an improvement over Dijkstra's algorithm, it still involves calculating the shortest path from each gate to every other point in the grid, which can result in redundant calculations and slower performance for larger grids.	We can consider every empty cell in the grid as a starting point, and then use Breadth-First Search (BFS) to traverse the entire grid and compute the distance of each empty cell from the nearest gate.<br><br>Why it's suboptimal: This approach has a time complexity of O((mn)^2), which is very inefficient and impractical for large grids.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Walls%20and%20Gates.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Walls%20and%20Gates.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Walls%20and%20Gates.java	https://youtu.be/8zvjsNRQtnM
Algorithms Pack	Breadth-First Search	Pacific Atlantic Water Flow	https://leetcode.com/problems/pacific-atlantic-water-flow	There is an *m x n* rectangular island that borders both the **Pacific Ocean** and **Atlantic Ocean**. The **Pacific Ocean** touches the island's left and top edges, and the **Atlantic Ocean** touches the island's right and bottom edges.<br><br>The island is partitioned into a grid of square cells. You are given an *m x n* integer matrix *heights* where *heights[r][c]* represents the **height above sea level** of the cell at coordinate *(r, c)*.<br><br>The island receives a lot of rain, and the rain water can flow to neighboring cells directly north, south, east, and west if the neighboring cell's height is **less than or equal to** the current cell's height. Water can flow from any cell adjacent to an ocean into the ocean.<br><br>Return a **2D list** of grid coordinates *result* where *result[i] = [ri, ci]* denotes that rain water can flow from cell *(ri, ci)* to **both the Pacific and Atlantic oceans**.	O(nm)	O(nm)	We're expecting an optimized solution of:<br>Time Complexity: **O(mn)**<br>Memory Complexity: **O(mn)**	BFS	Greedy	Flood-Fill Algorithm	Dijkstra's Algorithm	A	**Correct!**<br><br>We perform two BFS searches, one from each ocean's border. We mark all visited cells reachable from each border, respectively, in two separate visited arrays. Finally, we iterate through each cell and check whether it was visited by both BFSes, which means that it is reachable by both oceans.<br><br>Time Complexity: **O(mn)**.   Essentially, we perform two separate BFS searches, which take O(mn) time each, and iterate through the matrix once, which also takes O(mn) time.<br>Memory Complexity: **O(mn)**	**Incorrect!**<br><br>This approach does not guarantee that we will find all the cells that can flow to both oceans, as some lower cells may not be reachable from the highest cell. It also does not guarantee that we will find the optimal paths to the oceans.	An ironic name, given the problem, but an approach that will find you **lost at sea** in an interview.<br><br>We can flood fill the grid from every cell that is adjacent to the Pacific and Atlantic ocean to mark all the reachable cells. Then we can iterate through the grid and check if a cell is reachable from both oceans.<br><br>This approach can be slow and memory-intensive, especially for large grids, as it requires multiple flood fill iterations.<br><br>Time Complexity: **O(mn(m+n))**	**Not recommended for this at all**.<br><br> The approach would be to treat the matrix as a weighted graph, where each cell is a node and there is an edge between two nodes if rain water can flow from one cell to another. Then, we run Dijkstra's algorithm from each ocean to find the minimum distance to each cell. If the sum of the distances from both oceans to a cell is equal to the height of the cell, mark the cell as reachable from both oceans.<br><br>This approach has a high time complexity -and there are faster and simpler algorithms that can solve this problem in linear time.	We can start from every cell and do a recursive depth-first search to explore all the reachable cells that have a lower or equal height. We  keep track of whether a cell is reachable from the Pacific and Atlantic ocean by keeping track of which ocean is the starting point of the current search.<br><br>Why it's suboptimal:<br>This approach has an exponential time complexity in the worst-case scenario, which makes it impractical for large grids.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Pacific%20Atlantic%20Water%20Flow.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Pacific%20Atlantic%20Water%20Flow.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Pacific%20Atlantic%20Water%20Flow.java	https://youtu.be/mIPKgbne5gw
Algorithms Pack	Breadth-First Search	Stepping Numbers	https://leetcode.com/problems/stepping-numbers/	A **stepping number** is an integer such that all of its adjacent digits have an absolute difference of exactly *1*.<br>For example, *321* is a **stepping number** while *421* is not.<br><br>Given two integers *low* and *high*, return a *sorted list of all the* **stepping numbers** in the inclusive range *[low, high]*.	O(n)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n)**<br>Memory Complexity: **O(n)**	BFS	Recursive	Dynamic Programming	Backtracking	A	**Correct!**<br><br>In this approach, we use BFS to explore all possible stepping numbers within the given range *[low, high]*. We start by creating a queue and pushing all single-digit numbers (i.e., *0*-*9*) within the given range into it.<br><br>Then, we take each number from the queue and check if its last digit is not zero, in which case we subtract 1 from the last digit and enqueue the new number if it falls within the given range. Similarly, if the last digit is not 9, we add 1 to the last digit and enqueue the new number if it falls within the given range.<br><br>We continue this process until the queue becomes empty, maintaining a *visited* array to keep track of all the stepping numbers we have already visited. Finally, we return the visited array in sorted order.<br><br>Time Complexity: **O(n)**<br>Memory Complexity: **O(n)**	**No.**<br><br>This approach suffers from the same problem as the brute-force approach, generating and checking a large number of unnecessary numbers that are not stepping numbers, resulting in poor time complexity and inefficiency.<br><br>In addition, it uses recursion, which can cause a stack overflow for large ranges.	**Suboptimal**<br><br>It generates and checks a large number of unnecessary numbers that are not stepping numbers, resulting in poor time complexity and inefficiency.	For a problem that can be solved in O(n) using **BFS**, **backtracking** would be considered overkill here.	Simply generate all the numbers in the range [low, high] and check if each number is a stepping number by comparing adjacent digits.<br><br>However, this would check a large number of unnecessary numbers that are not stepping numbers, resulting in poor time complexity and inefficiency.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Stepping%20Numbers.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Stepping%20Numbers.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Stepping%20Numbers.java	https://youtu.be/acLsd0DSsQQ
Algorithms Pack	Breadth-First Search	Shortest Path With Alternating Colors	https://leetcode.com/problems/shortest-path-with-alternating-colors/	You are given an integer *n*, the number of nodes in a directed graph where the nodes are labeled from *0* to *n-1*. Each edge is red or blue in this graph, and there could be self-edges and parallel edges.<br><br>You are given two arrays *redEdges* and *blueEdges* where:<br>*redEdges[i] = [ai, bi]* indicates that there is a directed red edge from node *ai* to node *bi* in the graph, and<br>*blueEdges[j] = [uj, vj]* indicates that there is a directed blue edge from node *uj* to node *vj* in the graph.<br><br>Return an array *answer* of length *n*, where each *answer[x]* is the length of the shortest path from node *0* to node *x* such that the edge colors alternate along the path, or *-1* if such a path does not exist.	O(n+m)	O(n+m)	We're expecting an optimized solution of:<br>Time Complexity: **O(m+n)**<br>Memory Complexity: **O(m+n)**	BFS	Dijkstra	Floyd-Warshall	Niji's Algorithm	A	**Correct!**<br><br>The approach we use in our example is a variant of BFS called **bi-color BFS**. We start at node 0, add two nodes to a queue, with the first node having a red edge and the second having a blue edge.<br><br>We perform a level-order traversal on our graph, marking each node visited with its distance from the source node *0*, with the condition that the edge color alternates from the previous node. We use *visited* to keep track of the distance of each node for each color. If a node has already been visited for a different color than the current node, we do not consider it in this iteration.<br><br>Time Complexity: **O(E + V)**, where E is the number of edges and V is the number of vertices. We visit each vertex and edge once.<br>Memory Complexity: **O(E + V), where E is the number of edges and V is the number of vertices. We use an adjacency list to store the graph and two-dimensional arrays to keep track of the distances.	**Not quite.**<br><br>We can probably modify Dijkstra's algorithm to keep track of the color of the last edge traversed and only consider edges of the opposite color in the priority queue.<br><br>However, Dijkstra's algorithm is designed to find the shortest path in a **weighted** graph - in this case, the graph is not necessarily weighted. Additionally, this approach does not guarantee finding the shortest path between nodes.	**Suboptimal**, as is often the case with Floyd-Warshall!<br><br>The time complexity is O(V^3), which can be very inefficient for large graphs. Additionally, this approach may not always find the shortest path between nodes if there are cycles in the graph.	**Color me disappointed!**<br><br>You've picked one of the few entirely fictional algorithms. *Niji* is Japanese for *rainbow*.	Generate all possible paths from node 0 to each node in the graph, keeping track of the shortest path that alternates between red and blue edges.<br><br>Why it's suboptimal: the number of possible paths in the graph can be very large, making this approach very inefficient even for relatively small graphs.<br><br>This approach has an exponential time complexity.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Shortest%20Path%20with%20Alternating%20Colors.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Shortest%20Path%20with%20Alternating%20Colors.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Shortest%20Path%20with%20Alternating%20Colors.java	https://youtu.be/UPSB5SLaaoM
Algorithms Pack	Breadth-First Search	Water And Jug Problem	https://leetcode.com/problems/water-and-jug-problem/	You are given two jugs with capacities *jug1Capacity* and *jug2Capacity* liters. There is an infinite amount of water supply available. Determine whether it is possible to measure exactly *targetCapacity* liters using these two jugs.<br><br>If *targetCapacity* liters of water are measurable, you must have *targetCapacity* liters of water contained **within one or both buckets** by the end.<br><br>Operations allowed:<br>1) Fill any of the jugs with water.<br>2) Empty any of the jugs.<br>3) Pour water from one jug into another till the other jug is completely full, or the first jug itself is empty.	O(nm)	O(nm)	We're expecting an optimized solution of:<br>Time Complexity: **O(mn)**<br>Memory Complexity: **O(mn)**	Backtracking + Hash Set	Dijkstra's Algorithm	BFS	Binary Search	C	Not efficient at all, even compared to the other approaches! Theoretically, we'd do a backtracking search of all possible combinations of *jug1* and *jug2*, storing visited combinations in a hash set to avoid revisiting them.<br><br>This approach is much less efficient than the other approaches and only works for small input sizes.	**Incorrect!**<br><br>This algorithm is designed for finding the shortest path in a weighted graph. Although the jugs problem can be represented as a graph, it is not a weighted graph, so Dijkstra's algorithm would not be effective in solving the problem.	**Correct!**<br><br>In this approach, we use BFS to simulate the process of filling, emptying, and pouring the jugs. We keep track of the state of the jugs (represented as a pair of integers) and maintain a queue of states that we need to explore. We start with an empty state and consider all the possible states that can be reached from the current state by filling, emptying, or pouring the jugs. We add these states to the queue and continue until we find a state where one of the jugs has the target capacity or we have explored all possible states.<br><br>Time Complexity: **O(nm)**, where the number of possible states that we need to explore is equal to the product of the capacities of the jugs, which is O(cap1 X cap2)<br>Memory Complexity: **O(nm)**, or O(cap1 X cap2)	**Not a good approach**!	This is one of the rare cases where offering the brute-force approach gives away the answer. Go and watch *Die Hard With A Vengeance* to get your clue.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Water%20and%20Jug%20Problem.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Water%20and%20Jug%20Problem.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Water%20and%20Jug%20Problem.java	https://youtu.be/em0-DccIGJc
Algorithms Pack	Breadth-First Search	Sliding Puzzle	https://leetcode.com/problems/sliding-puzzle/	On an *2 x 3* board, there are five tiles labeled from *1* to *5*, and an empty square represented by *0*. A **move** consists of choosing *0* and a 4-directionally adjacent number and swapping it.<br><br>The state of the board is solved if and only if the board is *[[1,2,3],[4,5,0]]*.<br><br>Given the puzzle board *board*, return the **least number of moves required so that the state of the board is solved**. If it is impossible for the state of the board to be solved, return *-1*.	OO(mn X (mn)!)	O(mn X (mn)!)	We're expecting an optimized solution of:<br>Time Complexity: **O(box X key)**<br>Memory Complexity: **O(box)**	Greedy	BFS	A* Algorithm	Dijkstra's Algorithm	B	**Not ideal**<br><br>This approach may find a path to the goal state, but it's not guaranteed to be the shortest path. In some cases, it may even get stuck in a loop or fail to find a solution at all.	**Correct!**<br><br>Firstly, convert the 2D board into a single string by concatenating each element of the board. Then, define a vector of vectors to hold the indices of neighboring nodes for each position in the string.<br><br>Maintain a queue to keep track of the current state and a hash set to keep track of visited states. We start by adding the initial state to the queue and mark it as visited in the hash set.<br><br>We then iterate over the queue, popping off the first state and checking its neighbors by finding the index of the *0* tile in the string and swapping it with each of its neighboring tiles. If the resulting string matches the target string, we have found a solution, and we return the level of BFS (number of moves required) to reach that solution. If not, we add the resulting string to the queue and mark it as visited in the hash set.<br><br>If no match is found, we'll eventually return *-1*, indicating that it is impossible to reach the solution state.<br><br>Time Complexity: **O(mn X (mn)!)**<br>Memory Complexity: **O(mn X (mn)!)**	<br><br>This requires an admissible heuristic function, which is not easy to define for this problem. Moreover, the branching factor of the search tree is quite large, so A* may not be efficient enough for this particular problem.	Simply**no!**	Check all possible permutations of the board until we find the correct configuration.<br><br>This would involve generating all possible permutations and checking each one of them against the goal state.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Sliding%20Puzzle.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Sliding%20Puzzle.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Sliding%20Puzzle.java	https://youtu.be/_bMgYIiySw8
Algorithms Pack	Breadth-First Search	Maximum Candies From Boxes	https://leetcode.com/problems/maximum-candies-you-can-get-from-boxes	You have *n* boxes labeled from *0* to *n-1*. You are given four arrays: *status*, *candies*, *keys*, and *containedBoxes* where:<br>1) *status[i]* is *1* if the *ith* box is open and *0* if the *ith* box is closed<br>2) *candies[i]* is the number of candies in the *ith* box<br>3) *keys[i]* is a list of the labels of the boxes you can open after opening the *ith* box.<br>4) *containedBoxes[i]* is a list of the boxes you found inside the *ith* box.<br><br>You are given an integer array *initialBoxes* that contains the labels of the boxes you initially have. You can take all the candies in any open box and you can use the keys in it to open new boxes and you also can use the boxes you find in it.<br><br>Return the **maximum number of candies you can get** following the rules above.	O(n+m)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(m+n)**<br>Memory Complexity: **O(n)**	Greedy	Randomized Search	BFS	Rabin-Karp Algorithm	C	This **doesn't find the most optimal solution.**<br><br>We start with the initial boxes, collecting all the candies we can from them. We then choose the box with the most candies that we can open using the keys we have, and collect all the candies we can from that box.<br><br>We repeat this process until we can't open any more boxes or collect any more candies.<br><br>Time Complexity: **O(n^2)**<br>Memory Complexity: **O(n)**	**BFS** would be better!<br><br>However, this approach would start with the initial boxes and perform a randomized search on all the boxes we can access from them. At each step, we randomly choose one of the unvisited boxes that we can access, and add any new keys we find to our keyring, and use them to open any new boxes we come across. We keep track of the candies we collect along the way and return the total number of candies collected.<br><br>Time Complexity: **O(n^2)**, where n is the number of boxes.<br>Memory Complexity: **O(n)**, where n is the number of boxes.	**Correct!**<br><br>Here, we use BFS to traverse through the boxes and collect the maximum number of candies. Initially, we add keys from the initial boxes to the status of the boxes and collect candies from those boxes.<br><br>We then check the contained boxes in those boxes, add keys from those boxes to the status and collect candies from those boxes as well.  We continue to do this until there are no more unvisited boxes.<br><br>Time Complexity: **O(n+m)**, where n is the number of boxes and m is the total number of keys and contained boxes in the boxes. We traverse through all the boxes and perform constant time operations on each box, hence the linear time complexity.<br>Memory Complexity: **O(n)**, where n is the number of boxes. We use a boolean visited array to keep track of the visited boxes.	**Not applicable** in this case!	While barely viable as a realistic approach, we would start by iterating over all possible combinations of boxes we can choose to open. For each combination, we check if we can open all the boxes in the combination, and if we can, we calculate the total number of candies we can collect from them.<br><br>We return the maximum total number of candies we can collect from any combination of boxes.<br><br>Time Complexity: **O(n X 2^n)**, where n is the number of boxes	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Maximum%20Candies%20You%20Can%20Get%20from%20Boxes.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Maximum%20Candies%20You%20Can%20Get%20from%20Boxes.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Maximum%20Candies%20You%20Can%20Get%20from%20Boxes.java	https://youtu.be/Psfim3qitWA
Algorithms Pack	Breadth-First Search	Shortest Bridge	https://leetcode.com/problems/shortest-bridge	You are given an *n x n* binary matrix *grid* where *1* represents land and *0* represents water.<br><br>An **island** is a 4-directionally connected group of *1*'s not connected to any other *1*'s. There are exactly **two islands** in *grid*.<br><br>You may change *0*'s to *1*'s to connect the two islands to form **one island**.<br><br>Return the **smallest number** of *0*'s you must flip to **connect the two islands**.	O(n^2)	O(n^2)	We're expecting an optimized solution of:<br>Time Complexity: **O(n^2)**<br>Memory Complexity: **O(n^2)**	Union-Find	Browning's Approach	Flood Fill	BFS + Flood Fill	D	**Not quite!**<br><br>We can use the union-find algorithm to label each island with a unique number, and then try flipping each *0* to a *1*, while checking if we can connect both islands. Simply check is the two neighboring cells of the flipped *0* are part of different islands, and then join the islands if not. We repeat this process for all *0*'s in the grid and return the minimum number of flips needed to connect the islands.<br><br>Why it's suboptimal: This approach has a time complexity of O(n^2 alpha(n)) where alpha(n) is the inverse Ackermann function, which is a very slow-growing function. In practice, this function can be considered constant, but the worst-case time complexity is still worse than the **BFS + Flood Fill** approach.	**No!**<br><br>This refers to '*A Bridge Too Far'* rather than the '*Shortest Bridge*.<br><br>For what it's worth, **Browning's Approach** didn't work out particularly well in that instance.	**No**<br><br>Instead, **BFS** is better in terms of efficiency!<br><br>The approach you selected has a time complexity of O(n^3) since we need to iterate over all cells in the grid and for each cell, we may need to perform a BFS operation that could potentially traverse the entire grid.	**Correct!**<br><br>The Flood Fill algorithm is used to find one of the islands and change its values to a different number (*2* in this case). Then, **BFS** is used to explore the neighboring cells of this island until it reaches the other island. During BFS, we keep track of the level or depth of the BFS, which gives us the smallest number of *0*'s we need to flip to connect the two islands.<br><br>Time Complexity: **O(n^2)**, where n is the size of the grid, as we traverse the entire grid twice: once in the Flood Fill algorithm and once in the BFS algorithm.<br>Memory Complexity: **O(n^2)**, as we use a queue to store the cells during BFS, which can hold up to n^2 cells in the worst case.	We can try flipping each *0* to a *1* and then check if we can connect both islands. We can do this using DFS or BFS, checking if we can reach both islands from the flipped 0.<br><br>We repeat this process for all 0's in the grid and return the minimum number of flips needed to connect the islands.<br><br>Why it's suboptimal: this approach has a time complexity of **O(n^4)** since we need to iterate over all cells in the grid and for each cell, we may need to perform a BFS or DFS operation that could potentially traverse the entire grid.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Shortest%20Bridge.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Shortest%20Bridge.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Shortest%20Bridge.java	https://youtu.be/SihF6njhWvU
Algorithms Pack	Breadth-First Search	Tree Diameter	https://leetcode.com/problems/tree-diameter/	The **diameter** of a tree is the **number of edges** in the longest path in that tree.<br><br>There is an undirected tree of *n* nodes labeled from *0* to *n-1*. You are given a 2D array *edges* where *edges.length == n-1* and *edges[i] = [ai, bi]* indicates that there is an undirected edge between nodes *ai* and *bi* in the tree.<br><br>Return the **diameter of the tree**.	O(n)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n)**<br>Memory Complexity: **O(n)**	Floyd-Warshall	DFS	Dynamic Programming	Oak's Algorithm	B	**Not ideal!**<br><br>The Floyd-Warshall algorithm is a good algorithm for finding the shortest path between all pairs of nodes in a graph, but it is **overkill** for this problem. It has a time complexity of O(n^3), which is much worse than the O(n) time complexity of our solution.	**Correct!**<br><br>Our solution's algorithm uses *height* to calculate the diameter of the tree. Starting from a root node, we recursively traverse each child node and calculate its height, which is the maximum height of its children nodes plus one.<br><br>The two maximum heights among all children nodes are then added up to give the maximum path length that goes through the current node. We keep updating the maximum path length until we reach the end of the tree.<br><br>Time Complexity: **O(n)**<br>Memory Complexity: **O(n)**	**Not ideal**, although a reasonable idea!<br><br>DP can solve a complex problem by breaking it down into smaller subproblems and solving each subproblem only once. In the case of trees, we can define a function f(node) that returns the height of the node, and use it to calculate the diameter of the tree. We can then use memoization to store the results of each subproblem to avoid solving them multiple times. <br><br>The weakness here is the efficiency! The time and space complexity of this approach are O(n^2)	This doesn't exist, and you've *unearthed* a *tree-based pun*.<br><br>Q) Why did Professor Oak plant a tree on his computer?<br>A) So he could *log* on<br><br>^_^	We can calculate the diameter of the tree by comparing the length of **all** possible paths. The time complexity of this approach is O(n^2), where n is the number of nodes in the tree.<br><br>Obviously, there are better approaches to consider!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Tree%20Diameter.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Tree%20Diameter.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Tree%20Diameter.java	https://youtu.be/WzaPOq9W8IQ
Algorithms Pack	Topological Sorting	Course Schedule I	https://leetcode.com/problems/course-schedule/	There are a total of *numCourses* courses you have to take, labeled from *0* to *numCourses-1*. You are given an array *prerequisites* where *prerequisites[i] = [ai, bi]* indicates that you **must** take course *bi* first if you want to take course *ai*.<br>For example, the pair *[0, 1]*, indicates that to take course *0* you have to first take course *1*.<br><br>Return *true* if you can finish all courses. Otherwise, return *false*.	O(n+m)	O(n+m)	We're expecting an optimized solution of:<br>Time Complexity: **O(E+V)**, where E is the number of edges, and V the number of vertices<br>Memory Complexity: **O(E+V)**	Sorting	Kahn's Algorithm	Backtracking	Greedy	B	**Incorrect!**<br><br>Simply, this approach may not work in cases where there are circular dependencies between courses. It also does not find a valid ordering of courses.	**Correct!**<br><br>**Kahn's algorithm** finds a topological ordering of a directed acyclic graph (DAG), and can be used here to determine if it's possible to complete all the courses. The algorithm works by iteratively selecting nodes with zero incoming edges (indegree) and removing them from the graph until there are no more nodes left.<br><br>Time Complexity: **O(E+V)**, where V is the number of nodes and E is the number of edges. This is because we need to traverse each node and edge exactly once during the algorithm.<br>Memory Complexity: **O(E+V)**, because we need to store the graph and the indegree vector, each of which takes O(V+E) space.	This will work, but it is a rather **complicated** approach.<br><br>Topological sorting would be better.	**Incorrect!**<br><br>This approach may not always yield a valid ordering of the courses. For example, if there are two courses that have the same number of prerequisites, the greedy algorithm may choose the wrong course to take first.	We can generate all possible orderings of courses, and check if each ordering satisfies the prerequisites.<br><br>Why it's suboptimal: the number of possible orderings of courses is n!, where n is the number of courses, making this approach impractical for any reasonable value of n.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Course%20Schedule.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Course%20Schedule.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Course%20Schedule.java	https://youtu.be/qdHAZnYQUSo
Algorithms Pack	Topological Sorting	Parallel Courses	https://leetcode.com/problems/parallel-courses/	You are given an integer *n*, which indicates that there are *n* courses labeled from *1* to *n*. You are also given an array *relations* where *relations[i] = [prevCoursei, nextCoursei]*, representing a prerequisite relationship between course *prevCoursei* and course *nextCoursei*: course *prevCoursei* has to be taken before course *nextCoursei*.<br><br>In one semester, you can take **any number** of courses as long as you have taken all the prerequisites in the **previous** semester for the courses you are taking.<br><br>Return the **minimum number of semesters needed to take all courses**. If there is no way to take all the courses, return *-1*.	O(n+m)	O(n+m)	We're expecting an optimized solution of:<br>Time Complexity: **O(E+V)**, where E is the number of edges, and V the number of vertices<br>Memory Complexity: **O(E+V)**	DFS	Floyd-Warshall	Greedy	Kahn's Algorithm	D	**Suboptimal**<br><br>The method involves using DFS to generate all possible course orderings that satisfy the prerequisite relationships and return the minimum number of semesters needed among them. However, this approach would also be very inefficient, as the number of possible orderings grows exponentially with the number of courses and prerequisites.	**By no means the best approach!**<br><br>Method: compute the transitive closure of the graph of prerequisite relationships using the Floyd-Warshall algorithm. The length of the longest path in the resulting graph gives the minimum number of semesters required.<br><br>This approach has a time complexity of O(n^3), which is worse than the other approaches for large values of n.	**Incorrect!**<br><br>This won't work for cases where a course has uncompleted prerequisites, leading to an incomplete sequence of courses.	**Correct!**<br><br>We start by creating a directed graph from the given prerequisite *relationships*, with the courses as nodes and the prerequisites as edges. We then perform a topological sort on the graph, which orders the nodes such that for any directed edge *(u, v)*, *u* comes before *v* in the ordering. This allows us to determine the minimum number of semesters needed to take all courses, as each semester can consist of any set of courses that come after all their prerequisites in the ordering.<br><br>Time Complexity: **O(n+m)**, where n is the number of courses and m is the number of prerequisite relationships<br>Memory Complexity: **O(n+m)**, as we need to store the graph and the incoming edge counts for each node.	We can generate all possible combinations of courses that satisfy the prerequisite relationships and return the minimum number of semesters needed among them.<br><br>However, this approach would be very inefficient, as the number of possible combinations grows exponentially with the number of courses and prerequisites.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Parallel%20Courses.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Parallel%20Courses.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Parallel%20Courses.java	https://youtu.be/jWoc4tELCTk
Algorithms Pack	Topological Sorting	Sequence Reconstruction	https://leetcode.com/problems/sequence-reconstruction/	You are given an integer array *nums* of length *n* where *nums* is a permutation of the integers in the range *[1, n]*. You are also given a 2D integer array *sequences* where *sequences[i]* is a subsequence of *nums*.<br><br>Check if *nums* is the shortest possible and the only **supersequence**. The shortest **supersequence** is a sequence **with the shortest length** and has all *sequences[i]* as subsequences. There could be multiple valid **supersequences** for the given array *sequences*.<br>For example, for *sequences = [[1,2],[1,3]]*, there are two shortest **supersequences**, *[1,2,3]* and *[1,3,2]*.<br>While for *sequences = [[1,2],[1,3],[1,2,3]]*, the only shortest **supersequence** possible is *[1,2,3]*. *[1,2,3,4]* is a possible supersequence but not the shortest.<br><br>Return *true* if *nums* is the *only shortest* **supersequence** for *sequences*, or *false* otherwise.<br><br>A **subsequence** is a sequence that can be derived from another sequence by deleting some or no elements without changing the order of the remaining elements.	O(n+m)	O(n+m)	We're expecting an optimized solution of:<br>Time Complexity: **O(m+n)**<br>Memory Complexity: **O(m+n)**	Topological Sort	BFS on Subsequences	Hash Set + Iteration	Two Pointers	A	**Correct!**<br><br>Our approach uses a directed graph to represent the given sequences, where the vertices are the unique numbers from the array nums, and an edge *(u, v)* represents that *u* comes before *v* in some sequence. We then perform a topological sort on it, which will give us a linear ordering of the vertices. We check whether the resulting order is the same as the original array *nums*.<br><br>If the topological sort returns an empty vector, this means that the graph has a cycle, and we **cannot construct a valid supersequence**. If the resulting ordering has a different length than *nums*, then the resulting ordering is not a supersequence of sequences, and thus not the shortest.<br><br>Time Complexity: **O(n+m)**, where *n* is the number of unique elements in *nums*, and *m* is the total number of elements in *sequences*.<br>Memory Complexity: **O(n+m)**	**Expensive!**<br><br>**Topological sort** is a better approach.	Using **topological sort** would be better for this problem.<br><br>The approach involves storing all numbers in nums in a hash set. For each subsequence in *sequences*, iterate through the numbers in the subsequence and check if they are in the hash set. If all numbers are found in the hash set and appear in the correct order, return true, false otherwise.<br><br>Time complexity: O(mn), where m is the total number of subsequence relationships in *sequences*.  Topological sorting will achieve this in O(m+n) complexity.	**Suboptimal** compared to topological sorting.<br><br>The approach here would be to initialize two pointers, *i* and *j*, to point at the start of nums and the start of the first subsequence in sequences, respectively. Iterate through *sequences*, and for each subsequence, move pointer j to the first occurrence of the first number in the subsequence in nums.<br><br>Then, for each number in the subsequence, move pointer i to the next occurrence of the number in nums. If we reach the end of all subsequences and pointer i is at the end of nums, return true, false otherwise.  Time Complexity: O(mn), where m is the total number of subsequence relationships in *sequences*.  Topological sorting does it in O(m+n) time.	We can generate all permutations of the numbers in the range *[1, n]*, then check if each permutation is a supersequence of sequences. Return *true* if we find a **supersequence** that is the same as nums, false otherwise.<br><br>Time Complexity: O(n!), where n is the length of nums - which is **woeful** here.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Sequence%20Reconstruction.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Sequence%20Reconstruction.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Sequence%20Reconstruction.java	https://youtu.be/Hz7OrzMTBNI
Algorithms Pack	Topological Sorting	Minimum Height Trees	https://leetcode.com/problems/minimum-height-trees/	A tree is an undirected graph in which any two vertices are connected by exactly one path. In other words, any connected graph without simple cycles is a tree.<br><br>Given a tree of *n* nodes labelled from *0* to *n-1*, and an array of *n-1* *edges* where *edges[i] = [ai, bi]* indicates that there is an undirected edge between the two nodes *ai* and *bi* in the tree, you can choose any node of the tree as the root. When you select a node *x* as the root, the result tree has height *h*. Among all possible rooted trees, those with minimum height (i.e. *min(h)*) are called **minimum height trees** (MHTs).<br><br>*Return a list of all* **MHT**s' *root labels*. You can return the answer in **any order**.<br><br>The **height** of a rooted tree is the number of edges on the longest downward path between the root and a leaf.	O(n)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n)**, where n is the number of nodes in the graph<br>Memory Complexity: **O(n)**	Topological Sort	Greedy	Tree Diameter Calculation	BFS/DFS	A	**Correct!**<br><br>The idea is to iteratively remove the leaf nodes (i.e., the nodes with only one incoming edge) from the tree until there are at most two nodes left, which are the **roots** of the MHTs.  We start by building an **adjacency list** representation of the given tree using *addEdge*.<br><br>Then, we apply the topological sort algorithm using *topSort*, which returns the roots of the MHTs. In it, we compute the incoming edges for each node in the graph, and initialize a queue with the leaf nodes (i.e., nodes with only one incoming edge). Then, we iteratively remove the leaf nodes, updating the incoming edges of their neighbors, until there are at most two nodes left in the graph. Finally, we return the roots of the MHTs, which are the nodes left in the queue.<br><br>Time Complexity: **O(n)**, where n is the number of nodes in the graph.<br>Memory Complexity: **O(n)**	**Topological sorting** is better.<br><br>This approach does not always yield the correct answer. For instance, there can be cases where the initial root node is far away from the center of the tree, and the nodes closest to the root node are not the ones that form the minimum height trees.	**Not ideal!**<br><br>We can compute the diameter of the tree (i.e., the length of the longest path in the tree) and select the nodes that are in the middle of the diameter path as the root(s). The intuition behind this approach is that the root(s) of the minimum height trees should be close to the center of the tree.<br><br>The diameter-based approach can work for some cases, but it may not always yield the correct answer - there can be cases where the minimum height trees are not close to the center of the diameter path. Moreover, computing the diameter of the tree can be computationally expensive.	**Incorrect!**<br><br>This approach can work for some cases, but it may not always yield the correct answer. There can be cases where the minimum height trees are not formed by the farthest nodes from the two BFS's.<br><br>This approach can be computationally expensive, as we need to perform two BFS's in the worst case.	We can generate all possible trees by considering each node as the root and recursively constructing the subtree for each child. Then, calculate the height of each tree and return the roots of the trees with minimum height.<br><br>However, this approach has an exponential time complexity of O(n X 2^n), as there are 2^n possible subtrees for each of the n nodes in the tree.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Minimum%20Height%20Trees.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Minimum%20Height%20Trees.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Minimum%20Height%20Trees.java	https://youtu.be/JUtZUNyiqAA
Algorithms Pack	Topological Sorting	Longest Increasing Path In A Matrix	https://leetcode.com/problems/longest-increasing-path-in-a-matrix/	Given an *m x n* integers *matrix*, return the **length of the longest increasing path** in *matrix*.<br><br>From each cell, you can either move in four directions: left, right, up, or down. You **may not** move **diagonally** or move **outside the boundary** (i.e., wrap-around is not allowed).	O(mn + E)	O(nm)	We're expecting an optimized solution of:<br>Time Complexity: **O(mn + E)**<br>Memory Complexity: **O(mn)**	Dynamic Programming	Topological Sort + BFS	Greedy	BFS	B	**Suboptimal.**<br><br>For each cell, store the length of the longest increasing path that ends at that cell. We can then compute the length of the longest increasing path that ends at each cell based on the length of the longest increasing path that ends at its neighbors.<br><br>However, this approach is suboptimal because it doesn't take into account the fact that a longer path may be possible if we start at a cell with a lower value.	**Correct!**<br><br>We start by calculating the number of incoming edges for each node in the graph by checking its neighbouring nodes (if they are bigger, we create an edge from the original node to the bigger neighbour, otherwise, we count an incoming edge). We add all nodes with 0 incoming edges to a queue, which represents the start of our path.<br><br>We then perform a BFS on the graph, incrementing a counter for each level we explore. For each level, we remove all nodes that we have explored from the graph by decreasing the incoming edges of their neighbours and adding any new nodes with 0 incoming edges to the queue. The length of the longest path is the final counter value.<br><br>Time Complexity: **O(mn + E)** where E is the number of edges in the graph.<br>Memory Complexity: **O(mn)**	**Not so useful here!**<br><br>In some cases, the path that initially appears to be the longest can lead to a dead end that makes the actual longest path shorter.	BFS would be a more **nave** approach.<br><br>**Topological sorting** is much better here, as we can treat the matrix like a DAG.	We can iterate over every cell in the matrix, and for each cell, explore all possible paths that increase in value until we can't move any further.  We can track the length of the longest path seen so far and return it.<br><br>However, this approach is a little simplistic.  Can you find a better approach?	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Longest%20Increasing%20Path%20in%20a%20Matrix.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Longest%20Increasing%20Path%20in%20a%20Matrix.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Longest%20Increasing%20Path%20in%20a%20Matrix.java	https://youtu.be/2CFdl8B5Y4U
Algorithms Pack	Topological Sorting	Parallel Courses III	https://leetcode.com/problems/parallel-courses-iii/	You are given an integer *n*, which indicates that there are *n* courses labeled from *1* to *n*. You are also given a 2D integer array *relations* where *relations[j] = [prevCoursej, nextCoursej]* denotes that course *prevCoursej* has to be completed **before** course *nextCoursej* (prerequisite relationship). Furthermore, you are given a **0-indexed** integer array *time* where *time[i]* denotes how many **months** it takes to complete the *(i+1)th* course.<br><br>You must find the **minimum** number of months needed to complete all the courses following these rules:<br>1) You may start taking a course at **any time** if the prerequisites are met.<br>2) **Any number of courses** can be taken at the **same time**.<br><br>Return the **minimum number of months needed to complete all the courses**.<br><br>**Note**: The test cases are generated such that it is possible to complete every course (i.e., the graph is a directed acyclic graph).	O(n+m)	O(n+m)	We're expecting an optimized solution of:<br>Time Complexity: **O(E+V)**, where E is the number of edges, and V the number of vertices<br>Memory Complexity: **O(E+V)**	Sorting + Recursion	Topological Sorting + DP	DFS	BFS	B	**Incorrect!**<br><br>This approach may not work in all cases since there could be cases where a course with a longer completion time has prerequisites that are quicker to complete, making it less optimal to start with the longest course.	**Correct!**<br><br>Our graph is represented as an **adjacency list**, and we use **topological sorting** to obtain the order in which the courses can be taken. We start with nodes that have no incoming edges (prerequisites), and gradually remove nodes from the graph while keeping track of the order in which they were visited.<br><br>After that, we use DP to calculate the **minimum time required** to complete each course. For each course, we add its completion time to the time required to complete its prerequisites, and then take the maximum of these values to determine the minimum time required to complete the course.<br><br>Finally, we are able to return the maximum time required to complete any course.<br><br>Time Complexity: **O(E+V)**<br>Memory Complexity: **O(E+V)**	**No!**<br><br>**Topological sort** is a better approach.<br><br>However, this may not work in all cases, since DFS does not guarantee the correct order in which the courses should be taken, potentially leading to wrong solutions.	**Not quite!**<br><br>Although BFS can find a topological order of the graph and guarantee that all prerequisites are completed before each course, this approach does not take into account the fact that multiple courses can be taken at the same time, leading to suboptimal complexity in the solution.	Generate all possible course completion sequences, and for each sequence, check if all prerequisites are met before taking each course.<br><br>Then calculate the total time required for each sequence and return the minimum time among all valid sequences.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Parallel%20Courses%20III.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Parallel%20Courses%20III.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Parallel%20Courses%20III.java	https://youtu.be/IAQkySm81-E
Algorithms Pack	Topological Sorting	Largest Color Value In A Directed Graph	https://leetcode.com/problems/largest-color-value-in-a-directed-graph/	There is a **directed graph** of *n* colored nodes and *m* edges. The nodes are numbered from *0* to *n-1*.<br><br>You are given a string *colors* where *colors[i]* is a lowercase English letter representing the **color** of the *ith* node in this graph (**0-indexed**). You are also given a 2D array *edges* where *edges[j] = [aj, bj]* indicates that there is a **directed edge** from node *aj* to node *bj*.<br><br>A **valid path** in the graph is a sequence of nodes *x1 -> x2 -> x3 -> ... -> xk* such that there is a directed edge from *xi* to *xi+1* for every *1 <= i < k*. The **color value** of the path is the number of nodes that are colored the **most frequently** occurring color along that path.<br><br>Return the **largest color value of any valid path in the given graph**, or *-1 if the graph contains a cycle*.	O(m+n)	O(m)	We're expecting an optimized solution of:<br>Time Complexity: **O(E+V)**, where E is the number of edges, and V the number of vertices<br>Memory Complexity: **O(V)**	Floyd-Warshall	Niji's Algorithm	Dijkstra's Algorithm	Kahn's Algorithm + Dynamic Programming	D	**Suboptimal!**<br><br>We use the **Floyd-Warshall algorithm** to calculate the **shortest path between all pairs of nodes**, and for each path, calculate the maximum color value.<br><br>As you should know, it's O(n^3) in complexity, which is easily improved upon!	**Color me disappointed!** You've picked one of the few entirely fictional algorithms. *Niji* is Japanese for *rainbow*.	**Incorrect!**<br><br>Dijkstra's algorithm does not work on graphs with negative-weight cycles, and this problem explicitly asks to return *-1* if the graph contains a cycle.	**Correct!**<br><br>Our approach to this tough problem uses topological sort to traverse the graph. In each iteration, we calculate the color values of each node that can be reached from the current node by traversing an edge in the graph using DP. We keep track of the count of each color along the current path and update the maximum count of each color that we have seen so far at each node. The largest count of any color encountered along any valid path is the answer we return.<br><br>Time Complexity: **O(E+V)**, where V is the number of nodes and E is the number of edges in the graph.<br>Memory Complexity: **O(26V)**, where V is the number of nodes in the graph and 26 is the number of lowercase English letters.	We can generate all possible paths in the graph and compute the color value of each path, then return the maximum color value among all valid paths.<br><br>It's suboptimal because the number of possible paths in a graph can be very large, making this approach highly inefficient. The time complexity is exponential in the worst case.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Largest%20Color%20Value%20in%20a%20Directed%20Graph.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Largest%20Color%20Value%20in%20a%20Directed%20Graph.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Largest%20Color%20Value%20in%20a%20Directed%20Graph.java	https://youtu.be/wjXSYOWr-CA
Algorithms Pack	Topological Sorting	Strange Printer II	https://leetcode.com/problems/strange-printer-ii/	There is a strange printer with the following two special requirements:<br>1) On each turn, the printer will print a solid rectangular pattern of a single color on the grid. This will cover up the existing colors in the rectangle.<br>2) Once the printer has used a color for the above operation, **the same color cannot be used again**.<br><br>You are given a *m x n* matrix *targetGrid*, where *targetGrid[row][col]* is the color in the position *(row, col)* of the grid.<br><br>Return *true* **if it is possible to print the matrix** *targetGrid*, otherwise, return *false*.	O(n^3)	O(n^2)	We're expecting an optimized solution of:<br>Time Complexity: **O(n^3)**<br>Memory Complexity: **O(n^2)**	Topological Sorting	Backtracking	Greedy	Linear Programming	A	**Correct!**<br><br>Here, we represent the given matrix of colors as a graph, where each color is a node and there is a directed edge from node A to node B if color A appears in a rectangle that completely covers color B. We can then use topological sorting to find if there is a valid ordering of colors such that each rectangle can be printed in a valid order without violating the printer's rules. If we find a valid topological ordering, then we return true, otherwise, we return false.<br><br>Time Complexity is a little tough to calculate here, but we settled on O(mn + Colors^2 + V + E), which is effectively O(n^3).<br>Memory Complexity: O(n^2)	**No**<br><br>This would have exponential time complexity.	**Incorrect!**<br><br>The greedy coloring approach may fail to find a valid solution in some cases where mutually dependent colors are involved. In such cases, there may not be a valid order in which the colors can be colored such that each color can be covered by a single rectangular pattern.	**Not realistically viable!**<br><br>The LP approach can have high computational complexity and may not always guarantee finding an optimal or even feasible solution. Moreover, it may require a large number of variables and constraints, which can make the problem intractable for large grids.	We can try all possible combinations of colors that can be used to cover the entire target grid.<br><br>We start with the color that appears the least number of times in the target grid, and then try all possible rectangular patterns of that color that cover all the cells with that color. Then, we move on to the next least frequent color and repeat the process until we have tried all possible combinations of colors.<br><br>This approach would have exponential time complexity and is not practical for larger input sizes.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Strange%20Printer%20II.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Strange%20Printer%20II.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Strange%20Printer%20II.java	https://youtu.be/_7yroWKxcwU
Algorithms Pack	Topological Sorting	Course Schedule II	https://leetcode.com/problems/course-schedule-ii/	There are a total of *numCourses* courses you have to take, labeled from *0* to *numCourses-1*. You are given an array *prerequisites* where *prerequisites[i] = [ai, bi]* indicates that you must take course *bi* first if you want to take course *ai*.<br>For example, the pair *[0, 1]*, indicates that to take course *0* you have to first take course *1*.<br><br>Return the **ordering of courses you should take to finish all courses**. If there are many valid answers, return **any** of them. If it is impossible to finish all courses, return an **empty array**.	O(n+m)	O(n+m)	We're expecting an optimized solution of:<br>Time Complexity: **O(E+V)**, where E is the number of edges, and V the number of vertices<br>Memory Complexity: **O(E+V)**	Backtracking	Topological Sort	DFS + Memoization	Greedy	B	**Ouch!**<br><br>**Backtracking** is the brute-force approach!	**Correct!**<br><br>This very common problem can be modeled as a directed graph, where each course is a node, and each prerequisite relationship is an edge from the prerequisite course to the course that depends on it. We can use topological sorting to find a valid order to take the courses, which requires that we first take all the prerequisites for a course before taking the course itself.<br><br>Time Complexity: O(V+E), where V is the number of vertices (courses) and E is the number of edges (prerequisites).<br>Memory Complexity: O(V+E), where V is the number of vertices (courses) and E is the number of edges (prerequisites).	**No**<br><br>**Topological sorting** is preferred to this idea.<br><br>This approach uses memoization to avoid recomputing the prerequisites of each course, but it still generates multiple topological orders, and it is not clear which one is the correct one. In addition, it requires extra checks to detect invalid solutions.<br><br>Time Complexity: O(n^2), which is inferior to O(E+V).	**Suboptimal!**<br><br>The greedy approach does not always produce a valid ordering, and there can be multiple valid orderings that it might miss. For example, if there are two disjoint sets of courses that have no prerequisites within each set, the greedy algorithm may take courses in the wrong order, resulting in an invalid ordering. It also fails when there are cycles in the graph.<br><br>Try **topological sorting** instead.	Generate all possible permutations of the courses and check if they satisfy the prerequisites.<br><br>While this approach generates all possible permutations of the courses, it can be very time-consuming for large inputs. In addition, it does not take into account the prerequisites of each course, which can lead to invalid solutions.<br><br>Time Complexity: O(n!)	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Course%20Schedule%20II.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Course%20Schedule%20II.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Course%20Schedule%20II.java	https://youtu.be/B1S0Veyxxas
Algorithms Pack	Dynamic Programming (General)	Fibonacci Number	https://leetcode.com/problems/fibonacci-number/	The **Fibonacci numbers**, commonly denoted *F(n)* form a sequence, called the **Fibonacci sequence**, such that each number is the sum of the two preceding ones, starting from *0* and *1*. That is,<br>*F(0) = 0, F(1) = 1*<br>*F(n) = F(n - 1) + F(n - 2)*, for *n > 1*.<br><br>Given *n*, calculate *F(n)*.<br><br>*n* will **NEVER** be higher than 30.	O(n)	O(1)	We're expecting an optimized solution of:<br>Time Complexity: **O(n)**<br>Memory Complexity: **O(1)**	Simple Recursion	Simple Iterative Solution	Hash Table	Union Find	B	This approach is basically the brute-force method, and is **not recommended** for this problem.	**Correct!**<br><br>We use an iterative approach to compute the Fibonacci number at the *nth* position. We initialize two variables *a* and *b* to *0* and *1* respectively. Then, we iterate *n-1* times and update the value of *a* to *b* and *b* to *a+b*.<br><br>Finally, we return the value of *b*.<br><br>This approach uses constant space and takes linear time to compute the *nth* Fibonacci number. It is more efficient than the recursive approach, which has exponential time complexity.<br><br>Time Complexity: **O(n)**<br>Memory Complexity: **O(1)**	Using a wasteful data structure would be **unwise** for this simple problem.	Whether or not you happen engineer a solution with this approach, you are clearly **over-engineering** if you select this option.	Starting from *F(0)* and *F(1)*, we can recursively calculate each Fibonacci number by summing the previous two.<br><br>Why it's suboptimal: The time complexity of this approach is exponential, **O(2^n)**, making it impractical for larger values of n.<br>Find another way!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Fibonacci%20Number.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Fibonacci%20Number.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Fibonacci%20Number.java	https://youtu.be/BxvuAQ5rIpE
Algorithms Pack	Dynamic Programming (General)	Climbing Stairs	https://leetcode.com/problems/climbing-stairs/	You are climbing a staircase. It takes *n* steps to reach the top.<br><br>Each time you can either climb *1* or *2* steps. In how many distinct ways can you climb to the top?	O(n)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n)**<br>Memory Complexity: **O(n)**	Simple Recursion	Greedy	Divide And Conquer	Dynamic Programming (Memoization)	D	**Too expensive!**<br><br>We *can* recursively try all possible combinations of 1 step and 2 steps at each step, and count the number of combinations that lead to reaching *n* steps.<br><br>This approach is suboptimal because it has exponential time complexity, making it very inefficient for large n. For n=44, it takes over **700 million function calls** to compute the result!<br><br>Time Complexity: O(2^n)	**No**<br><br>This approach does not always lead to the optimal solution.  For example, if n=4, the greedy approach would choose two steps followed by two steps, which results in 2 distinct ways to reach the top.<br><br>However, the optimal solution is to take one step, followed by three steps, which results in 3 distinct ways to reach the top.	**Not ideal.**<br><br>This approach leads to a large number of repeated computations, and therefore, has a high time complexity.	**Correct!** Honestly, there is a better ITERATIVE approach too!<br><br>We use DP with memoization to solve this problem. We start by initializing a memory array of size n+1, where *memory[i]* will store the number of ways to reach step *i*. We know that we can reach step *i* by taking either one or two steps from step *i-1* or *i-2* respectively. Therefore, we can calculate the number of ways to reach step i by summing up the number of ways to reach step i-1 and i-2.<br><br>We store these values in *memory[i]* and return *memory[n]* as the answer.<br><br>We store the intermediate results in memory, which means we do not need to calculate the same subproblems repeatedly.<br><br>Time Complexity: **O(n)**<br>Memory Complexity: **O(n)**	Generate all possible combinations of 1's and 2's to climb the n steps and count the number of valid combinations.<br><br>However, this approach has an exponential time complexity of O(2^n) since for every step, we have two options (climb 1 or 2). It quickly becomes infeasible for large values of n.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Climbing%20Stairs.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Climbing%20Stairs.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Climbing%20Stairs.java	https://youtu.be/7lvvkWjUEqE
Algorithms Pack	Dynamic Programming (General)	Longest Increasing Subsequence	https://leetcode.com/problems/longest-increasing-subsequence/	Given an integer array *nums*, return the **length of the longest strictly increasing subsequence**.	O(n^2)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n^2)**<br>Memory Complexity: **O(n)**	Dynamic Programming (Memoization)	Greedy	Two Pointers	Prefix Max Array	A	**Correct!**<br><br>We begin by iterating through the array starting at each index and recursively find the length of the longest increasing subsequence for the remaining indices that are larger than the current index. We store the results of the recursive calls in a memoization table to avoid repeated work.<br><br>The base case is when we reach the end of the array, at which point we return 0. We add 1 to the result of each recursive call as we consider the current index as part of the subsequence.<br><br>The final result is the maximum length of the increasing subsequence found across all possible starting indices.<br><br>Time Complexity: **O(n^2)**<br>Memory Complexity: **O(n)**	**No!**<br><br>This approach does not guarantee that the resulting subsequence will be the longest possible increasing subsequence. For example, consider the array *[3,4,1,2,3,4]*. The greedy approach will give a subsequence of length 4, whereas the longest possible subsequence has length 5.	This approach **fails to account** for cases where the longest increasing subsequence is not a contiguous subsequence in the input array.	This (**incorrect**) approach involves creating a prefix max array of the input array, where each element of the prefix max array at index i stores the maximum value in the input array up to index i. Then, we iterate through the prefix max array and keep track of the maximum subsequence length encountered so far.<br><br>However, this approach fails to account for cases where the longest increasing subsequence is not a contiguous subsequence in the input array.	We can generate all possible subsequences and check if each subsequence is strictly increasing.<br><br>However, this approach has an exponential time complexity, as there are 2^n possible subsequences for a sequence of length n.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Longest%20Increasing%20Subsequence.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Longest%20Increasing%20Subsequence.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Longest%20Increasing%20Subsequence.java	https://youtu.be/5tWQR9pTCWc
Algorithms Pack	Dynamic Programming (General)	Longest Common Subsequence	https://leetcode.com/problems/longest-common-subsequence/	Given two strings *text1* and *text2*, return the **length of their longest common subsequence**. If there is no **common subsequence**, return *0*.<br><br>A **subsequence** of a string is a new string generated from the original string with some characters (can be none) deleted without changing the relative order of the remaining characters.<br>For example, *ace* is a subsequence of *abcde*.<br><br>A **common subsequence** of two strings is a subsequence that is common to both strings.	O(nm)	O(nm)	We're expecting an optimized solution of:<br>Time Complexity: **O(mn)**<br>Memory Complexity: **O(mn)**	Greedy	Divide and Conquer	Dynamic Programming (Memoization)	Backtracking	C	**Incorrect**<br><br>This approach does not guarantee we'll find the longest common subsequence in all cases.<br><br>In some cases, it may return a suboptimal solution.	**Overly complex**<br><br>This approach has a high time complexity, as it requires solving the problem recursively for multiple subproblems. We divide both input strings into two halves and recursively find the longest common subsequence between each pair of halves.<br><br>We then combine the two subsequences to find the longest common subsequence between the full input strings. **DP** is better!	**Correct!**<br><br>We define a recursive function *LCS* to compute the longest common subsequence of two given strings. At each recursive call, we check if we have already calculated the LCS of the current substrings, and if so, we simply return the value stored in our memoization table.<br><br>If not, we check if the current characters in both strings match. If they do, we add 1 to the length of the LCS, and recursively call LCS on the remaining substrings. If the characters do not match, we try two options: either skip the current character in the first string and move to the next character, or skip the current character in the second string and move to the next character. We take the maximum of the lengths of the two resulting LCSs.<br><br>Time Complexity: **O(mn)**, where m and n are the lengths of the two input strings.<br>Memory Complexity: **O(mn)**	**No!**<br><br>This approach has an exponential time complexity, as it generates all possible subsequences of one of the strings, which can be up to 2^n where n is the length of the string.  Additionally, it may miss some longer common subsequences that are not generated in the backtracking process.	We can enumerate all possible subsequences of *text1* and check if they are also a subsequence of *text2*. Return the length of the longest common subsequence found.<br><br>his approach is inefficient because it requires checking all possible subsequences of one string, which can take exponential time.<br><br>Specifically, there are 2^n possible subsequences of a string of length n, so the time complexity of this approach is O(2^n).	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Longest%20Common%20Subsequence.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Longest%20Common%20Subsequence.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Longest%20Common%20Subsequence.java	https://youtu.be/SG_-BJdz3FQ
Algorithms Pack	Dynamic Programming (General)	Partition Equal SubSet Sum	https://leetcode.com/problems/partition-equal-subset-sum/	Given an integer array *nums*, return *true* if you can **partition the array into two subsets such that the sum of the elements in both subsets is equal** or *false* otherwise.	O(nT)	O(nT)	We're expecting an optimized solution of:<br>Time Complexity: **O(nT)**<br>Memory Complexity: **O(nT)**	Recursive Memoization	Knapsack Algorithm	Sorting + Greedy	DFS	A	**Correct!**<br><br>In our approach, we first check if the sum of all elements is odd.  If so, we can immediately return *false* since we can't partition an odd sum into two equal parts.<br><br>If the sum is even, we divide it by 2 and check if it's possible to find a subset of the given array that sums up to this value.  We can use memoization with recursion to solve this subproblem, storing the result of all previously computed subproblems in a 2D array named **memory**, where *memory[i][j]* checks if it is possible to find a subset with the first i elements of the array that sum up to j. If the value of *memory[i][j]* is already computed, we return it instead of recomputing it.<br><br>Time Complexity: **O(nt)**, where n is the length of the given array and t is the sum of all the elements in the array divided by 2.<br>Memory Complexity: **O(nt)**, where n is the length of the given array and t is the sum of all the elements in the array divided by 2.	**Not quite!**<br><br>The knapsack algorithm is used to find the maximum value with limited capacity, but it doesn't guarantee that the selected items will sum up to exactly the capacity.	**No!** The idea is to sort the input array in non-increasing order and add elements to each subset starting from the largest element until one of the subsets' sum equals or exceeds half of the total sum. We then check if the two subsets' sum is equal or not.<br><br>However, this approach doesn't always work. There may be cases where the optimal solution does not contain the largest elements in the input array.<br><br>For example, consider the input array *[1, 2, 3, 4, 5]*. The greedy approach would put *5* in one subset and *[4,3,2,1]* in the other, resulting in an incorrect answer.	**Not ideal.**<br><br>This approach suffers from the same problem as the Brute-Force approach, which is the exponential number of possible subsets. Therefore, it's not feasible for large inputs.	We can use one observation to help us here.<br><br>For each element in the input array, we have two choices: either include it in one subset, **or** exclude it from both subsets. We can generate all possible combinations of subsets and check if their sum is equal or not.<br><br>However, the number of possible subsets is *2^n* where n is the size of the input array.  It's best to find another approach!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Partition%20Equal%20Subset%20Sum.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Partition%20Equal%20Subset%20Sum.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Partition%20Equal%20Subset%20Sum.java	https://youtu.be/h4jKQiziLT8
Algorithms Pack	Dynamic Programming (General)	Maximum Height By Stacking Cuboids	https://leetcode.com/problems/maximum-height-by-stacking-cuboids/	Given *n cuboids* where the dimensions of the *ith* cuboid is *cuboids[i] = [widthi, lengthi, heighti]* (**0-indexed**). Choose a **subset** of *cuboids* and place them on each other.<br><br>You can place cuboid *i* on cuboid *j* if *widthi <= widthj* and *lengthi <= lengthj* and *heighti <= heightj*. You can rearrange any cuboid's dimensions by rotating it to put it on another cuboid.<br><br>Return the **maximum height of the stacked cuboids**.	O(n^2)	O(n^2)	We're expecting an optimized solution of:<br>Time Complexity: **O(n^2)**<br>Memory Complexity: **O(n^2)**	DFS	Greedy	Recursive Approach + Memoization	Recursive Backtracking	C	**Suboptimal.**<br><br>Presumably, the idea is to represent each cuboid as a node in a graph. This approach may not always lead to the maximum height of the stacked cuboids, and the time complexity is exponential in the worst case.	**Suboptimal** since it may not always lead to calculating the maximum height of the stacked cuboids.	**Correct!**<br><br>We first sort each cuboid's dimensions and sort all cuboids in lexicographic order. Then we define a recursive function that, given the current and previous cuboids, computes the maximum height of a stack starting from the current cuboid.<br><br>We can make use of memoization to avoid repeated calculations.<br><br>Time Complexity: **O(n^2)**, because we need to compare each pair of cuboids to see if one can be placed on top of the other. However, sorting the cuboids in lexicographic order improves the time complexity of the recursive function. The use of memoization avoids repeated calculations, which makes the time complexity faster.<br>Memory Complexity: **O(n^2)**	**Not quite.**<br><br>This approach can still have an exponential time complexity in the worst case, as the number of possible combinations of cuboids can be very large. Additionally, the amount of memory required for memoization can also be very large.<br><br>The idea itself is sound as a tentative solution: we can try all possible combinations of cuboids recursively, memoize the maximum height obtained for each state (current cuboid and previous cuboid), and then then use the memoized values to avoid recomputing the same states multiple times.	We can generate all possible subsets of the given cuboids and check if they can be stacked on each other. For each valid subset, we can calculate the height of the stacked cuboids and return the maximum height among all subsets.<br><br>However, this approach has exponential time complexity since the number of possible subsets is 2^n.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Maximum%20Height%20by%20Stacking%20Cuboids.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Maximum%20Height%20by%20Stacking%20Cuboids.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Maximum%20Height%20by%20Stacking%20Cuboids.java	https://youtu.be/npCU-qFF4E4
Algorithms Pack	Dynamic Programming (General)	House Robber	https://leetcode.com/problems/house-robber/	You are a professional robber planning to rob houses along a street. Each house has a certain amount of money stashed, the only constraint stopping you from robbing each of them is that adjacent houses have security systems connected and **it will automatically contact the police if two adjacent houses were broken into on the same night**.<br><br>Given an integer array *nums* representing the amount of money of each house, return the **maximum amount of money you can rob tonight without alerting the police**.	O(n)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n)**<br>Memory Complexity: **O(n)**	Greedy	Recursive Approach + Memoization	Sorting + Linear Scan	Divide And Conquer	B	**Incorrect!**<br><br>This approach fails to consider cases where the optimal solution requires skipping some houses in order to maximize the total amount of money robbed.	**Correct!**<br><br>We use a recursive function to simulate the robbery at each house. At each house, we have two options: either rob the house or leave it.<br><br>If we rob the house, we add the value of the house to our stolen goods, and we move on to the next house but skip the one after it (since we cannot rob adjacent houses). If we leave the house, we move on to the next house. We use memoization to avoid recomputing overlapping subproblems.<br><br>Time Complexity: **O(n)**<br>Memory Complexity: **O(n)**	This is just a **terrible** idea.	**Incorrect!**<br><br>This approach does not consider the possibility that the optimal solution may involve robbing houses from **both halves** of the array. It is not guaranteed to give the optimal solution in all cases.	The brute force here would make one give up their life of crime!<br><br>We can rob the first house and recursively rob the rest of the houses not adjacent to the current house, or we can skip the current house and rob the rest of the houses. We take the maximum of the two options.<br><br>However, this approach has an exponential time complexity and is not practical for large input sizes.<br><br>**Crime doesn't pay**	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/House%20Robber.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/House%20Robber.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/House%20Robber.java	https://youtu.be/IzpJnd48bRY
Algorithms Pack	Dynamic Programming (General)	Best Time To Buy And Sell Stock With Cooldown	https://leetcode.com/problems/best-time-to-buy-and-sell-stock-with-cooldown/	You are given an array *prices* where *prices[i]* is the price of a given stock on the *ith* day.<br><br>Find the maximum profit you can achieve. You may complete as many transactions as you like (i.e., buy one and sell one share of the stock multiple times) with the following restrictions:<br>After you sell your stock, you cannot buy stock on the next day (i.e., cooldown one day).<br><br>**Note**: You may not engage in multiple transactions simultaneously (i.e., you must sell the stock before you buy again).	O(n)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n)**<br>Memory Complexity: **O(n)**	Stack	Divide and Conquer	Greedy	Dynamic Programming (State Machine + Memoization)	D	**Not quite.**<br><br>This approach makes it very tough to consider the cooldown period, and hence, it can lead to suboptimal solutions.<br><br>We can use a stack to keep track of the local minimum and maximum points, and calculate the profit by adding the difference between the local maximum and minimum points.  However, there are better approaches!	**No.**<br><br>This approach will fail to consider the possibility of multiple transactions within a single half of the array. It will only consider transactions that span both halves, leading to suboptimal profits.	**Suboptimal.**<br><br>This approach does not always yield the optimal solution, as it can miss out on potential profits if a local minimum is followed by a higher peak.	**Correct!**<br><br>We use DP to find the maximum profit that can be achieved at any day with one of the three states: *doing nothing*, *buying a stock*, and *selling a stock*. We keep track of these states along with the index of the day, and use memoization to avoid recalculating the same state twice.<br><br>We start with index 0, and make decisions based on whether we buy, sell, or do nothing on this day. We then move to the next day and repeat the process. The base case is when we have reached the end of the prices array.<br><br>DP examines all possible states and returns the maximum profit that can be achieved with the given prices.<br><br>Time Complexity: **O(n)**<br>Memory Complexity: **O(1)**	We can enumerate all possible combinations of buy and sell transactions and find the maximum profit among them.<br><br>A naive approach doing this would have an exponential time complexity, and is impractical for large inputs.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Best%20Time%20to%20Buy%20and%20Sell%20Stock%20with%20Cooldown.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Best%20Time%20to%20Buy%20and%20Sell%20Stock%20with%20Cooldown.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Best%20Time%20to%20Buy%20and%20Sell%20Stock%20with%20Cooldown.java	https://youtu.be/Y-nIPGHhTzE
Algorithms Pack	Dynamic Programming (General)	Minimum Number of Removals To Make Mountain Array	https://leetcode.com/problems/minimum-number-of-removals-to-make-mountain-array/	You may recall that an array *arr* is a **mountain array** if and only if:<br>1) *arr.length >= 3*<br>2) There exists some index *i* (**0-indexed**) with *0 < i < arr.length-1* such that:<br>a) *arr[0] < arr[1] < ... < arr[i - 1] < arr[i]*<br>b) *arr[i] > arr[i + 1] > ... > arr[arr.length - 1]*<br><br>Given an integer array *nums*, return the **minimum number of elements to remove** to make *nums* a **mountain array**.	O(n^2)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n^2)**<br>Memory Complexity: **O(n)**	Greedy	Divide and Conquer	Dynamic Programming	Linear Scan	C	**Incorrect.**<br><br>This approach assumes that removing the largest elements from the input array will always result in a mountain array. However, this is not always true, as removing a large element could break the mountain structure of the array.<br><br>Moreover, it can miss mountain arrays that are obtained by removing smaller elements.	**No.**<br><br>This approach assumes that the largest mountain array in the input array can be obtained by merging two smaller mountain arrays. However, this is not always true, as the largest mountain array could be entirely contained within one half of the input array.	**Correct!** This can be optimized to O(n log(n)) with binary search!<br><br>Our solution uses the Longest Increasing Subsequence (LIS) and Longest Decreasing Subsequence (LDS) subproblems to calculate the length of the longest possible mountain array that could be formed if the current element is chosen as the peak element.<br><br>The LIS and LDS subproblems are solved using a memoization technique where the previously computed result for an index is saved and reused when the same index is encountered later.<br><br>Our algorithm returns the number of elements that need to be removed to make the input array a mountain array, which is equal to the difference between the length of the input array and the length of the longest possible mountain array.<br><br>Time Complexity: **O(n^2)**<br>Memory Complexity: **O(n)**	Simply, **not a good approach**!	We can generate all possible sub-arrays of the input array with length 3 or greater and check if each sub-array is a mountain array with multiple loops, returning the smallest number of elements to remove to make any of the mountain arrays found.<br><br>This approach has awful time complexity, so we need to find a more sophisticated idea.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Minimum%20Number%20of%20Removals%20to%20Make%20Mountain%20Array.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Minimum%20Number%20of%20Removals%20to%20Make%20Mountain%20Array.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Minimum%20Number%20of%20Removals%20to%20Make%20Mountain%20Array.java	https://youtu.be/OkKvC93tyvI
Algorithms Pack	Dynamic Programming (Enumeration)	Edit Distance	https://leetcode.com/problems/edit-distance/	Given two strings *word1* and *word2*, return the **minimum number of operations required to convert** *word1* to *word2*.<br><br>You have the following three operations permitted on a word:<br>1) Insert a character<br>2) Delete a character<br>3) Replace a character	O(n^2)	O(n^2)	We're expecting an optimized solution of:<br>Time Complexity: **O(n^2)**<br>Memory Complexity: **O(n^2)**	A* Algorithm	Greedy	Backtracking	Levenshtein Distance Algorithm	D	**Too complex.**<br><br>The quality of the solution depends heavily on the quality of the heuristic function used, and finding a good heuristic function for this problem is difficult.<br><br>Additionally, implementing the algorithm can be complex and requires a lot of memory.	**Incorrect!**<br><br>The approach does not consider the global optimum and can lead to a suboptimal solution. For example, it may choose to delete a character that should have been replaced, leading to a higher number of operations.	**Backtracking** would be a huge waste of time in this problem!	**Correct!**  There are more optimal DP approaches, but felt this important to include.<br><br>The **Levenshtein distance algorithm** is a DP technique that computes the minimum number of operations (insertion, deletion, or replacement) required to transform one string into another. In this solution, we initialize a 2D array *memory* to store the results of subproblems, where *memory[i][j]* represents the minimum number of operations required to transform the substring str1[0:i] to the substring str2[0:j].<br><br>We recursively compute memory[i][j] by comparing the i-th character of str1 with the j-th character of str2 and choosing the minimum of three possible operations: insert a character, delete a character, or replace a character.<br><br>Time Complexity: O(n^2)<br>Memory Complexity: O(n^2)	We can generate all possible ways to transform word1 into word2 by performing every combination of insertions, deletions, and replacements, and return the minimum number of operations required.<br><br>However, this approach generates an enormous number of combinations to check, making it very inefficient, especially for long strings.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Edit%20Distance.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Edit%20Distance.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Edit%20Distance.java	https://youtu.be/mkrc6PsTSMc
Algorithms Pack	Dynamic Programming (Enumeration)	Integer Break	https://leetcode.com/problems/integer-break/	Given an integer *n*, break it into the sum of *k* **positive integers**, where *k >= 2*, and maximize the product of those integers.<br><br>Return the **maximum product you can get**.	O(n^2)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n^2)**<br>Memory Complexity: **O(n)**	Dynamic Programming (Memoization)	Backtracking	Greedy	Simple Recursion	A	**Correct!**<br><br>We use DP to store the maximum product of a number as the sum of k positive integers up to n, using memoization to store the result of previously computed sub-problems to avoid recomputing them.<br><br>Time Complexity: **O(n^2)**<br>Memory Complexity: **O(n)**	**No!**<br><br>The number of possible ways of breaking n into k positive integers is **exponential**, so this approach becomes impractical even for small values of n.	Simply, not as viable as using **DP**.	A simple recursive approach won't work as well as **DP !	We can try all possible ways of breaking the integer n into k positive integers and calculate the product of each way, returning the maximum product.<br><br>The number of possible ways of breaking n into k positive integers is exponential, so this approach becomes impractical even for small values of n.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Integer%20Break.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Integer%20Break.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Integer%20Break.java	https://youtu.be/clWtHkypq4g
Algorithms Pack	Dynamic Programming (Enumeration)	Min Cost Climbing Stairs	https://leetcode.com/problems/min-cost-climbing-stairs/	You are given an integer array *cost* where *cost[i]* is the cost of the *ith* step on a staircase. Once you pay the cost, you can either climb one or two steps.<br><br>You can either start from the step with index *0*, or the step with index *1*.<br><br>Return the **minimum cost to reach the top of the floor**.	O(n)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n)**<br>Memory Complexity: **O(n)**	Greedy	Dynamic Programming (Memoization)	Bubble Sort	Binary Search	B	**Nope.**<br><br>This approach does not consider the overall cost of the path and may lead to suboptimal results. For example, consider the input array [10, 15, 20].<br><br>The greedy approach would choose 10, then 15, and then finish with a cost of 25, whereas the optimal solution is to choose 15, then 20, with a total cost of 35.	**Correct!**<br><br>We use memoization to solve this problem recursively. We start from the first or second step and compute the cost of climbing to the top of the floor by taking either one or two steps at a time.<br><br>At each step, we choose the path with minimum cost, and use an array to store the minimum cost required to reach the top of the floor from a particular index.<br><br>Memoization reduces the time complexity of the algorithm from an exponential time complexity to a polynomial time complexity.<br><br>Time Complexity: **O(n)**<br>Memory Complexity: **O(n)**	This would be **highly inefficient** if employed as part of a solution.	**Problematic!**<br><br>Binary search is best reserved for dealing with sorted arrays!	We can try all possible combinations of climbing one or two steps at each index, and return the minimum cost to reach the top.<br><br>However, this approach has an exponential time complexity since it involves trying all possible combinations. As the size of the input array increases, the time complexity will grow very quickly and become infeasible.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Min%20Cost%20Climbing%20Stairs.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Min%20Cost%20Climbing%20Stairs.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Min%20Cost%20Climbing%20Stairs.java	https://youtu.be/3sBTJ_Etn9o
Algorithms Pack	Dynamic Programming (Enumeration)	Perfect Squares	https://leetcode.com/problems/perfect-squares/	Given an integer *n*, return the **least number of perfect square numbers that sum to n**.<br><br>A **perfect square** is an integer that is the square of an integer; in other words, it is the product of some integer with itself. For example, *1*, *4*, *9*, and *16* are perfect squares while *3* and *11* are not.	O(n * sqrt(n))	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n sqrt(n))**<br>Memory Complexity: **O(n)**	Greedy	BFS	Recursive Approach + Memoization	Simple Brute Force	C	Unfortunately, this approach is **suboptimal** because it does not always produce the correct result.<br><br>There are cases where a combination of smaller perfect squares would result in a smaller number of squares than using a single large perfect square.	This approach is **suboptimal** because it can be very memory-intensive for large values of n, since we need to store all possible nodes in the graph.<br><br>Additionally, finding the shortest path using BFS can be very time-consuming for large values of n.	**Correct!**<br><br>Our approach uses recursion with memoization, and starts by checking if n is less than or equal to 0. If so, we return 0 because there are no perfect squares that sum up to a negative number or zero.<br><br>Next, we check if we have already computed the answer for the given value of n. If so, we return the previously computed result. Otherwise, we iterate over all possible perfect squares (i.e., i^2) less than or equal to n and compute the minimum number of perfect squares required to sum up to n. Memoization greatly reduces the time complexity.<br><br>Time Complexity: **O(n X sqrt(n))**<br>Memory Complexity: **O(n)**	This is almost **never** the answer!	We could try all possible combinations of perfect squares, which leads to an exponential time complexity.<br><br>Find a better approach!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Perfect%20Squares.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Perfect%20Squares.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Perfect%20Squares.java	https://youtu.be/h2M-U_mrGjA
Algorithms Pack	Dynamic Programming (Range Patterns)	Longest Palindromic Substring	https://leetcode.com/problems/longest-palindromic-substring/	Given a string *s*, return the **longest palindromic substring in s**.	O(n^2)	O(n^2)	We're expecting an optimized solution of:<br>Time Complexity: **O(n^2)**<br>Memory Complexity: **O(n^2)**	Union-Find	Sliding Window	Dynamic Programming	Greedy	C	**No!**<br><br>Try a different approach!	**No**, but good luck!<br><br>It's very difficult to use this to find the longest palindromic substring.	**Correct!**<br><br>Dynamic Programming is by no means the most optimal approach here, but it is the best of the bunch we're giving you.<br><br>Time Complexity: **O(n^2)**<br>Memory Complexity: **O(n^2)**	The greedy algorithm is **not useful** for this problem.	Generate all possible substrings of the given string, and check each substring to see if it is a palindrome. Keep track of the longest palindrome found.<br><br>This approach has time complexity of O(n^3), where n is the length of the string.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Longest%20Palindromic%20Substring.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Longest%20Palindromic%20Substring.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Longest%20Palindromic%20Substring.java	https://youtu.be/XG3GWGBZECQ
Algorithms Pack	Dynamic Programming (Range Patterns)	Filling Bookcase Shelves	https://leetcode.com/problems/filling-bookcase-shelves/	You are given an array *books* where *books[i] = [thicknessi, heighti]* indicates the thickness and height of the *ith* book. You are also given an integer *shelfWidth*.<br><br>We want to place these books in order onto bookcase shelves that have a total width *shelfWidth*.<br><br>We choose some of the books to place on this shelf such that the sum of their thickness is less than or equal to *shelfWidth*, then build another level of the shelf of the bookcase so that the total height of the bookcase has increased by the maximum height of the books we just put down. We repeat this process until there are no more books to place.<br><br>Note that at each step of the above process, the order of the books we place is the same order as the given sequence of books.<br>For example, if we have an ordered list of *5* books, we might place the first and second book onto the first shelf, the third book on the second shelf, and the fourth and fifth book on the last shelf.<br><br>Return the **minimum possible height that the total bookshelf can be after placing shelves** in this manner.	O(n^2)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n^2), where n is the number of books**<br>Memory Complexity: **O(n)**	Greedy	Dynamic Programming (Memoization)	Backtracking	BFS	B	**Suboptimal**.  Here, we'd always choose the book with the maximum height first, and then fill the rest of the shelf with as many books as possible until we reach the shelf width limit.<br><br>This approach doesn't consider the order of the books and may not result in the minimum possible total height. For example, if we have a tall book followed by several short books that together exceed the shelf width, this approach would place the tall book alone on a shelf and waste space.	**Correct!**<br><br>*minHeightShelves* initializes the *books* and *shelfWidth* variables. We initialize the memory array to -1 before calling *arrange*, which checks if we've already computed the result for the current index. If so, we return the cached value. Otherwise, we initialize *ret* to a large value, and *width* and *mxHeight* to 0.<br><br>Then, we iterate over all possible splits from the current index to the end of the array. For each split, we add the thickness of the book to *width*, updating *mxHeight* as needed. If *width* exceeds the shelf width, we break out of the loop since we cannot add any more books to the current shelf.<br><br>Otherwise, we find the min possible height of the remaining books recursively by calling *arrange* with split+1 as the argument. We add the max height of the books we just placed to the result of the recursive call, updating *ret* to be the min value we have seen so far.<br><br>We return *ret*, which represents the min possible height of the bookshelf if we start placing books from the current index.<br><br>Time Complexity: **O(n^2)**<br>Memory Complexity: **O(n)**	**Highly suboptimal.**<br><br>In this approach, we start with an empty shelf and recursively try to place each book on the shelf, updating the minimum possible total height as we go. This approach tries every possible combination of books on each shelf, which has an exponential time complexity.	**No.**<br><br>This approach is not guaranteed to terminate or find the optimal solution because the graph can be infinitely large if the shelf width is too small or if the books have very small thicknesses.	We could try all possible combinations of books on each shelf, and then check the total height of each shelf to see which one is the smallest. This approach is suboptimal because it has an exponential time complexity.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Longest%20Palindromic%20Substring.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Longest%20Palindromic%20Substring.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Longest%20Palindromic%20Substring.java	https://youtu.be/KyVm1U2npCc
Algorithms Pack	Dynamic Programming (Range Patterns)	Partition Array for Maximum Sum	https://leetcode.com/problems/partition-array-for-maximum-sum/	Given an integer array *arr*, partition the array into (contiguous) subarrays of length **at most** *k*. After partitioning, each subarray has their values changed to become the maximum value of that subarray.<br><br>Return the **largest sum of the given array after partitioning**.	O(nk)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(nk)**<br>Memory Complexity: **O(n)**	Recursive Memoization	Greedy	Sliding Window	Binary Search	A	**Correct!**<br><br>The idea is to start at the beginning of the array and partition the array into subarrays of length at most *k*. We use memoization to store the results of the subproblems to avoid recomputing them.<br><br>We define *partition* to take the starting index of the current subarray as a parameter. At each recursive call, we loop over the next k elements of the array and determine the maximum value of the subarray. We then calculate the sum of the subarray multiplied by its maximum value and add it to the result of partitioning the remaining elements of the array, and take the maximum of all such values to get the final answer.<br><br>Time Complexity: **O(nk)**, where n is the length of the array and k is the maximum length of the subarrays.<br>Memory Complexity: **O(n)**	This approach does **not** always lead to the optimal solution.<br><br>In some cases, it may partition elements that should be left together, resulting in a suboptimal sum.	This approach does **not** always lead to the optimal solution.<br><br>In some cases, it may partition elements that should be left together, resulting in a suboptimal sum.	This approach is **incorrect**, as there can be multiple ways of partitioning the array to achieve the same maximum sum, and binary search may not be able to find all of them.	We can generate all possible partitions of the array and calculate the sum for each partition, and return the maximum sum.<br><br>However, this approach has an exponential time complexity and is therefore very inefficient, especially for large arrays.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Partition%20Array%20for%20Maximum%20Sum.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Partition%20Array%20for%20Maximum%20Sum.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Partition%20Array%20for%20Maximum%20Sum.java	https://youtu.be/x7E0Om0AttA
Algorithms Pack	Dynamic Programming (Range Patterns)	Minimum Insertion Steps to Make a String Palindromic	https://leetcode.com/problems/minimum-insertion-steps-to-make-a-string-palindrome/	Given a string *s*. In one step you can insert any character at any index of the string.<br><br>Return the **minimum number of steps** to make *s* a palindrome.<br><br>A **Palindrome String** is one that reads the same backward as well as forward.	O(n^2)	O(n^2)	We're expecting an optimized solution of:<br>Time Complexity: **O(n^2)**<br>Memory Complexity: **O(n^2)**	Greedy	Sorting	Divide And Conquer	Dynamic Programming (Memoization)	D	This approach does **not** always find the optimal solution.<br><br>For example, consider the string *abcbda*. The greedy approach would insert a *d* on the left side resulting in *adbcbda*, but the optimal solution is to insert a *b* on the right side resulting in *abcbdcbda*.	This approach is **incorrect** because it doesn't account for the characters that are missing from the string.	This approach would **not** work well with this problem!	**Correct!**<br><br>In our approach, we create a 2D array to store the results of all possible substrings of the string, and use memoization to avoid recomputing the results of already computed substrings.<br><br>We check if the first and the last character of the substring are the same. If so, then we can ignore these two characters and find the minimum steps required for the remaining substring. If not, then we have two options: either insert a character at the start of the substring or at the end of the substring.<br><br>We choose the option which results in the minimum number of steps required to make the substring a palindrome.<br><br>Time Complexity: **O(n^2)**, where n is the length of the input string.<br>Memory Complexity: **O(n^2)**	We can generate all possible ways to make *s* a palindrome by inserting characters at any index, and then calculate the number of steps required for each one to be a palindrome.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Minimum%20Insertion%20Steps%20to%20Make%20a%20String%20Palindrome.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Minimum%20Insertion%20Steps%20to%20Make%20a%20String%20Palindrome.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Minimum%20Insertion%20Steps%20to%20Make%20a%20String%20Palindrome.java	https://youtu.be/_dXrE5WLo0I
Algorithms Pack	Dynamic Programming (Range Patterns)	Minimum Cost To Cut a Stick	https://leetcode.com/problems/minimum-cost-to-cut-a-stick/	Given a wooden stick of length *n* units. The stick is labelled from *0* to *n*. For example, a stick of length **6** is labelled as follows:<br><br>|0-1-2-3-4-5-6|<br><br>Given an integer array *cuts* where *cuts[i]* denotes a position you should perform a cut at.<br><br>You should perform the cuts in order, you can change the order of the cuts as you wish.<br><br>The cost of one cut is the length of the stick to be cut, the total cost is the sum of costs of all cuts. When you cut a stick, it will be split into two smaller sticks (i.e. the sum of their lengths is the length of the stick before the cut). Please refer to the first example for a better explanation.<br><br>Return the **minimum total cost of the cuts**.	O(n^3)	O(n^2)	We're expecting an optimized solution of:<br>Time Complexity: **O(n^3)**<br>Memory Complexity: **O(n^2)**	Dynamic Programming	Binary Search	Greedy	Divide And Conquer	A	**Correct!**<br><br>Our approach stores the result of the previous subproblems in a 2D array named *memory*. We define a function named *cost* that calculates the minimum total cost of the cuts for a specific stick interval (*start* to *end*).<br><br>In each step, we iterate over all possible positions to cut the stick. We calculate the cost of the left and right segments and add the length of the stick to the total cost.<br><br>After that, we take the minimum of all possible costs and store it in the memory array, returning the value from the memory array at the end.<br><br>Time Complexity: **O(n^3)**, where n is the size of the cuts vector.<br>Memory Complexity: **O(n^2)**	This approach does **not** always result in the optimal solution as making the cuts at the midpoints of the remaining sticks does not necessarily minimize the overall cost.	This approach does **not** always result in the optimal solution as making the longest cut at each step does not necessarily minimize the overall cost.	**Incorrect!**<br><br>This approach can result in a suboptimal solution as making the longest cut at each step does not necessarily minimize the overall cost.	We can try all possible combinations of cuts and calculate the cost of each combinatio, returning the minimum cost<br><br>However, this approach has an exponential time complexity as the number of possible combinations grows exponentially with the number of cuts. Therefore, it is not feasible for large input sizes.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Minimum%20Cost%20to%20Cut%20a%20Stick.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Minimum%20Cost%20to%20Cut%20a%20Stick.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Minimum%20Cost%20to%20Cut%20a%20Stick.java	https://youtu.be/zqZO-wCwsIc
Algorithms Pack	Dynamic Programming (Range Patterns)	Burst Balloons	https://leetcode.com/problems/burst-balloons/	You are given *n* balloons, indexed from *0* to *n-1*. Each balloon is painted with a number on it represented by an array *nums*. You are asked to burst all the balloons.<br><br>If you burst the *ith* balloon, you will get *nums[i - 1] X nums[i] X nums[i + 1]* coins. If *i-1* or *i+1* goes out of bounds of the array, then treat it as if there is a balloon with a *1* painted on it.<br><br>Return the **maximum coins you can collect by bursting the balloons wisely**.	O(n^3)	O(n^2)	We're expecting an optimized solution of:<br>Time Complexity: **O(n^3)**<br>Memory Complexity: **O(n^2)**	Dynamic Programming	Greedy	Divide And Conquer	Hash Table + Linear Scan	A	**Correct!**<br><br>The idea is to use a recursive formula to find the maximum *coins* we can get by bursting a subarray of balloons from *start* to *end*. To avoid recomputing the same subproblems, we can use memoization to store the results of already computed subproblems in a 2D array.<br><br>At each recursive step, we try every possible balloon to be the last one to burst in the subarray, and compute the maximum coins we can get in this case.<br><br>Time Complexity: **O(n^3)**<br>Memory Complexity: **O(n^2)**	Unfortunately, the greedy approach does **not** necessarily lead to the optimal solution because it only considers the current state and does not take into account the impact of its current decision on future states.<br><br>In other words, the greedy approach is myopic and does not consider the long-term consequences of its actions.	Unfortunately, D&C is **suboptimal** because the optimal solution may involve bursting balloons from both subarrays simultaneously, which is not captured by this approach.<br><br>In other words, the subproblems are not independent, and the optimal solution may involve a combination of solutions from the subproblems.	Quite simply, **no**. DP is the key to solving this very difficult problem.	We can generate all possible orderings of balloons and calculate the maximum coins that can be obtained by bursting them.<br><br>However, this approach has an exponential time complexity because the number of possible orderings is n factorial (n!) - making it impractical, even for small inputs.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Burst%20Balloons.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Burst%20Balloons.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Burst%20Balloons.java	https://youtu.be/TRECp1OS4rU
Algorithms Pack	Dynamic Programming (Counting)	Decode Ways	https://leetcode.com/problems/decode-ways/	A message containing letters from *A-Z* can be encoded into numbers using the following mapping:<br>*'A' -> '1'*<br>*'B' -> '2'*<br>*...*<br>*'Z' -> '26'*<br><br>To **decode** an encoded message, all the digits must be grouped then mapped back into letters using the reverse of the mapping above (there may be multiple ways). For example, *'11106'* can be mapped into:<br>*'AAJF'* with the grouping *(1 1 10 6)*<br>*'KJF'* with the grouping *(11 10 6)*<br><br>Note that the grouping *(1 11 06)* is invalid because *'06'* cannot be mapped into *'F'* since *'6'* is different from *'06'*.<br><br>Given a string *s* containing only digits, **return the number of ways to decode it**.	O(n)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n)**<br>Memory Complexity: **O(n)**	Recursive Approach + Backtracking	Recursive Approach + Memoization	Greedy + Memoization	Regex + Memoization	B	This is the brute-force approach, and is **not recommended**.<br><br>The time complexity is very high since we're exploring all possible combinations, which is exponential in the worst-case scenario. We're doing a lot of redundant work since we're exploring the same combinations multiple times.<br><br>Try another approach!	**Correct**<br><br>We start at the beginning of the string and recursively count the number of ways to decode the string, using memoization to avoid redundant calculations. At each index, we consider taking either a single digit or a two-digit number if it's valid.<br><br>Time Complexity: **O(n)**, where n is the length of the input string.<br>Memory: **O(1)** - the cache only needs to look at *DP[n-1]* and *DP[n-2]*.	**No!**<br><br>It doesn't always yield the correct result since the optimal grouping for a specific digit depends on the previous grouping.<br><br>Also, there are cases where a greedy approach might result in a valid grouping but not the optimal one.	**Not quite!**<br><br>We could convert the string of digits into a regular expression pattern that represents all possible valid combinations of grouping digits together and maps them to letters - and then count the number of matches for the pattern in the string.<br><br>Although it's a different approach, it's still exploring all possible combinations, which is unnecessary since we only need to count the valid ones.	We can brute-force this by generating all possible ways of decoding the string by exploring all combinations of grouping digits together and mapping them to letters.<br><br>We check that each combination is valid, and return the count of valid combinations at the end.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Decode%20Ways.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Decode%20Ways.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Decode%20Ways.java	https://youtu.be/tEUYx6whbGQ
Algorithms Pack	Dynamic Programming (Counting)	Coin Change 2	https://leetcode.com/problems/coin-change-2/	You are given an integer array *coins* representing coins of different denominations and an integer *amount* representing a total amount of money.<br><br>Return the **number of combinations that make up that amount**. If that amount of money cannot be made up by any combination of the coins, return *0*.<br><br>You may assume that you have an infinite number of each kind of coin.	O(nm)	O(m)	We're expecting an optimized solution of:<br>Time Complexity: **O(mn)**<br>Memory Complexity: **O(m)**	Backtracking	Greedy	Dynamic Programming	Recursion	C	**Suboptimal!**<br><br>Backtracking can be very slow and impractical for larger inputs, and it doesn't take advantage of the fact that we can use the same coin multiple times.	**No.**<br><br>Greedy algorithms don't always yield the optimal solution, and in this case, it may skip over smaller coin denominations that are necessary to reach the target amount.	**Correct!**<br><br>The approach involves initializing a 1D array with size (amount+1), where the element at index i represents the number of ways to form the amount i using the given coins. We then iterate over each coin in the input vector and for each coin, we iterate over all possible targets (amounts) starting from 0 up to the maximum amount. We update the number of ways to form the current target by adding the number of ways to form (target-coin value) to the current number of ways to form the target. Finally, we return the value at the index equal to the given amount.<br><br>Time Complexity: **O(mn)**, where n is the size of the coins vector and m is the amount given.<br>Memory Complexity: **O(m)**, where m is the amount given.	This is identical to the brute-force solution, and is not ideal for this problem at all.	One simple brute-force solution would be to recursively check all possible combinations of coins to reach the target amount.<br><br>However, the time complexity is exponential, which makes it very slow and impractical for larger inputs.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Coin%20Change%20II.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Coin%20Change%20II.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Coin%20Change%20II.java	https://youtu.be/ZlGqXKXhuYY
Algorithms Pack	Dynamic Programming (Counting)	Combination Sum IV	https://leetcode.com/problems/combination-sum-iv/	Given an array of **distinct** integers *nums* and a target integer *target*, return the **number of possible combinations that add up to target**.<br><br>Test cases are generated so the answer can fit in a **32-bit** integer.	O(nT)	O(T)	We're expecting an optimized solution of:<br>Time Complexity: **O(nT)**, where *n* is the size of the vector, and *T* the size of the target<br>Memory Complexity: **O(T)**	Greedy	Dynamic Programming	Backtracking	Two Pointers	B	**Suboptimal**, because it may not find all possible combinations that add up to the target value.<br><br>For example, if the nums vector is *[1,2,3]* and the target is *4*, this approach will return only one combination *(3,1)*, but there is another combination *(2,2)* that adds up to *4*.	**Correct!**<br><br>We define *combi*, which takes a target value as input and returns the number of possible combinations that add up to that target.  We use *memory* to store the results of previously computed targets, initialized to -1. In *combi*, we check if the target is less than zero, in which case we return 0 since there are no possible combinations. If the target is 0, we return *1* since there is only one possible combination.<br><br>If the result of *combi* for the given target **has already been computed**, we return that value from memory.<br><br>Otherwise, we iterate through *nums*, and for each number, we recursively call *combi* with the target value reduced by that number. We add up all the returned values from these recursive calls, and store the sum in memory.  Eventually, we return the value of *combi(target)*.<br><br>Time Complexity: **O(nT)**<br>Memory Complexity: **O(target)**	**Not the best choice!** This approach can be very time-consuming, especially if the *nums* vector has many elements, because it generates all possible combinations, even if they cannot add up to the target value.	**Incorrect!** This approach may miss some possible combinations because it only considers combinations that include the first and last elements of the sorted vector.<br><br>For example, if the nums vector is *[1,2,3,4]* and the target is *5*, this approach will miss the combination *(2,3)*.	Using recursion, we can generate all possible combinations of the given *nums* vector and count the ones that add up to the target value.<br><br>This approach is suboptimal because it generates all possible combinations, even if they cannot add up to the target value. It can be very time-consuming, especially if the nums vector has many elements.<br><br>Time Complexity: O(2^n)	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Combination%20Sum%20IV.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Combination%20Sum%20IV.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Combination%20Sum%20IV.java	https://youtu.be/WbdzV_0lsaw
Algorithms Pack	Dynamic Programming (Counting)	Number of Dice Rolls With Target Sum	https://leetcode.com/problems/number-of-dice-rolls-with-target-sum/	You have *n* dice, and each die has *k* faces numbered from *1* to *k*.<br><br>Given three integers *n*, *k*, and *target*, return the **number of possible ways (out of the kn total ways) to roll the dice, so the sum of the face-up numbers equals target**. Since the answer may be too large, return it **modulo** *10^9+7*.	O(nT)	O(nT)	We're expecting an optimized solution of:<br>Time Complexity: **O(nT)**<br>Memory Complexity: **O(nT)**	Recursive Approach + Memoization	Backtracking	Greedy	Dynamic Programming (Memoization)	D	**Not quite.**<br><br>Although this approach can speed up the computation significantly, it still has an exponential time complexity in the worst case, and it requires a lot of memory to store the memoization table.	This approach is **slow** and can produce duplicate combinations, leading to **incorrect** results.	This approach can produce **incorrect** results, since the maximum value may not always be the best choice, and there may not be a combination of rolls that can add up to the target sum.	**Correct!**<br><br>We can use DP + memoization to avoid recalculating values that we've already computed. We start with the base case where we have zero dice and a target of zero, which is equal to one.<br><br>We then recursively calculate the number of ways to get a target sum by rolling the dice one by one and summing up the face-up numbers until we reach our target sum, storing the computed values in a 2D array to avoid recomputing them later.<br><br>Time Complexity: **O(nT)**, where n is the number of dice, and T is the target sum.<br>Memory Complexity: **O(nT)	We can try all possible combinations of rolling n dice with k faces and count the number of combinations that add up to target.<br><br>However, the number of combinations is k^n, which grows exponentially with the number of dice, making it impractical for large values of n.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Number%20of%20Dice%20Rolls%20With%20Target%20Sum.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Number%20of%20Dice%20Rolls%20With%20Target%20Sum.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Number%20of%20Dice%20Rolls%20With%20Target%20Sum.java	https://youtu.be/oD0k4mw0sjI
Algorithms Pack	Dynamic Programming (On Grid)	Unique Paths II	https://leetcode.com/problems/unique-paths-ii/	You are given an *m x n* integer array *grid*. There is a robot initially located at the **top-left corner** (i.e., *grid[0][0]*). The robot tries to move to the **bottom-right corner** (i.e., *grid[m-1][n-1]*). The robot can only move either down or right at any point in time.<br><br>An obstacle and space are marked as *1* or *0* respectively in *grid*. A path that the robot takes cannot include **any** square that is an obstacle.<br><br>**Return the number of possible unique paths that the robot can take to reach the bottom-right corner**.<br><br>The testcases are generated so that the answer will be less than or equal to *2 X 10e9*.	O(mn)	O(mn)	We're expecting an optimized solution of:<br>Time Complexity: **O(mn)**<br>Memory Complexity: **O(mn)**	DFS	Simple Recursion	Greedy	Recursion + Memoization	D	DFS **isn't ideal**, as it can generate an exponential number of paths, while the use of a stack can cause stack overflow for large inputs.	This is simply the brute-force approach, and is**not ideal**.	This approach is **incorrect** as it does not consider all possible paths and can miss some valid paths.	**Correct!**<br><br>In our approach, we first check if the robot is at the bottom-right corner or not. If it is, we return 1.  Otherwise, we check if the current cell is an obstacle or not. If so, we return 0 as the robot can't move to that cell.<br><br>We then check if we've calculated the number of ways to reach the current cell before. If so, we return the stored result. Otherwise, we recursively calculate the number of ways to reach the cell by moving either down or right.<br><br>Time Complexity: **O(mn)**, where m and n are the dimensions of the input grid.<br>Memory Complexity: **O(mn)**, where m and n are the dimensions of the input grid.	Generate all possible paths from the top-left corner to the bottom-right corner, avoiding obstacles, and count the valid ones.<br><br>This approach generates an exponential number of paths, making it very inefficient, especially for large inputs.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Unique%20Paths%20II.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Unique%20Paths%20II.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Unique%20Paths%20II.java	https://youtu.be/Re8oT7ft3X8
Algorithms Pack	Dynamic Programming (On Grid)	Minimum Falling Path Sum	https://leetcode.com/problems/minimum-falling-path-sum/	Given an *n x n* array of integers *matrix*, **return the minimum sum of any falling path** through *matrix*.<br><br>A **falling path** starts at any element in the first row and chooses the element in the next row that is either directly below or diagonally left/right. Specifically, the next element from position *(row, col)* will be *(row+1, col-1)*, *(row+1, col)*, or *(row+1, col+1)*.	O(n^2)	O(n^2)	We're expecting an optimized solution of:<br>Time Complexity: **O(n^2)**<br>Memory Complexity: **O(n^2)**	Greedy	DFS	Recursion + Memoization	Backtracking	C	**Not used** here.<br><br>The greedy approach does not always lead to the optimal solution, and can easily get stuck in a local minimum. It can also miss potential paths with smaller sums.	**Not ideal** compared to **recursion + memoization**.<br><br>DFS can easily get stuck in a local minimum, and can be very slow for large matrices. It can also explore many unnecessary paths.	**Correct!**<br><br>We recursively compute the minimum sum of falling paths through each element of the first row, and memoize the results to avoid redundant computation. We compute the minimum sum of each falling path by considering the element directly below, and the two elements diagonally below it.<br><br>We continue this process until we reach the last row, where we simply return the element. Finally, we return the minimum sum of all possible falling paths through the first row.<br><br>Time Complexity: **O(n^2)**<br>Memory Complexity: **O(n^2)**	Backtracking is **suboptimal** because it requires generating and calculating the sum of all possible paths, leading to an exponential time complexity.	Simply try all possible falling paths starting from each element in the first row, calculate the sum of each path, and return the minimum sum.<br><br>The number of falling paths is exponential, and trying all of them would result in a very high time complexity. This approach is only feasible for small input sizes.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Minimum%20Falling%20Path%20Sum.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Minimum%20Falling%20Path%20Sum.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Minimum%20Falling%20Path%20Sum.java	https://youtu.be/PQoeU_6qitc
Algorithms Pack	Dynamic Programming (On Grid)	Out of Boundary Paths	https://leetcode.com/problems/out-of-boundary-paths/	There is an *m x n* grid with a ball. The ball is initially at the position *[startRow, startColumn]*. You are allowed to move the ball to one of the four adjacent cells in the grid (possibly out of the grid crossing the grid boundary). You can apply **at most** *maxMove* moves to the ball.<br><br>Given the five integers *m*, *n*, *maxMove*, *startRow*, *startColumn*, **return the number of paths to move the ball out of the grid boundary**. Since the answer can be very large, return it **modulo** *10e9 + 7*.	O(mnl)	O(mnl)	We're expecting an optimized solution of:<br>Time Complexity: **O(mnl)**<br>Memory Complexity: **O(mnl)**	Dynamic Programming (Memoization)	DFS	DFS + Backtracking	Niji's Algorithm	A	**Correct!**We store the number of ways to move the ball out of the grid at the given position and with the given number of moves remaining. The count function checks if the current position is out of bounds or if the maximum number of moves has been reached.<br><br>If either of these conditions is true, it returns 1 or 0 respectively. Otherwise, it checks the memory array to see if the value has already been computed for this position and remaining number of moves.<br><br>If it has, it returns the stored value. Otherwise, it recursively computes the number of ways to move the ball out of the grid from each of the four adjacent cells and stores the sum in the memory array for future use.<br><br>Time Complexity: **O(mnl)**, where l is the maximum number of moves allowed.<br>Memory Complexity: **O(mnl)**, where l is the maximum number of moves allowed.	**No!**<br><br>DFS also explores all possible paths, making it impractical for large values of maxMove.<br><br>Additionally, DFS can lead to stack overflow errors for deep recursion, which can be an issue for larger grids.	**Not ideal.**<br><br>This approach can still have exponential time complexity in the worst case, as there can be up to 4^maxMove possible paths to explore.<br><br>It can also lead to stack overflow errors for large maxMove values, and the backtracking can be costly.	**Color me disappointed!**<br><br>You've picked one of the few entirely fictional algorithms. *Niji* is Japanese for *rainbow*.	Try all possible paths starting from the given position and count the number of paths that move the ball out of the grid boundary within maxMove moves.<br><br>However, the number of possible paths grows exponentially with the number of moves, making this approach impractical for large values of maxMove.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Out%20of%20Boundary%20Paths.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Out%20of%20Boundary%20Paths.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Out%20of%20Boundary%20Paths.java	https://youtu.be/pLJEeFATluU
Algorithms Pack	Dynamic Programming (On Grid)	Dungeon Game	https://leetcode.com/problems/dungeon-game/	The demons had captured the princess and imprisoned her in the **bottom-right corner** of a *dungeon*. The *dungeon* consists of *m x n* rooms laid out in a 2D grid. Our valiant knight was initially positioned in the **top-left room** and must fight his way through *dungeon* to rescue the princess.<br><br>The knight has an initial health point represented by a positive integer. If at any point his health point drops to *0* or below, he dies immediately.<br><br>Some of the rooms are guarded by demons (represented by negative integers), so the knight loses health upon entering these rooms; other rooms are either empty (represented as 0) or contain magic orbs that increase the knight's health (represented by positive integers).<br><br>To reach the princess as quickly as possible, the knight decides to move only **rightward** or **downward** in each step.<br><br>Return the **knight's minimum initial health so that he can rescue the princess**.<br><br>**Note** that any room can contain threats or power-ups, even the first room the knight enters and the bottom-right room where the princess is imprisoned.	O(mn)	O(mn)	We're expecting an optimized solution of:<br>Time Complexity: **O(mn)**<br>Memory Complexity: **O(mn)**	Greedy	Dynamic Programming	DFS	BFS	B	This approach is **suboptimal** because it does not take into account the possibility of gaining health in future rooms.	**Correct!**<br><br>We use the 2D array *memory* to store the minimum health needed to reach the bottom-right corner from any given cell in the dungeon. We then traverse the dungeon recursively from the top-left corner to the bottom-right corner, computing the minimum health needed to move forward in each cell, and store it in the memory array. We then return the value stored in memory[0][0].<br><br>Time Complexity: **O(mn)**<br>Memory Complexity: **O(mn)**	**Suboptimal** because it generates all possible paths, which can be exponential in the worst case.	**Suboptimal** because it generates all possible paths, which can be exponential in the worst case.	We regret to have omitted the brute-force solution to this problem. Sorry!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Dungeon%20Game.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Dungeon%20Game.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Dungeon%20Game.java	https://youtu.be/9XpSLtQ1C5U
Algorithms Pack	Dynamic Programming (Tabulation)	Minimum Swaps To Make Sequences Increasing	https://leetcode.com/problems/minimum-swaps-to-make-sequences-increasing/	You are given two integer arrays of the same length *nums1* and *nums2*. In one operation, you are allowed to swap *nums1[i]* with *nums2[i]*.<br><br>For example, if *nums1 = [1,2,3,8]*, and *nums2 = [5,6,7,4]*, you can swap the element at *i = 3* to obtain *nums1 = [1,2,3,4]* and *nums2 = [5,6,7,8]*.<br><br>Return the **minimum number of needed operations to make** *nums1* and *nums2* **strictly increasing**. The test cases are generated so that the given input always makes it possible.<br><br>An array *arr* is **strictly increasing if and only if** *arr[0] < arr[1] < arr[2] < ... < arr[arr.length - 1]*.	O(n)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n)**<br>Memory Complexity: **O(n)**	DFS	Dynamic Programming	Greedy	Dijkstra's Algorithm	B	**Suboptimal!**<br><br>This approach has a time complexity of O(n!), making it infeasible for arrays of even moderate size.	**Correct!**<br><br>We create a 2D memory array to keep track of the minimum swaps required to make *nums1* and *nums2* strictly increasing till the *ith* index. Here, memory[i][0] represents the minimum swaps required if we do not swap the ith index, and memory[i][1] represents the minimum swaps required if we swap the ith index.<br><br>We initialize memory[0][0] = 0 and memory[0][1] = 1, as no swaps are required at the first index, and a swap is required to make the arrays strictly increasing.<br><br> We then iterate through the arrays, and for each index, we check if we swap or not. If we do not swap, we check if the current index can be added to the increasing sequence formed till now. If it can, we update memory[i][0] as the minimum of memory[i][0] and memory[i-1][0]. If it cannot be added, we ignore it.<br><br> If we swap, we check if the current index can be added to the increasing sequence formed till now after swapping. If it can, we update memory[i][1] as the minimum of memory[i][1] and 1+memory[i-1][0], as 1 swap is required. If it cannot be added, we ignore it.<br><br>Finally, we return the minimum of memory[A.size()-1][0] and memory[A.size()-1][1] as the answer.<br><br>Time Complexity: **O(n)**<br>Memory Complexity: **O(n)**	This approach may **not** always find the optimal solution, as swapping elements without considering the future consequences may lead to a dead end.	This is **not at all** an optimal approach for this problem.	We can generate all possible swaps, and calculate the number of swaps required to make both arrays strictly increasing. Eventually, we return the minimum number of swaps.<br><br>However, the time complexity of this approach is O(n!), making it infeasible for arrays of even moderate size.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Minimum%20Swaps%20To%20Make%20Sequences%20Increasing.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Minimum%20Swaps%20To%20Make%20Sequences%20Increasing.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Minimum%20Swaps%20To%20Make%20Sequences%20Increasing.java	https://youtu.be/M5Y0Mzzn2YA
Algorithms Pack	Priority Queue	Ugly Number II	https://leetcode.com/problems/ugly-number-ii/	An **ugly number** is a positive integer whose prime factors are limited to *2*, *3*, and *5*.<br><br>Given an integer *n*, return the *nth* **ugly number**.	O(n log(n))	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n log(n))**<br>Memory Complexity: **O(n)**	Dynamic Programming	Greedy	Priority Queue	DFS	C	**Suboptimal!**<br><br>This approach has a high memory complexity since we would need to store all n ugly numbers in an array. Additionally, we would need to iterate through the array multiple times to generate the next ugly numbers, leading to a higher *time complexity*.	**Incorrect!**<br><br>This approach will frequently return the wrong answer.	**Correct!**<br><br>We start by adding the first ugly number (1) to a priority queue. At each iteration, we take out the smallest ugly number from the priority queue, which is at the top of the heap.<br><br>After that, we add the 2nd, 3rd, and 5th multiples of the removed ugly number to the priority queue. By doing this, we ensure that we have all the ugly numbers in the priority queue in ascending order. We repeat the process until we find the nth ugly number.<br><br>Time Complexity: **O(n log(n))**<br>Memory Complexity: **O(n)**	This approach is **very slow** and requires generating a lot of unnecessary numbers. It can also run into issues with stack overflow for large values of n.	We can generate all positive integers and check whether each integer is an ugly number. We can stop checking once we have found the nth ugly number.<br><br>However, this approach has an extremely high time complexity since we would need to generate and check a potentially infinite number of integers until we find the nth ugly number.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Ugly%20Number%20II.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Ugly%20Number%20II.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Ugly%20Number%20II.java	https://youtu.be/9aQLXHZ2fAc
Algorithms Pack	Dynamic Programming (Tabulation)	Wiggle Subsequence	https://leetcode.com/problems/wiggle-subsequence/	A **wiggle sequence** is a sequence where the differences between successive numbers strictly alternate between positive and negative. The first difference (if one exists) may be either positive or negative. A sequence with one element and a sequence with two non-equal elements are trivially wiggle sequences.<br>1) For example, *[1, 7, 4, 9, 2, 5]* is a **wiggle sequence** because the differences ((6, -3, 5, -7, 3)* alternate between positive and negative.<br>2) In contrast, *[1, 4, 7, 2, 5]* and *[1, 7, 4, 5, 5]* are not wiggle sequences. The first is not because its first two differences are positive, and the second is not because its last difference is zero.<br><br>A **subsequence** is obtained by deleting some elements (possibly zero) from the original sequence, leaving the remaining elements in their original order.<br><br>Given an integer array *nums*, return the **length of the longest wiggle subsequence** of *nums*.	O(n)	O(1)	We're expecting an optimized solution of:<br>Time Complexity: **O(n)**<br>Memory Complexity: **O(1)**	Dynamic Programming	Sorting	Greedy	Divide and Conquer	C	**Suboptimal**<br><br>While possible, and better than brute-force, it still has a time complexity of O(n^2), since we need to compute the maximum of the up and down values for each element in the input array.	**Suboptimal**.<br><br>This approach has a time complexity of **O(n log(n))** due to the sorting step. Additionally, it does not give us the length of the longest wiggle subsequence, only a boolean answer as to whether the input array is a wiggle sequence or not.	**Correct!**<br><br>The approach is based on the observation that we only need to keep track of the difference between successive numbers and whether they are positive or negative. We use two variables, *up* and *down*, to keep track of the **longest wiggle subsequence** (LWS) ending at the current index, where *up* represents the LWS ending at the current index with the last two elements as an up-down pair, and *down* represents the length of the LWS ending at the current index with the last two elements as a down-up pair.<br><br>At each index, we update these values depending on whether the difference between the current element and the previous element is positive or negative.  If the difference is positive, we update up to *down+1*, because the last two elements are a *down-up* pair. If the difference is negative, we update down to *up+1* because the last two elements are an *up-down* pair. Finally, the length of the LWS is 1 plus the maximum of *up* and *down*.<br><br>Time Complexity: **O(n)**<br>Memory Complexity: **O(1)**	**No!**<br><br>This approach may produce incorrect results for certain inputs, especially if the longest wiggle sequence crosses over from one half to the other.	Generate all possible subsequences of the input array with nested loops and check if each subsequence is a wiggle sequence or not. Simply keep track of the length of the longest wiggle sequence found so far.<br><br>However, the time complexity of this approach is O(2^n) where n is the length of the input array, since we generate all possible subsequences.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Wiggle%20Subsequence.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Wiggle%20Subsequence.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Wiggle%20Subsequence.java	https://youtu.be/SefC4ZO_AiI
Algorithms Pack	Backtracking	All Paths From Source To Target	https://leetcode.com/problems/all-paths-from-source-to-target/	Given a directed acyclic graph (**DAG**) of *n* nodes labeled from *0* to *n-1*, find all possible paths from node *0* to node *n-1* and return them in **any order**.<br><br>The graph is given as follows: *graph[i]* is a list of all nodes you can visit from node *i* (i.e., there is a directed edge from node *i* to node *graph[i][j]*).	O(2^n*n)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n X 2^n)**<br>Memory Complexity: **O(n)**	BFS	Dijkstra's Algorithm	Greedy	Backtracking	D	**No.**<br><br>This approach does not explore all possible paths and can get stuck in a dead end.	This approach **does not work** for graphs with negative weights, and it is not guaranteed to find the shortest path in a directed graph.	**No!**<br><br>This approach does not explore all possible paths and can get stuck in a dead end. Also, it is not guaranteed to find the shortest path in a directed graph.	**Correct!**<br><br>In the backtracking approach, we start with a node and explore all its neighbors. If the neighbor is the target node, then we add the current path to our answer. Otherwise, we recursively repeat the same process by exploring the neighbor's neighbors until we reach the target node. We keep track of the current path in a vector and backtrack by removing the last node from the vector and exploring other neighbors.<br><br>Time Complexity: **O(n X 2^n)**, where n is the number of nodes in the graph. The worst-case scenario is when all nodes are connected to each other, and we end up exploring all possible paths, leading to a time complexity of O(2^n). Also, for each path, we need to copy the path to our answer, which takes O(n) time.<br>Memory Complexity: **O(n)**	*Recursive traversal* can be used.<br><br>Starting from node 0, to explore all possible paths by recursively traversing all its neighbors until we reach node n-1.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/All%20Paths%20From%20Source%20to%20Target.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/All%20Paths%20From%20Source%20to%20Target.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/All%20Paths%20From%20Source%20to%20Target.java	https://youtu.be/IC4rmvbcyYE
Algorithms Pack	Backtracking	N-Queens	https://leetcode.com/problems/n-queens/description/	The **n-queens** puzzle is the problem of placing *n* queens on an *n x n* chessboard such that no two queens attack each other.<br><br>Given an integer *n*, return all distinct solutions to the **n-queens puzzle**. You may return the answer in **any order**.<br><br>Each solution contains a distinct board configuration of the n-queens' placement, where *Q* and *.* both indicate a queen and an empty space, respectively.	O(n!)	O(n^2)	We're expecting an optimized solution of:<br>Time Complexity: **O(n!)**<br>Memory Complexity: **O(n^2)**	Backtracking	Greedy	Simulated Annealing	Greedy + Deque	A	**Correct!**<br><br>Backtracking is an algorithmic approach that works by incrementally building a solution to a problem while searching among all possible candidates. In the n-queens problem, we place one queen at a time in a column and move to the next row, avoiding any placement that would allow another queen to attack the current one. When a solution is found, it is added to the list of solutions.<br><br>Time Complexity: **(O(n!)**, since there are n possibilities for the first row, n-2 possibilities for the second row (as one column is already occupied), n-4 possibilities for the third row (as two columns are already occupied), and so on. The number of possibilities reduces rapidly, making the algorithm's time complexity feasible for small values of n.<br>Memory Complexity: **O(n^2)**, since the algorithm uses O(n^2) memory to store the board, O(n) memory for the column array, and O(2n-1) memory for the two diagonal arrays. The total memory usage is, therefore, O(n^2).	**Incorrect.**<br><br>This approach could get stuck in a local minimum, and fail to find a solution even if one exists. It's not a guaranteed solution, and could potentially miss valid solutions.	**Not viable** here.<br><br>This approach may get stuck in a local optimum and fail to find a global solution. It may also fail to find a solution for some values of n. The time complexity is not well-defined.	**No!**<br><br>Even adding a deque to the greedy algorithm isn't going to help this approach work.	There's no viable brute-force for this problem!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/N-Queens.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/N-Queens.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/N-Queens.java	https://youtu.be/561NIls4dLM
Algorithms Pack	Backtracking	Sudoku Solver	https://leetcode.com/problems/sudoku-solver/	Write a program to solve a Sudoku puzzle by filling the empty cells.<br><br>A sudoku solution must satisfy all of the following rules:<br>1) Each of the digits *1-9* must occur exactly once in each row.<br>2) Each of the digits *1-9* must occur exactly once in each column.<br>3) Each of the digits *1-9* must occur exactly once in each of the 9 *3x3* sub-boxes of the grid.<br><br>The *.* character indicates empty cells.	O(9^(mn))	O(1)	We're expecting an optimized solution of:<br>Time Complexity: **O(9^(mn()**<br>Memory Complexity: **O(1)**	Backtracking	Greedy	Sieve of Eratosthenes	Merge Sort	A	**Correct!**<br><br>Our backtracking algorithm tries all possible values for each empty cell of the board until it finds a valid solution. At each empty cell, we try all the digits from 1 to 9 and check if the digit is valid for that cell according to the Sudoku rules.<br><br>If a valid digit is found, we mark it as used in the row, column, and box and move on to the next empty cell. If there is no valid digit for the current cell, we backtrack to the previous cell and try a different digit.<br><br>Time Complexity: **O(9^(mn))**<br>Memory Complexity: **O(1)**, since the algorithm uses three 2D arrays of size 9x9 to keep track of the used digits in each row, column, and box, and two vectors to store the coordinates of the empty cells. Therefore, the memory complexity of this algorithm is O(1) since the size of the input board is fixed.	**No!**<br><br>Greedy algorithms make locally optimal choices at each step without considering the global optimal solution. This can lead to incorrect solutions if the initial choice was incorrect or if there are multiple solutions to the puzzle.	**Not viable** for this problem.	**Nope!**<br><br>Merge sort won't help here.	There's no viable brute-force for this problem!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Sudoku%20Solver.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Sudoku%20Solver.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Sudoku%20Solver.java	https://youtu.be/CmeuavhCxkQ
Algorithms Pack	Backtracking	Permutations	https://leetcode.com/problems/permutations/	Given an array *nums* of distinct integers, return all the **possible permutations**. You can return the answer in **any order**.	O(n*n!)	O(n*n!)	We're expecting an optimized solution of:<br>Time Complexity: **O(n X n!)**<br>Memory Complexity: **O(n X n!)**	Sort + Swap	Backtracking	Divide And Conquer	Bubble Sort	B	**No!**<br><br>This approach involves sorting the array in ascending order and then generating permutations by swapping adjacent elements. However, this approach fails to generate all possible permutations for the input array and can lead to incorrect answers.	**Correct!**<br><br>In this approach, we generate all possible permutations of the given distinct integers array by recursively swapping each element with all other remaining elements one by one until all possible permutations are generated. This is a type of backtracking where we start with the first element, swap it with each element in the array and recursively permute the remaining elements.<br><br>We backtrack by swapping back the elements, so we can generate all permutations. When the current index becomes equal to the length of the array, it means we have generated one possible permutation of the array.<br><br>Time Complexity: **O(n X n!)**, where n is the length of the array. The number of permutations of n elements is n!, and for each permutation, we have to perform n swaps.<br>Memory Complexity: **O(n X n!)**, since we need to store all possible permutations.	**No!**<br><br>This approach does not work because merging the permutations of both parts is not as simple as just concatenating them. It is not possible to get all permutations by just merging the permutations of both parts.	**no!**<br><br>This won't help at all with generating the permutations by itself.	There's no viable brute-force for this problem!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Permutations.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Permutations.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Permutations.java	https://youtu.be/7aHr8j-mWgA
Algorithms Pack	Backtracking	Permutations II	https://leetcode.com/problems/permutations-ii/	Given a collection of numbers, *nums*, that might contain duplicates, **return all possible unique permutations in any order**.	O(n! * n)	O(n! * n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n X n!)**<br>Memory Complexity: **O(n X n!)**	Sorting	Subset Generation	Backtracking + Set	Random Sampling	C	**Suboptimal.**<br><br>This approach relies on the input array being sorted, which adds an extra O(n log(n)) time complexity to the solution. It also skips generating permutations that are equivalent to the previous one, but this does not account for cases where there are repeated elements that are not adjacent in the sorted array.<br><br>Therefore, it does not guarantee unique permutations for all possible inputs.	**Even less efficient than backtracking**.<br><br>Theoretically, we can generate all subsets of the input array, and then generate permutations for each subset. This ensures that there are no repeated elements in the permutation, but it requires generating a large number of subsets, which can be highly inefficient.<br><br>This approach generates a large number of subsets, which can be highly inefficient, especially for larger inputs. It also generates permutations for each subset, which adds an extra time complexity to the solution.	**Correct!**<br><br>We use a backtracking approach to generate all possible permutations of the given numbers while keeping track of unique permutations using a filter (a set in this case).<br><br>Time Complexity: **O(n X n!)**, since generating n! permutations and checking uniqueness takes O(n) time for each permutation.<br>Memory Complexity: **O(n X n!)**, since storing all permutations in a set requires O(n X n!) space.	**Not viable** at all.<br><br>This approach is highly inefficient as it involves generating a large number of random permutations until all unique permutations have been found. It also may not generate all unique permutations for certain inputs, resulting in incorrect output.	There's no viable brute-force for this problem!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Permutations%20II.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Permutations%20II.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Permutations%20II.java	https://youtu.be/4EqvRxfcqus
Algorithms Pack	Divide And Conquer	Global And Local Inversions	https://leetcode.com/problems/global-and-local-inversions/	You are given an integer array *nums* of length *n* which represents a permutation of all the integers in the range *[0, n-1]*.<br><br>The number of **global inversions** is the number of the different pairs *(i, j)* where:<br>1) *0 <= i < j < n*<br>2) *nums[i] > nums[j]*<br><br>The number of **local inversions** is the number of indices *i* where:<br>1) *0 <= i < n-1*<br>2) *nums[i] > nums[i+1]*<br><br>Return *true* if the number of **global inversions** is equal to the number of **local inversions**.	O(n log(n))	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n log(n))**<br>Memory Complexity: **O(n)**	Insertion Sort	Two Pointers	Merge Sort	Divide And Conquer without Sorting	C	**Too slow** compared to Merge Sort. We complement the Insertion Sort algorithm and count the number of local inversions while sorting the array, then count global inversions using the brute-force approach.<br><br>This approach has a suboptimal time complexity of **O(n^2)** for counting global inversions, which is worse than our optimal solution here.	**Incorrect!**<br><br>This approach only counts local inversions and does not account for global inversions, so it may return a false positive result.	**Correct!**  In this approach, we use *merge sort* to count the number of global inversions in the given array, as it inherently counts inversions during the merging process. We also count the number of local inversions using a simple loop. Finally, we compare the counts of local and global inversions to determine if they are equal.<br><br>We use the *mergeSortedSubarrays* function to merge two sub-arrays in the merge sort process. This returns the number of inversions that occur between the two sub-arrays being merged. If the current element of the right sub-array is smaller than the current element of the left sub-array, we count an inversion and add the number of remaining elements in the left sub-array to the inversion count.<br><br>Time Complexity: **O(n log(n))**<br>Memory Complexity: **O(n)**	**Incorrect!**<br><br>This approach only counts global inversions between the two halves and does not account for local inversions or global inversions within each half, so it may return a false negative result. We want Divide&Conquer with Merge Sort!	We can generate all pairs of indices (i, j) and check if they satisfy the condition nums[i] > nums[j] for global inversions, or nums[i] > nums[i+1] for local inversions.<br><br>This approach is suboptimal because it has a time complexity of O(n^2), since we need to generate all possible pairs of indices.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Global%20and%20Local%20Inversions.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Global%20and%20Local%20Inversions.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Global%20and%20Local%20Inversions.java	https://youtu.be/_YC-yHb8UT4
Algorithms Pack	Divide And Conquer	Pow(x,n)	https://leetcode.com/problems/powx-n/	Implement **pow(x, n)**, which calculates *x* raised to the power *n* (i.e., *x^n*)	O(log(n))	O(log(n))	We're expecting an optimized solution of:<br>Time Complexity: **O(log (n))**<br>Memory Complexity: **O(log (n))**	Divide & Conquer	Dynamic Programming	Recursion	Hash Table	A	**Correct!**<br><br>Our approach, *recursive binary exponentiation*, is a widely used technique to calculate the power of a number.  Instead of multiplying the base number with itself 'n' times, we divide the power by two repeatedly until it reaches the base case (power equals 0) while multiplying the base with itself at every step.<br><br>We use a recursive function, *fastPow*, that takes the base number and power as input and returns the result of base to the power of n. It uses D&C to calculate the power more efficiently.  In *fastPow*, if n is 0, we return 1, as anything raised to the power of 0 is 1. Otherwise, we call the function recursively with n/2 and multiply the result by itself. If n is odd, we also multiply the result by x.<br><br>In the main function, *myPow*, we first convert n to a long long variable N to avoid overflow issues. If n is negative, we update x to be its reciprocal, and then calculate the power using *fastPow*.<br><br>Time Complexity: **O(log(n))** since we divide the power by 2 at each recursive call, reducing the number of recursive calls from n to log(n).<br>Memory Complexity: **O(log(n))** as we need to store the intermediate result at each level of the recursive call stack.	This would be **over-engineering** the approach.	This brute-force approach becomes **highly inefficient**, as it will always require n-1 multiplications.	This is **completely unnecessary** for this problem.	We multiply the base number with itself 'n' times to calculate the power.<br><br>However, this approach is highly inefficient as it requires n-1 multiplications to calculate the power.<br><br>Time Complexity: **O(n)**	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Pow(x%2C%20n).cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Pow(x%2C%20n).py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Pow(x%2C%20n).java	https://youtu.be/58sPeKHtoJU
Algorithms Pack	Shortest Path Algorithms	Network Delay Time	https://leetcode.com/problems/network-delay-time/	You are given a network of *n* nodes, labeled from *1* to *n*. You are also given *times*, a list of travel times as directed edges *times[i] = (ui, vi, wi)*, where *ui* is the source node, *vi* is the target node, and *wi* is the time it takes for a signal to travel from source to target.<br><br>We will send a signal from a given node *k*. Return the **minimum time it takes for all the n nodes to receive the signal**. If it is impossible for all the *n* nodes to receive the signal, return *-1*.	O(n+e log(n))	O(e log(n))	We're expecting an optimized solution of:<br>Time Complexity: **O(E + V log(V))**<br>Memory Complexity: **O(E log(V))**	Floyd-Warshall	Bellman-Ford Algorithm	Prim's Algorithm	Dijkstra's Algorithm	D	**Suboptimal**<br><br>This works, but the time complexity of this approach is O(n^3), which is inefficient for larger networks. It also requires more memory than Dijkstra's Algorithm.	**Suboptimal**<br><br>This will work, but is slightly less efficient than Dijkstra's Algorithm for this problem.	**Suboptimal**<br><br>Prim is best used for MST (Minimum Spanning Tree) problems. Dijkstra is the best choice here.	**Correct!**<br><br>We construct an *adjacency list* from the given *times* vector and perform **Dijkstra's algorithm** starting from the source node *k* to find the minimum time it takes for all nodes to receive the signal.<br><br>This approach is guaranteed to find the shortest path to all nodes from the source node in O((E+V)logV) time complexity, where E is the number of edges and V is the number of vertices.	Check all possible paths between nodes and find the minimum time it takes for all nodes to receive the signal.  Why it's suboptimal: This approach checks all possible paths between nodes, which is very time-consuming and inefficient. It has a time complexity of O(n^3) using the Floyd-Warshall algorithm.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Network%20Delay%20Time.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Network%20Delay%20Time.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Network%20Delay%20Time.java	https://youtu.be/ZNn9sh4NpAs
Algorithms Pack	Shortest Path Algorithms	Number of Ways To Arrive At Destination	https://leetcode.com/problems/number-of-ways-to-arrive-at-destination/	You are in a city that consists of *n* intersections numbered from *0* to *n-1* with **bi-directional** roads between some intersections. The inputs are generated such that you can reach any intersection from any other intersection and that there is at most one road between any two intersections.<br><br>You are given an integer *n* and a 2D integer array *roads* where *roads[i] = [ui, vi, timei]* means that there is a road between intersections *ui* and *vi* that takes *timei* minutes to travel. You want to know in how many ways you can travel from intersection *0* to intersection *n-1* in the **shortest amount of time**.<br><br>Return the **number of ways** you can arrive at your destination in the **shortest amount of time**. Since the answer may be large, return it **modulo** *1e9+7*.	O(m log(n))	O(n+m)	We're expecting an optimized solution of:<br>Time Complexity: **O(E log(V))**<br>Memory Complexity: **O(E+V)**	Dijkstra's Algorithm	Greedy	DFS	Kruskal's Algorithm	A	**Correct!**<br><br>Time Complexity: **O(E log(V))**, where E is the number of edges and V is the number of nodes. This is because we need to process each edge once and for each edge, we may add or remove a node from the priority queue, which takes log(n) time.<br>Memory Complexity: **O(E+V)**, where V is the number of nodes and E is the number of edges.	**Suboptimal**<br><br>This approach may get stuck in a suboptimal solution and miss the globally optimal solution.	**Suboptimal**<br><br>This approach may require exploring a large number of combinations, leading to an exponential time complexity.	**No!**<br><br>Kruskal's Algorithm is best used for MST (Minimum Spanning Tree) problems. Dijkstra is the best choice here.	For this problem, brute-force search is **especially slow and impractical** when dealing with large problem sizes, as it checks every possible solution without any optimization.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Number%20of%20Ways%20to%20Arrive%20at%20Destination.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Number%20of%20Ways%20to%20Arrive%20at%20Destination.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Number%20of%20Ways%20to%20Arrive%20at%20Destination.java	https://youtu.be/fAVF9vDTHjY
Algorithms Pack	BFS	Minimum Cost To Make At Least One Valid Path in a Grid	https://leetcode.com/problems/minimum-cost-to-make-at-least-one-valid-path-in-a-grid/	Given an *m x n* grid. Each cell of the grid has a sign pointing to the next cell you should visit if you are currently in this cell. The sign of *grid[i][j]* can be:<br>*1*, which means go to the cell to the right. (i.e go from *grid[i][j]* to *grid[i][j+1]*)<br>*2* which means go to the cell to the left. (i.e go from *grid[i][j]* to *grid[i][j-1]*)<br>*3* which means go to the lower cell. (i.e go from *grid[i][j]* to *grid[i+1][j]*)<br>*4* which means go to the upper cell. (i.e go from *grid[i][j]* to *grid[i-1][j]*)<br><br>Notice that there could be some signs on the cells of the grid that point outside the grid.<br><br>You will initially start at the upper left cell *(0, 0)*. A valid path in the grid is a path that starts from the upper left cell *(0, 0)* and ends at the bottom-right cell *(m-1, n-1)* following the signs on the grid. The valid path does not have to be the shortest.<br><br>You can modify the sign on a cell with *cost = 1*. You can modify the sign on a cell **one time only**.<br><br>Return the **minimum cost to make the grid have at least one valid path**.	O(nm)	O(nm)	We're expecting an optimized solution of:<br>Time Complexity: **O(mn)**<br>Memory Complexity: **O(mn)**	Greedy + Deque	BFS+Deque	Dijkstra's Algorithm + Deque	Brute Force + Deque	B	**No.**<br><br>This approach does not take into account that changing a sign might lead to a cheaper or more optimal path, and may lead to getting stuck in an infinite loop.	**Correct!**<br><br>We use BFS to explore the grid from the start cell, and keep track of the distance from the start cell to each visited cell. We use a deque to store the cells that we need to visit next, and prioritize cells with zero edge weight (i.e. cells that we can visit without incurring any cost).<br><br>We start by setting the distance of the start cell to zero, and we add it to the deque. Then, for each cell in the deque, we explore its neighbors and update their distances if necessary. If the edge weight between the current cell and its neighbor is zero, we add the neighbor to the front of the deque (since it has zero cost), otherwise we add it to the back of the deque.<br><br>Finally, we return the distance of the bottom-right cell from the start cell. If the bottom-right cell is not reachable from the start cell, its distance will be equal to infinity (i.e. the initial value we set for all cells), so we return a value that indicates that the grid has no valid path.<br><br>Time Complexity: **O(mn)**<br>Memory Complexity: **O(mn)**	**Not optimal at all!**<br><br>This approach fails to take into account that we can change the direction of the signs in the grid, and may lead to finding a non-optimal path.	**No!**<br><br>Good luck with that approach! A BFS + Deque will work here.	We could use a **recursive search**!<br><br>Basically, we try every possible path starting from (0, 0) to (m-1, n-1). For each cell, recursively check all possible directions until either a valid path is found or all paths are exhausted.<br><br>However, this approach has exponential time complexity as it explores all possible paths, making it very expensive for even moderately large grids.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Minimum%20Cost%20to%20Make%20at%20Least%20One%20Valid%20Path%20in%20a%20Grid.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Minimum%20Cost%20to%20Make%20at%20Least%20One%20Valid%20Path%20in%20a%20Grid.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Minimum%20Cost%20to%20Make%20at%20Least%20One%20Valid%20Path%20in%20a%20Grid.java	https://youtu.be/InealzgYMjg
Algorithms Pack	Shortest Path Algorithms	Number of Restricted Paths From First To Last Node	https://leetcode.com/problems/number-of-restricted-paths-from-first-to-last-node/	There is an undirected weighted connected graph. You are given a positive integer *n* which denotes that the graph has *n* nodes labeled from *1* to *n*, and an array *edges* where each *edges[i] = [ui, vi, weighti]* denotes that there is an edge between nodes *ui* and *vi* with weight equal to *weighti*.<br><br>A path from node *start* to node *end* is a sequence of nodes *[z0, z1, z2, ..., zk]* such that *z0 = start* and *zk = end* and there is an edge between *zi* and *zi+1* where *0 <= i <= k-1*.<br><br>The distance of a path is the sum of the weights on the edges of the path. Let *distanceToLastNode(x)* denote the shortest distance of a path between node *n* and node *x*. A **restricted path** is a path that also satisfies that *distanceToLastNode(zi) > distanceToLastNode(zi+1)* where *0 <= i <= k-1*.<br><br>Return the **number of restricted paths** from node *1* to node *n*. Since that number may be too large, return it **modulo** *10e9+7*.	O(m log(n) + n^2)	O(n + m)	We're expecting an optimized solution of:<br>Time Complexity: **O(m log(n) + n^2)**<br>Memory Complexity: **O(m+n)**	Exhaustive Search + Brute Force	Greedy	Dynamic Programming + Dijkstra's Algorithm	Randomized Search	C	**Not the right approach.**<br><br>This approach has a high time complexity and is only practical for small problem sizes. For larger problems, the search space becomes too large to explore in a reasonable amount of time.	**Incorrect!**<br><br>This approach is suboptimal because it may not always lead to the optimal solution. Greedy algorithms may be fooled by a locally optimal choice that does not lead to the globally optimal solution.	**Correct!**<br><br>Our approach involves using Dijkstra's Algorithm to find the shortest distance from node n to every other node in the graph. We then apply DP to count the number of restricted paths from node 1 to node n.<br><br>To count the number of restricted paths, we use a recursive function *countPaths* that counts the number of restricted paths from the current node to the destination node (n).<br><br>Time Complexity: **O(m log(n))**, where m is the number of edges and n is the number of nodes.<br>Space Complexity: **O(n + m)**, where n is the number of nodes and m is the number of edges.	This is just a **terrible idea** for solving this problem.	There's very little worth to the brute-force in this case.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Number%20of%20Restricted%20Paths%20From%20First%20to%20Last%20Node.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Number%20of%20Restricted%20Paths%20From%20First%20to%20Last%20Node.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Number%20of%20Restricted%20Paths%20From%20First%20to%20Last%20Node.java	https://youtu.be/2A2qD0cpsjY
Algorithms Pack	Minimum Spanning Tree	Minimum Cost To Connect All Points	https://leetcode.com/problems/min-cost-to-connect-all-points/	You are given an array *points* representing integer coordinates of some points on a 2D-plane, where *points[i] = [xi, yi]*.<br><br>The cost of connecting two points *[xi, yi]* and *[xj, yj]* is the **Manhattan distance** between them: *|xi - xj| + |yi - yj|*, where *|val|* denotes the absolute value of *val*.<br><br>Return the **minimum cost to make all points connected**. All points are connected if there is **exactly one simple path between any two points**.	O(n^2 log(n))	O(n^2)	We're expecting an optimized solution of:<br>Time Complexity: **O(n^2 log(n))**<br>Memory Complexity: **O(n^2)**	Kruskal's Algorithm + Union Find	Floyd-Warshall	Dijkstra's Algorithm	DFS	A	**Correct!**<br><br>Actually, there's an optimized version of Prim's Algorithm that is superior to this, too.<br><br>We use Kruskal's Algorithm to find the minimum spanning tree (MST) of the complete graph formed by the given points, where the weight of each edge is the Manhattan distance between the two points it connects. We create an edge list with all possible edges and their weights and sort it in decreasing order of weight. We then use Union-Find data structure to iteratively add edges to the MST while avoiding cycles until all nodes are connected.<br><br>Time Complexity: **O(n^2 log(n))**<br>Memory Complexity: **O(n^2)**	**No.**<br><br>The Floyd-Warshall algorithm has a time complexity of O(n^3), which is worse than the O(n^2 log(n)) time complexity of Kruskal's algorithm on this problem.	**No.**<br><br>Dijkstra's algorithm is designed for finding the shortest path between two points in a graph, and is not suitable for finding minimum spanning trees, which violates the problem constraints.	**Incorrect!**<br><br>DFS can only find a spanning tree of the graph, which may not be the minimum spanning tree. Moreover, it may not work on disconnected graphs, which violates the problem constraints.	Brute-force is arguably more difficult than the correct answer.<br><br>Calculate the Manhattan distance between all pairs of points, and then choose an MST algorithm to find the minimum spanning tree of the graph.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Min%20Cost%20to%20Connect%20All%20Points.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Min%20Cost%20to%20Connect%20All%20Points.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Min%20Cost%20to%20Connect%20All%20Points.java	https://youtu.be/0lzuqGIB3XY
Algorithms Pack	Minimum Spanning Tree	Similar String Groups	https://leetcode.com/problems/similar-string-groups/	Two strings, *X* and *Y*, are considered similar if either they are identical or we can make them equivalent by swapping at most two letters (in distinct positions) within the string *X*.<br><br>For example, *tars* and *rats* are similar (swapping at positions *0* and *2*), and *rats* and *arts* are similar, but *star* is not similar to *tars*, *rats*, or *arts*.<br><br>Together, these form two connected groups by similarity: *{'tars', 'rats', 'arts'}* and *{'star'}*. Notice that *'tars'* and *'arts'* are in the same group even though they are not similar. Formally, each group is such that a word is in the group if and only if it is similar to at least one other word in the group.<br><br>We are given a list *strs* of strings where every string in *strs* is an anagram of every other string in *strs*. How many groups are there?	O(n^2 * k)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n^2 X k)**<br>Memory Complexity: **O(n)**	Hash Table	Union Find	DFS	Brute Force + Deque	B	**Suboptimal** due to increased time and memory complexity.<br><br>The approach would:<br>1) For each word, generate a set of all similar words using the isSimilar function.<br>2) Add each word and its set of similar words to a hash table.<br>3) For each word in the hash table, union it with all the words in its set of similar words.	**Correct!**<br><br>We first initialize an instance of the Union-Find data structure with n sets, where n is the number of strings in the given list. Each set represents a group of similar strings. We iterate through each pair of strings in the list and check if they are similar using the *isSimilar* function. If two strings are similar, we perform a union operation on the corresponding sets in the Union-Find data structure using the *union_set* function. Finally, we return the number of sets, which represents the number of groups of similar strings.<br><br>Time Complexity: **O(n^2 X k)**, where n is the number of strings in the given list and k is the length of each string.<br>Memory Complexity: **O(n)**, since we initialize an instance of the Union-Find data structure with n sets, and each set requires a constant amount of memory.	**Not quite!**<br><br>This solution has dreadful time complexity because we need to check each pair of words to see if they are similar.	With or without a deque, a **Union-Find** would be a better choice for solving this problem.	Sorry, we're not offering a brute-force suggestion for this problem.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Similar%20String%20Groups.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Similar%20String%20Groups.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Similar%20String%20Groups.java	https://youtu.be/mpSDMbCJJIY
Algorithms Pack	Minimum Spanning Tree	Number of Operations To Make Network Connected	https://leetcode.com/problems/number-of-operations-to-make-network-connected/	There are n computers numbered from *0* to *n-1* connected by ethernet cables *connections* forming a network where *connections[i] = [ai, bi]* represents a connection between computers *ai* and *bi*. Any computer can reach any other computer directly or indirectly through the network.<br><br>You are given an initial computer network *connections*. You can extract certain cables between two directly connected computers, and place them between any pair of disconnected computers to make them directly connected.<br><br>Return the **minimum number of times you need to do this in order to make all the computers connected**. If it is not possible, return *-1*.	O(n + m log(n))	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n + m log(n))**<br>Memory Complexity: **O(n)**	Floyd-Warshall	BFS	Recursive Backtracking	Union Find	D	**Suboptimal**, and incorrect even if the edges were guaranteed to exist.	BFS **does not guarantee** that all nodes are connected. It only counts the number of connected components.	Even if this were engineered to work, this would be **wildly inferior** to a Union-Find approach.	**Correct!**<br><br>Our approach is based on the Union-Find algorithm. First, we check if the number of connections is less than *n-1*, which is the minimum number of edges required to connect all nodes. If there are fewer edges than required, we know there are disconnected components in the graph. Then we create a Union-Find data structure *uf* with n nodes, and iterate through each connection in connections and call the *union_set* method to join the nodes represented by the current connection. This first finds the sets to which the nodes belong using *find_set*, and then merges the smaller set into the larger set using the *link* method of uf. If the sets were merged, it decrements the forests counter of uf by 1. Finally, we return the number of connected components in the graph, which is equal to the number of forests in uf minus 1.<br><br>Time Complexity: **O(n + m log(n))**, where n is the number of nodes and m is the number of edges. The union_set operation takes O(log(n)) time, and we call it once for each edge in the input.<br>Memory Complexity: **O(n)**	We can generate all possible sets of edges that connect all nodes, and count the minimum number of operations required to form a connected graph from the given edges.<br><br>However, this approach generates all possible subsets of edges, which results in a time complexity of O(2^m), where m is the number of edges in the graph. This quickly becomes infeasible even for moderately sized graphs.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Number%20of%20Operations%20to%20Make%20Network%20Connected.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Number%20of%20Operations%20to%20Make%20Network%20Connected.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Number%20of%20Operations%20to%20Make%20Network%20Connected.java	https://youtu.be/OQPmhj9qjJY
Algorithms Pack	Minimum Spanning Tree	Find Critical And Pseudo-Critical Edges In Minimum Spanning Tree	https://leetcode.com/problems/find-critical-and-pseudo-critical-edges-in-minimum-spanning-tree/	Given a weighted undirected connected graph with *n* vertices numbered from *0* to *n-1*, and an array *edges* where *edges[i] = [ai, bi, weighti]* represents a bidirectional and weighted edge between nodes *ai* and *bi*. A minimum spanning tree (MST) is a subset of the graph's edges that connects all vertices without cycles and with the minimum possible total edge weight.<br><br>Find all the **critical and pseudo-critical edges in the given graph's MST**. An MST edge whose deletion from the graph would cause the MST weight to increase is called a **critical edge**. On the other hand, a **pseudo-critical** edge is that which can appear in *some* MSTs but *not all*.<br><br>Note that you can return the indices of the edges in any order.	O(m log(n))	O(n+m)	We're expecting an optimized solution of:<br>Time Complexity: **O(m log(n))**<br>Memory Complexity: **O(m+n)**	Kruskal's Algorithm + Union Find	DFS	Dijkstra's Algorithm	Rabin-Karp Algorithm	A	**Correct!**<br><br>We first sort the given edges in non-descending order and adds edges one by one to the set of edges if they do not form a cycle in the current set. We maintain the MST weight so far as we add edges. We then iterate over all edges and remove each one in turn to check whether it is critical, pseudo-critical or neither.<br><br>For this, we compute the MST weight of the remaining edges. If removing the edge causes an increase in the MST weight, then it is a critical edge. If the edge is required to be included in every MST of the graph, it is a pseudo-critical edge. Otherwise, the edge is neither critical nor pseudo-critical.<br><br>Time Complexity: **O(m log(m))**, where m is the number of edges in the graph. Sorting the edges takes O(m log(m)) time, and performing union-find on edges requires O(m log(n)) time in the worst case, where log is the iterated logarithm function.<br>Memory Complexity: **O(m+n)**, where n is the number of nodes in the graph. The Union-Find data structure uses O(n) memory, and the edge list uses O(m) memory.	**Suboptimal** because it requires computing a new MST for each modified graph, which may take a lot of time, especially for large graphs.	**No!**<br><br>Given that we're looking for a minimum spanning tree here, you should be thinking about Prim or Kruskal's algorithms.	**No!**<br><br>Given that we're looking for a minimum spanning tree here, you should be thinking about Prim or Kruskal's algorithms. This is a string searching algorithm!	We'd need to generate all possible subsets of edges, and find the minimum spanning tree for each subset. Among all the minimum spanning trees, identify critical and pseudo-critical edges.<br><br>However, the number of possible subsets is 2^m where m is the number of edges in the graph. Therefore, the time complexity of this algorithm is prohibitively expensive for graphs with even moderate numbers of edges!	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Find%20Critical%20and%20Pseudo-Critical%20Edges%20in%20Minimum%20Spanning%20Tree.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Find%20Critical%20and%20Pseudo-Critical%20Edges%20in%20Minimum%20Spanning%20Tree.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Find%20Critical%20and%20Pseudo-Critical%20Edges%20in%20Minimum%20Spanning%20Tree.java	https://youtu.be/t-cpaIXowWI
Algorithms Pack	Minimum Spanning Tree	Minimum Malware Spread	https://leetcode.com/problems/minimize-malware-spread/	You are given a network of *n* nodes represented as an *n x n* adjacency matrix *graph*, where the *ith* node is directly connected to the *jth* node if *graph[i][j] == 1*.<br><br>Some nodes *initial* are initially infected by malware. Whenever two nodes are directly connected, and at least one of those two nodes is infected by malware, both nodes will be infected by malware. This spread of malware will continue until no more nodes can be infected in this manner.<br><br>Suppose *M(initial)* is the final number of nodes infected with malware in the entire network after the spread of malware stops. We will remove **exactly one node** from *initial*.<br><br>Return the node that, if removed, would minimize *M(initial)*. If multiple nodes could be removed to minimize *M(initial)*, return such a node with **the smallest index**.<br><br>Note that if a node was removed from the *initial* list of infected nodes, it might still be infected later due to the malware spread.	O(n^2)	O(n)	We're expecting an optimized solution of:<br>Time Complexity: **O(n^2)**<br>Memory Complexity: **O(n)**	Union Find + Connected Components	Greedy	Divide And Conquer	Shell Sort	A	**Correct!** (DFS is also viable here).<br><br>We build a Union Find data structure with the given adjacency matrix, and use it to find the connected components (CCs) in the graph.  We use the *rootMalwareCount* vector to store the count of the number of malware nodes in each CC, and then iterate over the initial list of malware nodes and find the CC that contains the current node. If this CC contains exactly one malware node, we check if the size of this CC is greater than the current *maxInfected* value. If it is, we update *maxInfected* and *nodeRemove*.<br><br>Time Complexity: O(n^2 + nlogn) = **O(n^2)** (where n is the number of nodes in the graph)<br>Memory Complexity: **O(n)**, since we use a UnionFind data structure of size n.	Very **unlikely to succeed**.<br><br>This would make the locally optimal choice at each step in the hope of finding a global optimum.<br><br>Why it's suboptimal: Greedy algorithms do not always lead to the best solution, as the globally optimal choice may not always be the locally optimal one. It can also get stuck in local optima and fail to find the global optimum.	**No!**<br><br>Divide and conquer algorithms may not always produce the optimal solution, as the optimal solution may require combining the solutions of the sub-problems in a specific way.	Time to sort out a revision plan**incorrect!**	This is the worst problem for any brute-force approach!  We cannot recommend one here.	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackCPP/Minimize%20Malware%20Spread.cpp	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackPythonSolutions/Minimize%20Malware%20Spread.py	https://raw.githubusercontent.com/rabogan/tentativeSolutions/main/AlgorithmsPackJavaSolutions/Minimize%20Malware%20Spread.java	https://youtu.be/01ulSaglaE4